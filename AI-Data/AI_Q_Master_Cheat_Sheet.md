# AI QUIZ MASTER CHEAT SHEET
**Slides 1-86 Complete Reference**

---

## ğŸ“‘ Table of Contents
1. [Search Problem Formulation (Slides 1-10)](#section-1-search-problem-formulation)
2. [Uninformed Search - BFS & DFS (Slides 11-25)](#section-2-uninformed-search---bfs--dfs)
3. [Uninformed Search - UCS, DLS, IDS (Slides 26-35)](#section-3-uninformed-search---ucs-dls-ids)
4. [Complexity Comparison (Slides 36-40)](#section-4-complexity-comparison-table)
5. [Heuristic Functions (Slides 41-50)](#section-5-heuristic-functions)
6. [A* Algorithm & Admissibility (Slides 51-60)](#section-6-a-algorithm--admissibility)
7. [Consistency & Dominance (Slides 51-60 continued)](#section-7-consistency--dominance)
8. [Memory-Bounded Heuristics (Slides 61-65)](#section-8-memory-bounded-heuristics)
9. [Local Search - Hill Climbing (Slides 66-75)](#section-9-local-search---hill-climbing)
10. [Simulated Annealing (Slides 66-75 continued)](#section-10-simulated-annealing)
11. [Genetic Algorithms (Slides 76-82)](#section-11-genetic-algorithms)
12. [Local Search Comparison (Slides 83-86)](#section-12-local-search-comparison)
13. [AND-OR Search Trees (Slides 83-86)](#section-13-and-or-search-trees)

---

## SECTION 1: SEARCH PROBLEM FORMULATION
**Coverage: Slides 1-10**

### ğŸ¯ MNEMONIC: "SAGI-H"

- **S** - State Space (all possible configurations)
- **A** - Actions/Operators (moves between states)
- **G** - Goal Test (condition to check success)
- **I** - Initial State (starting point)
- **H** - Heuristic (optional - for informed search)

### ğŸ“Œ Example 1: Romania Travel Problem

- **State:** Current city name
- **Actions:** Drive(city1, city2)
- **Goal:** InCity(Bucharest)
- **Initial:** InCity(Arad)
- **Heuristic:** StraightLineDistance(current, Bucharest)

### ğŸ“Œ Example 2: Water Jug Problem

- **State:** (x, y) where x=gal in 3-jug, y=gal in 4-jug
- **Actions:** Fill(jug), Empty(jug), Pour(jug1, jug2)
- **Goal:** (x, 2) - get exactly 2 gallons in 4-gallon jug
- **Initial:** (0, 0) - both empty

### ğŸ”‘ Key Concepts

- **Solution:** Sequence of actions from Initial to Goal
- **Path Cost:** g(n) = sum of action costs
- **State Space Graph:** Nodes=states, Edges=actions

---

## SECTION 2: UNINFORMED SEARCH - BFS & DFS
**Coverage: Slides 11-25**

### ğŸ¯ MNEMONIC: "BUD-DI Searches Blindly"

- **B** - Breadth-First Search
- **U** - Uniform-Cost Search
- **D** - Depth-First Search
- **D** - Depth-Limited Search
- **I** - Iterative-Deepening Search

---

### ğŸ” BREADTH-FIRST SEARCH (BFS)

**Visual:** Burger Line (Queue - FIFO)  
**Mnemonic:** "Big Fat Storage Queen"

#### Data Structure: QUEUE (First In First Out)

```
â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”
â”‚ A â”‚ B â”‚ C â”‚ğŸ”â”‚ â†’ Remove from front
â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜
  â†‘ Add to back
```

#### Algorithm:
1. OPEN = [start]
2. Remove FIRST from OPEN (FIFO)
3. If GOAL â†’ return path
4. Add children to END of OPEN
5. Repeat from step 2

#### Properties: "COTS"
- âœ… **Complete:** YES (finds solution if exists)
- âœ… **Optimal:** YES* (only for uniform cost!)
- â±ï¸ **Time:** O(b^d) - exponential
- ğŸ’¾ **Space:** O(b^d) - HUGE! ğŸ’¥

**Where:** b = branching factor, d = depth of goal

#### âš ï¸ QUIZ TRAP
**Q:** "Is BFS always optimal?"  
**A:** NO! Only for UNIFORM cost. Use UCS for varying costs.

---

### ğŸ“š DEPTH-FIRST SEARCH (DFS)

**Visual:** Book Stack (Stack - LIFO)  
**Mnemonic:** "Deep Diver, Small Bag"

#### Data Structure: STACK (Last In First Out)

```
  â•‘ C â•‘ â† Remove from top
  â•‘ B â•‘
  â•‘ A â•‘
  â•šâ•â•â•â•
    â†‘ Add to top
```

#### Algorithm:
1. OPEN = [start]
2. Remove LAST from OPEN (LIFO)
3. If GOAL â†’ return path
4. Add children to FRONT of OPEN
5. Repeat from step 2

#### Properties: "COTS"
- âŒ **Complete:** NO (can loop infinitely)
- âŒ **Optimal:** NO
- â±ï¸ **Time:** O(b^m) - exponential in max depth
- ğŸ’¾ **Space:** O(bm) - LINEAR! â­ BEST SPACE

**Where:** m = maximum depth of tree

#### âš ï¸ QUIZ TRAP
**Q:** "DFS saves space, so use it always?"  
**A:** NO! Not complete - might never find solution.

---

### ğŸ’¡ MEMORY TRICK: "Q-S" (Queue-Stack)
- BFS = **Q** for Queue
- DFS = **S** for Stack

---

## SECTION 3: UNINFORMED SEARCH - UCS, DLS, IDS
**Coverage: Slides 26-35**

---

### ğŸ† UNIFORM-COST SEARCH (UCS)

**Visual:** VIP Priority Line  
**Mnemonic:** "Use Cost to Sort"

#### Data Structure: PRIORITY QUEUE (by path cost g(n))

```
[3:A] [5:B] [8:C] â†’ Always remove lowest cost
  â†‘ lowest
```

#### Algorithm:
1. OPEN = [(start, cost=0)]
2. Remove LOWEST COST from OPEN
3. If GOAL â†’ return path
4. For each child:
   - g(child) = g(parent) + edge_cost
   - Add (child, g(child)) to OPEN
5. Repeat from step 2

#### ğŸ”‘ KEY FORMULA:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ g(child) = g(parent) + cost(edge)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Properties:
- âœ… **Complete:** YES (if step cost > Îµ > 0)
- âœ… **Optimal:** ALWAYS âœ“ (no conditions!)
- â±ï¸ **Time:** O(b^d)
- ğŸ’¾ **Space:** O(b^d)

#### Difference from BFS:
- **BFS** â†’ expands by DEPTH
- **UCS** â†’ expands by COST (cheapest first)

#### âš ï¸ QUIZ TRAP
**Q:** "When to use UCS vs BFS?"  
**A:** Varying edge costs â†’ UCS. Uniform costs â†’ BFS/IDS.

---

### ğŸ“ DEPTH-LIMITED SEARCH (DLS)

**Concept:** DFS with maximum depth limit L

#### Algorithm:
DFS but stop at depth L

#### Properties:
- âŒ **Complete:** NO (only if solution at depth â‰¤ L)
- âŒ **Optimal:** NO
- â±ï¸ **Time:** O(b^L)
- ğŸ’¾ **Space:** O(bL)

**Use Case:** When you know maximum depth

---

### ğŸªœ ITERATIVE DEEPENING SEARCH (IDS) â­â­ MOST IMPORTANT!

**Visual:** Climb Ladder (one step at a time)  
**Mnemonic:** "Increment Depth Slowly"

#### Algorithm:

```python
for depth_limit = 0, 1, 2, 3, ... âˆ:
    result = DLS(depth_limit)
    if result found:
        return result
```

#### Visual Process:
- **Depth 0:** Check root only
- **Depth 1:** Check root + children
- **Depth 2:** Check root + children + grandchildren
- ...until goal found

#### Properties: "COTS"
- âœ… **Complete:** YES
- âœ… **Optimal:** YES* (for uniform cost)
- â±ï¸ **Time:** O(b^d)
- ğŸ’¾ **Space:** O(bd) â† AMAZING! â­â­

#### WHY IDS IS THE BEST:
âœ“ BFS optimality (finds shallowest)  
âœ“ DFS space efficiency (linear memory)  
âœ“ Best of both worlds!

#### âš ï¸ QUIZ TRAP
**Q:** "IDS wastes time repeating?"  
**A:** Only ~11% overhead! Saves EXPONENTIAL space.

**Example:** For b=10, d=5:
- BFS: 100,000 nodes in memory
- IDS: 50 nodes in memory (2000x better!)

---

## SECTION 4: COMPLEXITY COMPARISON TABLE
**Coverage: Slides 36-40** â­ MEMORIZE!

| Algo | Data Structure | Complete | Optimal | Time    | Space   | Best For      |
|------|---------------|----------|---------|---------|---------|---------------|
| BFS  | Queue         | âœ“        | âœ“*      | O(b^d)  | O(b^d)ğŸ’¥ | Shallowest    |
| UCS  | Priority      | âœ“        | âœ“       | O(b^d)  | O(b^d)ğŸ’¥ | Varying cost  |
| DFS  | Stack         | âœ—        | âœ—       | O(b^m)  | O(bm)â­  | Memory limit  |
| DLS  | Stack         | âœ—        | âœ—       | O(b^L)  | O(bL)    | Known depth   |
| IDS  | Stack         | âœ“        | âœ“*      | O(b^d)  | O(bd)â­â­ | Unknown depth |

**\*Optimal for uniform cost only**

### ğŸ“ Notation:
- **b** = branching factor (avg children per node)
- **d** = depth of shallowest goal
- **m** = maximum depth of tree
- **L** = depth limit

### ğŸ’¡ MEMORY TRICK: "Q-S-P-S-S"
BFS=Queue, DFS=Stack, UCS=Priority, DLS=Stack, IDS=Stack

### ğŸ”‘ KEY INSIGHT:

**Exponential O(b^d) vs Linear O(bd) is MASSIVE!**

**Example:** b=10, d=12
- **BFS:** 10^12 = 1 TRILLION nodes ğŸ’€
- **IDS:** 120 nodes âœ“ (8 billion times better!)

---

## SECTION 5: HEURISTIC FUNCTIONS
**Coverage: Slides 41-50**

### ğŸ“– Definition:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ h(n) = Estimated cost from node n to goal   â”‚
â”‚                                              â”‚
â”‚ Visual: ğŸ§­ Compass pointing to treasure      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### âœ… Requirements:
1. **h(goal) = 0** â† ALWAYS! (at goal, no cost remaining)
2. **h(n) â‰¥ 0** â† Non-negative for all nodes
3. **Domain-specific** (depends on problem)

---

### ğŸ“Œ Common Heuristics

#### 1ï¸âƒ£ ROMANIA PROBLEM: Straight-Line Distance

**h(n) = Euclidean distance from current city to Bucharest**

```
City A ğŸ“â”â”â”â”â”â”â”â”â”â”ğŸ Bucharest
         â†‘ straight line
```

**Why admissible?** Roads â‰¥ straight line (can't go through buildings)

---

#### 2ï¸âƒ£ 8-PUZZLE: Two Common Heuristics

##### hâ‚ = Number of Misplaced Tiles

```
Current:  [1 2 3]    Goal: [1 2 3]
          [4 _ 6]          [4 5 6]
          [7 5 8]          [7 8 _]

Misplaced: 5, 6, 8 â†’ hâ‚ = 3 tiles wrong
```

##### hâ‚‚ = Manhattan Distance (Sum of distances)

**For each tile:** |x_current - x_goal| + |y_current - y_goal|

**Example for tile 5:**
- Current position: (2, 1)
- Goal position: (1, 1)
- Distance = |2-1| + |1-1| = 1

Sum for all tiles â†’ hâ‚‚

**KEY FACT:** hâ‚‚ dominates hâ‚ (hâ‚‚ â‰¥ hâ‚ for all states, so hâ‚‚ is more informed)

---

### ğŸ“ MANHATTAN DISTANCE FORMULA â­ MEMORIZE!

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ h = |xâ‚ - xâ‚‚| + |yâ‚ - yâ‚‚|        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Visual:** Taxi-cab distance ğŸš•

```
    (xâ‚,yâ‚)
       â”Œâ”€â”€â”€â”€â”€â”
       â”‚  â†’  â”‚ horizontal: |xâ‚-xâ‚‚|
       â””â”€â”€â”  â”‚
          â†“  â”‚ vertical: |yâ‚-yâ‚‚|
          â””â”€â”€â”˜
        (xâ‚‚,yâ‚‚)
```

**Example:** (1,2) to (4,6)
= |1-4| + |2-6| = 3 + 4 = **7**

---

### ğŸ¯ MNEMONIC: "ACORN" for Heuristic Properties

- **A** - Admissible (h â‰¤ h*)
- **C** - COnsistent (triangle inequality)
- **O** - Optimistic (never overestimate)
- **R** - Remain non-negative
- **N** - Null at goal (h(goal) = 0)

---

## SECTION 6: A* ALGORITHM & ADMISSIBILITY
**Coverage: Slides 51-60** â­â­â­

### ğŸŒŸ A* SEARCH - THE BEST INFORMED ALGORITHM

#### EVALUATION FUNCTION â­ MOST IMPORTANT FORMULA!

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                 â”‚
â”‚      f(n) = g(n) + h(n)        â”‚
â”‚             â†‘       â†‘           â”‚
â”‚           past   future         â”‚
â”‚           cost   estimate       â”‚
â”‚                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Where:**
- **g(n)** = actual cost from start to node n
- **h(n)** = estimated cost from n to goal
- **f(n)** = estimated total cost through n

#### Visual Analogy: GPS Navigation ğŸ§­

```
START â”â”â”â”â”â”â” YOU â”â”â”â”â”â”â” GOAL
       g(n)          h(n)
    (distance      (distance
     traveled)     remaining)
```

---

### ğŸ”„ Algorithm:

1. OPEN = [(start, g=0, h=h(start), f=h(start))]
2. Remove node with LOWEST f(n) from OPEN
3. If node is GOAL â†’ return path
4. For each child of node:
   - g(child) = g(node) + cost(node â†’ child)
   - h(child) = heuristic(child)
   - f(child) = g(child) + h(child)
   - Add to OPEN (or update if better path found)
5. Repeat from step 2

---

### ğŸ“Š Properties:

- âœ… **Complete:** YES
- âœ… **Optimal:** YES (if h is admissible)
- â±ï¸ **Time:** O(b^d)
- ğŸ’¾ **Space:** O(b^d) â† Main drawback

---

### ğŸ¯ ADMISSIBLE HEURISTIC â­ CRUCIAL CONCEPT

**Definition:** "Never Overpromise" ğŸ¤¥âŒ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  h(n) â‰¤ h*(n)  for ALL nodes n  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Where:** h*(n) = true optimal cost from n to goal

**Meaning:** Heuristic NEVER OVERESTIMATES actual cost

#### Visual: Speed Limit Analogy

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  [55 mph] â† Your estimate â”‚
â”‚     â‰¤                      â”‚
â”‚  [60 mph] â† Actual limit  â”‚
â”‚                            â”‚
â”‚  55 â‰¤ 60 âœ“ Admissible!    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### WHY IT MATTERS:
**If h is admissible â†’ A* finds OPTIMAL solution**

#### âš ï¸ QUIZ TRAP
**Q:** "Can A* with inadmissible h find optimal?"  
**A:** YES, by luck! But NOT GUARANTEED. Admissibility GUARANTEES optimality.

---

### ğŸƒğŸ’¨ GREEDY BEST-FIRST SEARCH (for comparison)

**Evaluation:** f(n) = h(n) ONLY (ignores g!)

**Visual:** "Sprint to goal blindly"

#### Properties:
- âŒ **Complete:** NO (can loop)
- âŒ **Optimal:** NO (ignores path cost)
- âš¡ **Fast but risky**

#### Difference from A*:
- **Greedy** = h only â†’ Can be misled
- **A*** = g + h â†’ Balanced, optimal

---

## SECTION 7: CONSISTENCY & DOMINANCE
**Coverage: Slides 51-60 continued**

### ğŸ“ CONSISTENT (MONOTONIC) HEURISTIC â­ STRONGER PROPERTY

**Definition:** "Triangle Inequality"

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  h(n) â‰¤ c(n, a, n') + h(n')             â”‚
â”‚  for all n, a, n'                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Where:** c(n, a, n') = cost of action a from n to n'

#### Visual Triangle:

```
      n
     /|\
  h / | \ c(n,a,n')
   /  |  \
  /   |   \
 G    |    n'
      | h(n')
```

**Meaning:** Estimated cost from n cannot be more than (actual step cost + estimate from next node)

---

### ğŸ”— RELATIONSHIP WITH ADMISSIBILITY:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Consistent â†’ Admissible (ALWAYS) âœ“ â”‚
â”‚  Admissible â†› Consistent (NOT) âœ—    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Mnemonic:** "C before A in alphabet" (Consistent is stronger, implies Admissible)

---

### ğŸ¯ KEY CONSEQUENCE:

If h is consistent:
- f-values are NON-DECREASING along any path
- A* never re-expands nodes (with graph search)
- First path to goal is optimal

---

### ğŸ“ˆ DOMINANCE

**Definition:** hâ‚‚ dominates hâ‚ if:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  hâ‚‚(n) â‰¥ hâ‚(n) for ALL n      â”‚
â”‚  AND both are admissible       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Consequence:**
- hâ‚‚ is MORE INFORMED
- A* with hâ‚‚ expands FEWER nodes

**Example (8-Puzzle):**
- hâ‚ = Misplaced tiles
- hâ‚‚ = Manhattan distance

For any state: hâ‚‚(n) â‰¥ hâ‚(n)  
Therefore: hâ‚‚ dominates hâ‚  
**Result:** Use hâ‚‚ for better performance!

---

## SECTION 8: MEMORY-BOUNDED HEURISTICS
**Coverage: Slides 61-65**

**Problem:** A* uses O(b^d) space - too much for large problems!  
**Solution:** Memory-bounded variants

---

### ğŸ”„ ITERATIVE DEEPENING A* (IDA*)

**Concept:** IDS + A* = IDA*

#### Algorithm:

```python
threshold = h(start)
while True:
    result = DFS with f-cost limit = threshold
    if result is goal:
        return result
    if result is âˆ:
        return failure
    threshold = minimum f-cost that exceeded limit
```

#### Difference from IDS:
- **IDS:** Cutoff by DEPTH
- **IDA*:** Cutoff by F-COST

#### Properties:
- âœ… **Complete:** YES
- âœ… **Optimal:** YES (if h admissible)
- â±ï¸ **Time:** Similar to A*
- ğŸ’¾ **Space:** O(bd) â­ BEST!

**Use when:** Large search space, limited memory

---

### ğŸ” RECURSIVE BEST-FIRST SEARCH (RBFS)

**Key Idea:** Remember f-value of best alternative path

#### Properties:
- âœ… **Complete:** YES
- âœ… **Optimal:** YES (if h admissible)
- ğŸ’¾ **Space:** O(bd) â­
- âš ï¸ May re-expand nodes (unlike A*)

#### Comparison:
- **IDA*** = Simpler, easier to understand
- **RBFS** = More sophisticated, less re-expansion

---

## SECTION 9: LOCAL SEARCH - HILL CLIMBING
**Coverage: Slides 66-75**

**Philosophy:** Don't care about PATH, only SOLUTION  
**Applications:** Optimization in HUGE spaces (>10^30,000 states)

### ğŸ¯ MNEMONIC: "HiSaGa - High School Saga"

- **Hi** - Hill Climbing
- **Sa** - Simulated Annealing
- **Ga** - Genetic Algorithm

---

### ğŸ§— HILL CLIMBING

**Visual:** Climb nearest peak  
**Mnemonic:** "Greedy Local Search"

#### State-Space Landscape:

```
     â›°ï¸ Local Max              ğŸ”ï¸ Global Max
     /\    (STUCK!)             /\
    /  \                       /  \
   /    \________             /    \
  /              \___________/      \
 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
 â†‘ Start here
```

#### Algorithm:

```python
current = initial_state
loop:
    neighbor = highest_valued_neighbor(current)
    if neighbor.value â‰¤ current.value:
        return current  # Local maximum reached
    current = neighbor
```

---

### âš ï¸ PROBLEMS: "LRP" â­ MEMORIZE!

#### L - Local Maxima
Gets stuck on small peaks, can't see higher ones

#### R - Ridges
Long, flat slopes that are difficult to navigate

#### P - Plateaus (two types)
- **Shoulders:** Flat regions with upward exit
- **Flat maxima:** Flat regions at peak

---

### ğŸ“Š Properties:

- âŒ **Complete:** NO (gets stuck)
- âŒ **Optimal:** NO (local maximum â‰  global)
- ğŸ’¾ **Memory:** O(1) - single state â­
- âš¡ **Speed:** FAST

---

### ğŸ”€ Variants:

#### 1. Stochastic Hill Climbing
- Pick random uphill move
- Less optimal but faster

#### 2. First-Choice Hill Climbing
- Generate neighbors randomly
- Accept first improvement
- Good for large branching factor

#### 3. Random-Restart Hill Climbing
- Try multiple random starts
- Pick best result
- Increases chance of finding global max

---

## SECTION 10: SIMULATED ANNEALING
**Coverage: Slides 66-75 continued**

### ğŸ”¥â„ï¸ SIMULATED ANNEALING (SA)

**Visual:** Temperature Chef  
**Mnemonic:** "Sometimes Accept Worse"

**Key Idea:** Accept worse moves with probability to escape local max

---

### â­ ACCEPTANCE FORMULA - MOST IMPORTANT!

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  P(accept worse) = e^(Î”E/T)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Where:**
- **Î”E** = new_value - current_value (negative if worse)
- **T** = temperature (decreases over time)

**Mnemonic:** "e to the Delta E over T"

---

### ğŸ”„ Algorithm:

```python
current = initial_state
for t = 1 to âˆ:
    T = schedule(t)  # Temperature decreases
    if T = 0:
        return current
    next = random_successor(current)
    Î”E = next.value - current.value
    
    if Î”E > 0:  # Better move
        current = next
    else:  # Worse move
        with probability e^(Î”E/T):
            current = next  # Accept anyway!
```

---

### ğŸŒ¡ï¸ TEMPERATURE CONTROL:

#### High T (early): ğŸ”¥
- High probability of accepting worse moves
- EXPLORATION phase
- Jumps around search space

#### Low T (late): â„ï¸
- Low probability of accepting worse moves
- EXPLOITATION phase
- Converges to solution

---

### ğŸ“‰ COOLING SCHEDULES â­ MEMORIZE AT LEAST ONE!

#### 1. Geometric (Most Common)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ T(k+1) = Î± Ã— T(k)         â”‚
â”‚ where Î± âˆˆ [0.8, 0.99]     â”‚
â”‚ Typical: Î± = 0.95         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 2. Linear
T(k) = Tâ‚€ - kÂ·Î”T

#### 3. Logarithmic
T(k) = C / ln(k+2)

#### 4. Exponential
T(k) = Tâ‚€ Ã— Î±^k

---

### ğŸ§® EXAMPLE CALCULATION:

**Given:**
- Current solution value = 100
- New solution value = 90 (worse!)
- Temperature T = 50

**Calculate:**
- Î”E = 90 - 100 = -10
- P = e^(-10/50) = e^(-0.2) â‰ˆ **0.82**

**Interpretation:** 82% chance to accept this worse move!

---

### ğŸ“Š Properties:

- âœ… **Complete:** YES (with proper schedule)
- âœ… **Optimal:** Probabilistically YES (given infinite time)
- ğŸ’¾ **Memory:** O(1) â­
- âœ… **Escapes local maxima:** YES

**Use for:** TSP, scheduling, VLSI layout, continuous optimization

---

## SECTION 11: GENETIC ALGORITHMS
**Coverage: Slides 76-82**

### ğŸ§¬ GENETIC ALGORITHMS (GA)

**Visual:** Evolution Search  
**Mnemonic:** "GERMS" â­ MOST IMPORTANT!

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ G - Generate initial population  â”‚
â”‚ E - Evaluate fitness             â”‚
â”‚ R - Reproduce (crossover parents)â”‚
â”‚ M - Mutate (random changes)      â”‚
â”‚ S - Select survivors             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### ğŸ”‘ KEY CONCEPTS:

#### Individual/Chromosome
Encoded solution  
**Example:** [1,0,1,1,0,1,0,0] (binary string)

#### Gene
Single element of chromosome  
**Example:** Each bit is a gene

#### Population
Set of N individuals  
**Example:** 100 different binary strings

#### Fitness
Quality measure (how good the solution is)  
**Example:** fitness([1,0,1,1,0,1,0,0]) = 8.5

---

### ğŸ”„ Algorithm:

```
1. Generate initial population randomly

2. Repeat until termination:

   a) Evaluate fitness of each individual

   b) Select parents (based on fitness)
      - Roulette wheel, tournament, rank, etc.

   c) Reproduce (crossover)
      - Combine parent chromosomes â†’ children

   d) Mutate offspring (with small probability)
      - Randomly change genes

   e) Form new population from offspring

3. Return best individual found
```

---

### ğŸ° SELECTION METHODS:

#### 1. ROULETTE WHEEL SELECTION â­

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ P(select i) = fitness(i) / Î£(fitness)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Example:**
- Individual A: fitness = 10 â†’ P = 10/25 = **40%**
- Individual B: fitness = 8  â†’ P = 8/25  = **32%**
- Individual C: fitness = 5  â†’ P = 5/25  = **20%**
- Individual D: fitness = 2  â†’ P = 2/25  = **8%**

#### 2. TOURNAMENT SELECTION
- Pick k random individuals
- Select best among them
- Repeat for desired number

---

### ğŸ§¬ CROSSOVER (REPRODUCTION):

#### 1. SINGLE-POINT CROSSOVER â­

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Parent 1: A B C | D E F    â”‚
â”‚ Parent 2: a b c | d e f    â”‚
â”‚            â†‘ cut point     â”‚
â”‚ Child 1:  A B C | d e f    â”‚
â”‚ Child 2:  a b c | D E F    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 2. TWO-POINT CROSSOVER

```
Parent 1: A B | C D E | F
Parent 2: a b | c d e | f
Child 1:  A B | c d e | F
Child 2:  a b | C D E | f
```

#### 3. UNIFORM CROSSOVER
Randomly pick each gene from either parent

---

### ğŸ§¬ MUTATION:

**Probability:** Typically 0.01 to 0.1 (1-10%)

**Binary strings:** Flip bit

```
Before: 1 0 1 1 0 1 0 0
After:  1 0 1 0 0 1 0 0  (4th bit flipped)
             â†‘
```

**Purpose:**
- Maintain diversity in population
- Prevent premature convergence
- Escape local optima

---

### ğŸ“Š Properties:

- ğŸ’¾ **Memory:** O(N) where N = population size
- âœ…âœ… **Escapes local optima:** BEST
- ğŸ“ˆ **Diversity:** HIGH (population-based)
- ğŸŒ **Speed:** SLOW (many fitness evaluations)
- âš¡ **Parallelizable:** YES

**Use for:** Multi-objective optimization, complex landscapes, feature selection, neural architecture search

---

## SECTION 12: LOCAL SEARCH COMPARISON
**Coverage: Slides 83-86**

| Criterion     | Hill       | SA         | GA         | Key Difference    |
|---------------|------------|------------|------------|-------------------|
| Search Width  | Narrow     | Medium     | Broad      | GA explores most  |
| Accept worse? | NO         | Prob.e^Î”E/T| YES        | HC never does     |
| Escape local? | STUCK      | Escapes    | Avoids     | HC worst          |
| Memory        | O(1)       | O(1)       | O(N)       | GA needs pop      |
| Speed/iter    | Fastâš¡     | MediumğŸš¶   | SlowğŸŒ     | HC fastest        |
| Parameters    | Few        | Medium     | Many       | GA most complex   |
| Diversity     | None       | Low        | High       | Population wins   |
| Best for      | Convex     | TSP        | Multi-obj  | Match to problem  |

---

### ğŸ“‹ DECISION GUIDE:

#### Use HILL CLIMBING when:
âœ“ Smooth, convex landscape  
âœ“ Quick prototyping needed  
âœ“ Local optimum acceptable  
âœ“ Memory extremely limited

#### Use SIMULATED ANNEALING when:
âœ“ TSP, scheduling problems  
âœ“ Single objective  
âœ“ Rugged landscape with local optima  
âœ“ Memory limited (can't use GA)

#### Use GENETIC ALGORITHM when:
âœ“ Multi-objective optimization  
âœ“ Very complex search space  
âœ“ Building-block structure exists  
âœ“ Parallel processing available  
âœ“ Memory not a constraint

---

## SECTION 13: AND-OR SEARCH TREES
**Coverage: Slides 83-86**

**Context:** Non-deterministic environments (outcomes uncertain)

---

### ğŸ”€ NODE TYPES:

#### OR NODE (â—‹): Pick ANY child to succeed

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Cost = MIN(children costs)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Visual:** ğŸ• Pizza toppings  
"Pick Pepperoni OR Mushroom OR Veggie"  
Cost = min($8, $7, $9) = **$7** (choose cheapest)

---

#### AND NODE (âŒ’): ALL children must succeed

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Cost = SUM(children costs)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Visual:** ğŸ•ğŸ¥¤ğŸ° Complete meal  
"Need Pizza AND Drink AND Dessert"  
Cost = $8 + $2 + $3 = **$13** (add all costs)

---

### ğŸŒ³ EXAMPLE TREE:

```
          OR (root)
         /  \
       AND   OR
       /\     |
      3  4    5

Solution:
AND node: 3 + 4 = 7
OR nodes: min(7, 5) = 5
Root cost: 5
```

---

### ğŸ’¡ MEMORY TRICKS:

- **OR** = "Options" â†’ Choose best â†’ **MIN**
- **AND** = "All" â†’ Need everything â†’ **SUM**

---

### âš ï¸ QUIZ TRAP
**Q:** "Do AND nodes take minimum?"  
**A:** NO! AND = SUM (need all), OR = MIN (pick best)

---

## ğŸ¯ EMERGENCY QUICK REFERENCE

### Top 25 Must-Know Facts

1. BFSâ†’Queue, DFSâ†’Stack, UCSâ†’Priority, IDSâ†’Stack
2. f(n) = g(n) + h(n) [A* formula]
3. Admissible: h(n) â‰¤ h*(n)
4. Consistent: h(n) â‰¤ c(n,a,n') + h(n')
5. Consistent â†’ Admissible (ALWAYS)
6. IDS space: O(bd) â­ BEST
7. UCS ALWAYS optimal (no conditions)
8. BFS optimal ONLY for uniform cost
9. DFS NOT complete (infinite loops)
10. Manhattan = |xâ‚-xâ‚‚| + |yâ‚-yâ‚‚|
11. h(goal) = 0 ALWAYS
12. Greedy = h(n) only (NOT optimal)
13. SA formula: P = e^(Î”E/T)
14. Alpha-Beta: prune when Î± â‰¥ Î²
15. AND = SUM, OR = MIN
16. Hill climbing: stuck at local maxima (LRP)
17. GA: GERMS (Generate, Evaluate, Reproduce, Mutate, Select)
18. Roulette wheel: P(i) = fitness(i)/Î£fitness
19. IDA*: IDS with f-cost cutoff
20. hâ‚‚ dominates hâ‚ if hâ‚‚ â‰¥ hâ‚ for all n
21. Î²-cutoff at MIN, Î±-cutoff at MAX
22. Search problem: SAGI-H
23. Uninformed: BUD-DI
24. Local search: HiSaGa
25. Heuristics: ACORN

---

### âš ï¸ Common Traps

- âŒ "BFS always optimal" â†’ NO (uniform cost only)
- âŒ "Admissible â†’ Consistent" â†’ NO (other way!)
- âŒ "DFS best space so use it" â†’ NO (not complete!)
- âŒ "Higher h is better" â†’ NO (must be â‰¤ h*)
- âŒ "IDS wastes time" â†’ NO (11% overhead, saves exponential space)
- âŒ "AND nodes take MIN" â†’ NO (AND=SUM, OR=MIN)

---
