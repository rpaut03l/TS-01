# ğŸŒ³ Decision Tree Theory

> **Topic:** Decision Tree Learning using Entropy & Information Gain

---

## ğŸ“‹ Table of Contents

1. [Introduction](#1-introduction)
2. [The Loan Dataset](#2-the-loan-dataset)
3. [A Decision Tree from the Loan Data](#3-a-decision-tree-from-the-loan-data)
4. [Using the Decision Tree](#4-using-the-decision-tree)
5. [Is the Decision Tree Unique?](#5-is-the-decision-tree-unique)
6. [From Tree to Rules](#6-from-tree-to-rules)
7. [Algorithm for Decision Tree Learning](#7-algorithm-for-decision-tree-learning)
8. [The Formal Algorithm](#8-the-formal-algorithm)
9. [Choosing an Attribute to Partition](#9-choosing-an-attribute-to-partition)
10. [Information Theory](#10-information-theory)
11. [Entropy Measure](#11-entropy-measure)
12. [Entropy: Worked Examples](#12-entropy-worked-examples)
13. [Information Gain](#13-information-gain)
14. [Full Worked Example â€” Loan Dataset](#14-full-worked-example--loan-dataset)
15. [Building the Final Tree](#15-building-the-final-tree)
16. [Handling Continuous Attributes](#16-handling-continuous-attributes)
17. [Continuous Attribute Example](#17-continuous-attribute-example)
18. [Avoiding Overfitting](#18-avoiding-overfitting)
19. [Overfitting Example](#19-overfitting-example)
20. [Other Issues in Decision Tree Learning](#20-other-issues-in-decision-tree-learning)
21. [Cheat Sheet and Mnemonics](#21-cheat-sheet-and-mnemonics)
22. [Formula Quick Reference](#22-formula-quick-reference)

---

## 1. Introduction

> *Slide 15 â€” CS583, Bing Liu, UIC*

Decision tree learning is one of the most **widely used techniques for classification**.

```
Key properties:
  âœ… Competitive accuracy with other methods
  âœ… Very efficient to train and predict
  âœ… Produces a human-readable model (a tree)
  âœ… C4.5 by Ross Quinlan is the best-known system
```

**What is a Decision Tree?**

The classification model is a **tree structure** where:

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Root Node    â”‚  â† First split (best attribute)
                    â”‚  (Attribute?) â”‚
                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             val1         val2        val3
               â”‚           â”‚           â”‚
          â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”´â”€â”€â”€â”€â”  â”Œâ”€â”€â”´â”€â”€â”€â”€â”€â”
          â”‚Decision â”‚  â”‚  Leaf  â”‚  â”‚Decisionâ”‚
          â”‚  Node   â”‚  â”‚(Class) â”‚  â”‚  Node  â”‚
          â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
           â”Œâ”€â”€â”€â”´â”€â”€â”€â”               â”Œâ”€â”€â”€â”´â”€â”€â”€â”
         true    false           true    false
           â”‚       â”‚               â”‚       â”‚
        â”Œâ”€â”€â”´â”€â”  â”Œâ”€â”€â”´â”€â”         â”Œâ”€â”€â”´â”€â”  â”Œâ”€â”€â”´â”€â”
        â”‚LEAFâ”‚  â”‚LEAFâ”‚         â”‚LEAFâ”‚  â”‚LEAFâ”‚
        â”‚Yes â”‚  â”‚ No â”‚         â”‚Yes â”‚  â”‚ No â”‚
        â””â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”˜

Legend:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”  = Decision Node (internal) â€” asks a question
  LEAF       = Leaf Node â€” gives the final class label
```

**Key Terminology:**

| Term | Definition |
|---|---|
| **Root Node** | The topmost decision node; the first attribute tested |
| **Decision Node** | An internal node that tests an attribute and branches |
| **Leaf Node** | A terminal node that holds a class label (the answer) |
| **Branch / Edge** | A path from a node labelled with an attribute value |
| **Path** | A sequence of nodes from root to leaf |
| **Depth** | Number of edges from root to deepest leaf |

[ğŸ” Back to Top](#-table-of-contents)

---

## 2. The Loan Dataset

> *Slides 16 and 24 â€” The Loan Data (reproduced)*

15 applicants, 4 features, binary outcome (Approve loan or not).

| ID | Age | Has Job | Own House | Credit Rating | Class |
|---|---|---|---|---|---|
| 1 | young | false | false | fair | **No** |
| 2 | young | false | false | good | **No** |
| 3 | young | true | false | good | **Yes** |
| 4 | young | true | true | fair | **Yes** |
| 5 | young | false | false | fair | **No** |
| 6 | middle | false | false | fair | **No** |
| 7 | middle | false | false | good | **No** |
| 8 | middle | true | true | good | **Yes** |
| 9 | middle | false | true | excellent | **Yes** |
| 10 | middle | false | true | excellent | **Yes** |
| 11 | old | false | true | excellent | **Yes** |
| 12 | old | false | true | good | **Yes** |
| 13 | old | true | false | good | **Yes** |
| 14 | old | true | false | excellent | **Yes** |
| 15 | old | false | false | fair | **No** |

```
Summary:
  Total examples : 15
  Approved (Yes) :  9
  Rejected (No)  :  6
  Features       :  4  (Age, Has Job, Own House, Credit Rating)

Attribute types:
  Age           categorical   {young, middle, old}
  Has Job       boolean       {true, false}
  Own House     boolean       {true, false}
  Credit Rating categorical   {fair, good, excellent}
```

[ğŸ” Back to Top](#-table-of-contents)

---

## 3. A Decision Tree from the Loan Data

> *Slide 17 â€” Decision nodes and leaf nodes (classes)*

One possible tree using Age as root (from the slides):

```
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚   Age?   â”‚
                         â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         young             middle               old
           â”‚                 â”‚                   â”‚
     â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
     â”‚  Has job?  â”‚    â”‚ Own house? â”‚    â”‚Credit rating?â”‚
     â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
        â”Œâ”€â”€â”´â”€â”€â”           â”Œâ”€â”€â”´â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”
      true  false       true  false     fair   good  excellent
        â”‚      â”‚          â”‚      â”‚        â”‚      â”‚       â”‚
      Yes     No         Yes    No       No     Yes     Yes
      (2/2)  (3/3)      (3/3) (2/2)    (1/1)  (2/2)   (2/2)
```

**Reading notation (x/x):** `(2/2)` = 2 examples reach this leaf, 2 classified correctly. Every leaf here is 100% confident.

**Two node types side-by-side:**

```
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  DECISION NODE  â”‚  vs  â”‚        LEAF NODE           â”‚
  â”‚                 â”‚      â”‚                            â”‚
  â”‚     Age?        â”‚      â”‚   Class: Yes               â”‚
  â”‚   â†™   â†“   â†˜   â”‚      â”‚   Confidence: 2/2 = 100%   â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   Asks a question           Gives the final answer
```

[ğŸ” Back to Top](#-table-of-contents)

---

## 4. Using the Decision Tree

> *Slide 18 â€” Use the decision tree*

**How to classify a new example:** follow the path from root to leaf.

**New example:** Age=young, Has Job=false, Own House=false, Credit Rating=good

```
  Trace:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚   Age?   â”‚  â†’ Age = young â†’ take "young" branch
  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
  â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
  â”‚  Has job?  â”‚  â†’ Has Job = false â†’ take "false" branch
  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
        â”‚
       No  â† LEAF: Class = No (reject the application)
      (3/3)

  Answer: REJECT
```

**General rule:** at each decision node, match the attribute value â†’ follow that branch â†’ repeat until leaf.

[ğŸ” Back to Top](#-table-of-contents)

---

## 5. Is the Decision Tree Unique?

> *Slide 19 â€” Is the decision tree unique?*

**No.** The slide shows two valid trees for the same data:

```
  TREE (A) â€” Rooted at Age          TREE (B) â€” Rooted at Own House
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€          â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Age? â”‚                          â”‚ Own house? â”‚
        â””â”€â”€â”€â”¬â”€â”€â”€â”˜                          â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”
  Young   Middle   Old                 true           false
    â”‚       â”‚        â”‚                   â”‚               â”‚
  No:3    No:2     No:1               No: 0           No: 6
  Yes:2   Yes:3    Yes:4              Yes: 6          Yes: 3

  (A) No branch is pure              (B) Left branch is PURE!
                                         â†’ TREE B IS BETTER
```

**Key insight from the slide:**

```
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  We want: SMALLER tree  +  ACCURATE tree                        â”‚
  â”‚  Smaller = easier to understand + generalise better             â”‚
  â”‚                                                                 â”‚
  â”‚  BUT: Finding the globally BEST tree is NP-hard.                â”‚
  â”‚  All current tree-building algorithms are HEURISTIC.            â”‚
  â”‚  They make the best LOCAL choice at each step.                  â”‚
  â”‚  Not guaranteed to find the global optimum.                     â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

[ğŸ” Back to Top](#-table-of-contents)

---

## 6. From Tree to Rules

> *Slide 20 â€” From a decision tree to a set of rules*

Every decision tree converts mechanically to IF-THEN rules. **Each root-to-leaf path = one rule.**

```
  THE TREE (from slide):

        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Own house? â”‚
        â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
         â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
       true      false
         â”‚          â”‚
        Yes      â”Œâ”€â”€â”´â”€â”€â”€â”€â”€â”€â”
       (6/6)     â”‚ Has job?â”‚
                 â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
               â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
             true           false
               â”‚               â”‚
              Yes              No
             (5/5)            (4/4)

  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  THE 3 RULES:

  Rule 1 (Path: Own house=true â†’ Yes):
    Own house = true
    â†’ Class = Yes    [support=6/15, confidence=6/6=100%]

  Rule 2 (Path: Own house=false, Has job=true â†’ Yes):
    Own house = false  AND  Has job = true
    â†’ Class = Yes    [support=5/15, confidence=5/5=100%]

  Rule 3 (Path: Own house=false, Has job=false â†’ No):
    Own house = false  AND  Has job = false
    â†’ Class = No     [support=4/15, confidence=4/4=100%]
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

**Support vs Confidence:**

| Metric | Formula | Meaning |
|---|---|---|
| **Support** | (examples covered) / (total) | How often this rule fires |
| **Confidence** | (correctly classified) / (covered) | How accurate when it fires |

[ğŸ” Back to Top](#-table-of-contents)

---

## 7. Algorithm for Decision Tree Learning

> *Slide 21 â€” Algorithm for decision tree learning*

**Basic algorithm: greedy divide-and-conquer**

```
  PROPERTIES:
    Type:       Greedy, divide-and-conquer
    Direction:  Top-down, recursive
    Start:      All training examples at the root
    Split:      Based on impurity function (e.g., information gain)

  FLOW:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  ALL examples at root                                       â”‚
  â”‚              â†“                                              â”‚
  â”‚  Pick BEST attribute â†’ split into subsets                   â”‚
  â”‚              â†“                                              â”‚
  â”‚  For each subset â†’ RECURSE                                  â”‚
  â”‚              â†“                                              â”‚
  â”‚  Stop when a stopping condition is met                      â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Three Stopping Conditions:**

```
  STOP 1 â€” Purity:
    All examples in subset belong to the SAME class
    â†’ Leaf node labelled with that class

  STOP 2 â€” No attributes:
    A is empty (all attributes used up)
    â†’ Leaf node labelled with MAJORITY class

  STOP 3 â€” No examples:
    Subset is empty
    â†’ Leaf node labelled with parent's majority class
```

[ğŸ” Back to Top](#-table-of-contents)

---

## 8. The Formal Algorithm

> *Slide 22 â€” Decision tree learning algorithm (pseudocode)*

```
Algorithm decisionTree(D, A, T)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
INPUT:
  D = training examples at current node
  A = set of available attributes
  T = current tree node being built
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1   IF D contains only examples of the same class ci THEN
2       make T a leaf node labeled class ci
        RETURN                              â† Stopping Condition 1

3   ELSEIF A = empty THEN
4       make T a leaf node labeled with majority class in D
        RETURN                              â† Stopping Condition 2

5   ELSE   (D has mixed classes, attributes still available)

6       p0 = impurityEval(D)               â† baseline entropy

7       FOR each attribute Ai in A DO
8           pi = impurityEval(Ai, D)       â† post-split entropy
9       END

10      Select Ag = attribute with BIGGEST reduction (p0 - pi)

11      IF (p0 - p_Ag) < threshold THEN   â† gain too small
12          make T a leaf with majority class
            RETURN                          â† Pre-pruning stop

13      ELSE
14          Make T a DECISION NODE on Ag
15          Partition D into subsets D1, D2, ..., Dm
            (one subset per value of Ag)

16          FOR each Dj in {D1, ..., Dm} DO
17              IF Dj is not empty THEN
18                  Create child node Tj
19                  decisionTree(Dj, A minus {Ag}, Tj)   â† RECURSE
20              END
21          END
22      END
23  END
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

**Line-by-line plain English:**

| Lines | What happens |
|---|---|
| 1â€“2 | All one class â†’ leaf. Done. |
| 3â€“4 | No attributes left â†’ majority vote â†’ leaf. Done. |
| 6 | Measure current entropy (how impure is D now). |
| 7â€“9 | For each remaining attribute, measure post-split entropy. |
| 10 | Pick the attribute with the biggest entropy drop. |
| 11â€“12 | If best gain is tiny â†’ stop now (pre-pruning). |
| 14â€“15 | Make a decision node; split D into one subset per value. |
| 16â€“21 | For each non-empty subset: recurse (Ag removed from A). |

[ğŸ” Back to Top](#-table-of-contents)

---

## 9. Choosing an Attribute to Partition

> *Slide 23 â€” Choose an attribute to partition data*

```
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  GOAL: Choose the attribute that reduces                    â”‚
  â”‚        impurity as MUCH as possible                         â”‚
  â”‚                                                             â”‚
  â”‚  KEY DEFINITION:                                            â”‚
  â”‚  A subset is PURE if ALL instances belong to the same class â”‚
  â”‚  â†’ entropy = 0                                              â”‚
  â”‚                                                             â”‚
  â”‚  C4.5 HEURISTIC:                                            â”‚
  â”‚  Choose attribute with maximum Information Gain             â”‚
  â”‚  or Gain Ratio (based on information theory)                â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Purity spectrum:**

```
  entropy = 0            entropy â‰ˆ 0.5          entropy = 1
      â”‚                      â”‚                      â”‚
      â–¼                      â–¼                      â–¼
  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ           â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘           â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘
  All one class        Mostly one class       50/50 split
  PURE â€” stop!         Leaning one way        MAX confusion
```

**Choosing the right question:**

```
  Good split â†’ subsets become PURER than before â†’ high gain
  Bad split  â†’ subsets stay as mixed as before  â†’ low gain

  Intuition: "Which question helps me guess the answer fastest?"
```

[ğŸ” Back to Top](#-table-of-contents)

---

## 10. Information Theory

> *Slides 26 and 27 â€” Information theory*

The entropy formula comes from information theory.

```
  CORE IDEA:
  "How much would you pay for advance information about an outcome?"

  Fair coin (50/50):
    â†’ You have NO idea what's coming
    â†’ Advance information is VERY VALUABLE
    â†’ High entropy = high uncertainty

  Rigged coin (heads 99%):
    â†’ You already know the outcome
    â†’ Advance information has little value
    â†’ Low entropy = low uncertainty

  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  Less you know  =  more valuable the info        â”‚
  â”‚  More you know  =  less valuable the info        â”‚
  â”‚  Entropy MEASURES this uncertainty in BITS       â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**What is a bit?**

```
  1 bit = enough information to answer 1 yes/no question
          about which you have NO prior knowledge

  Example â€” fair coin flip:
    Before: you know nothing â†’ 1 bit of uncertainty
    After:  you know the result â†’ 0 bits remaining

  Example â€” 99% heads coin:
    Before: you strongly suspect heads â†’ < 0.08 bits
    After:  almost no new information gained
```

**Why bits instead of dollars?**

Information theory translates the "value of information" from subjective dollars into an objective, universal, mathematical unit: **bits**.

[ğŸ” Back to Top](#-table-of-contents)

---

## 11. Entropy Measure

> *Slide 28 â€” Information theory: Entropy measure*

**The Entropy Formula:**

$$\text{entropy}(D) = -\sum_{j=1}^{|C|} \Pr(c_j) \cdot \log_2 \Pr(c_j)$$

**Every symbol explained:**

```
  entropy(D)         Measure of disorder / impurity in dataset D

  |C|                Number of distinct classes
                     (binary classification: |C|=2)

  j                  Class index  (j = 1, 2, ..., |C|)

  Pr(cj)             Probability of class cj in D
                     = count(class cj) / total examples

  log2 Pr(cj)        Always NEGATIVE (probabilities < 1)
                     The rarer the class, the more negative

  -Pr(cj) * log2Pr   Each class contributes a POSITIVE term
                     (negative times negative = positive)

  Sigma (sum)        Sum contributions from ALL classes
```

**What the values mean:**

```
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  entropy = 0.000   Dataset is PERFECTLY PURE                 â”‚
  â”‚                    All examples are one class                â”‚
  â”‚                    â†’ Make a LEAF node, stop splitting        â”‚
  â”‚                                                              â”‚
  â”‚  entropy = 1.000   Maximum UNCERTAINTY (binary case)         â”‚
  â”‚                    50% positive, 50% negative                â”‚
  â”‚                    â†’ Must keep splitting                     â”‚
  â”‚                                                              â”‚
  â”‚  0 < entropy < 1   Somewhere in between                      â”‚
  â”‚                    Lower = purer = better                    â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Special convention:** `0 Ã— logâ‚‚(0) = 0` (defined, not undefined). A class with 0 examples contributes 0 to entropy.

[ğŸ” Back to Top](#-table-of-contents)

---

## 12. Entropy: Worked Examples

> *Slide 29 â€” Entropy measure: let us get a feeling*

### Case 1 â€” 50/50 split â†’ entropy = 1.0 (maximum)

```
  Pr(positive) = 0.5,   Pr(negative) = 0.5
  log2(0.5) = -1

  entropy = -(0.5 Ã— -1) - (0.5 Ã— -1)
           = 0.5 + 0.5
           = 1.0   â† maximum disorder

  Meaning: Complete uncertainty. You know nothing.
```

### Case 2 â€” 20/80 split â†’ entropy = 0.722

```
  Pr(positive) = 0.2,   Pr(negative) = 0.8

  log2(0.2) = log10(0.2)/0.3010 = -0.699/0.301 = -2.322
  log2(0.8) = log10(0.8)/0.3010 = -0.097/0.301 = -0.322

  entropy = -(0.2 Ã— -2.322) - (0.8 Ã— -0.322)
           = 0.464 + 0.258
           = 0.722

  Meaning: Leaning towards negative, but still some uncertainty.
```

### Case 3 â€” 100/0 split â†’ entropy = 0.0 (pure!)

```
  Pr(positive) = 1.0,   Pr(negative) = 0.0

  log2(1.0) = 0
  0 Ã— log2(0) = 0   (special convention)

  entropy = -(1.0 Ã— 0) - (0)
           = 0.0   â† perfectly pure!

  Meaning: All examples are the same class. Make a leaf node. Stop.
```

### The Slide's Key Takeaway:

```
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  "As the data become purer and purer, the entropy value         â”‚
  â”‚   becomes smaller and smaller. This is useful to us!"           â”‚
  â”‚                                        â€” Bing Liu, UIC          â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Entropy Curve

```
  entropy
   1.0 â”¤              *              â† 50/50 peak
       â”‚          *       *
   0.7 â”¤       *             *       â† 20/80 or 80/20
       â”‚     *                 *
   0.4 â”¤   *                     *
       â”‚  *                       *
   0.1 â”¤ *                         *
       â”‚*                           *
   0.0 *â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€* â† 0/100 or 100/0 (pure)
       0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1.0
                         Pr(positive class)
```

[ğŸ” Back to Top](#-table-of-contents)

---

## 13. Information Gain

> *Slides 30 and 31 â€” Information gain*

**Concept:** How much does splitting on attribute Aáµ¢ reduce entropy?

```
  Gain = entropy BEFORE split  -  entropy AFTER split
       = How messy things were  -  How messy things are now
```

### Formula 1 â€” Entropy of current node:

$$\text{entropy}(D) = -\sum_{j=1}^{|C|} \Pr(c_j) \cdot \log_2 \Pr(c_j)$$

### Formula 2 â€” Expected entropy after splitting on Aáµ¢:

$$\text{entropy}_{A_i}(D) = \sum_{j=1}^{v} \frac{|D_j|}{|D|} \times \text{entropy}(D_j)$$

### Formula 3 â€” Information Gain:

$$\text{gain}(D, A_i) = \text{entropy}(D) - \text{entropy}_{A_i}(D)$$

**Symbol reference:**

| Symbol | Meaning | Example |
|---|---|---|
| `D` | Dataset at current node | All 15 loan records |
| `Aáµ¢` | Attribute being evaluated | "Own House" |
| `v` | Number of distinct values of Aáµ¢ | Own House: v=2 |
| `Dâ±¼` | Subset where Aáµ¢ = its j-th value | Dâ‚ = {Own House=true} |
| `Dâ±¼ / D` | Weight = fraction of data in subset | 6/15 = 0.40 |

**Decision rule:**

```
  â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
  â•‘  Choose the attribute with the HIGHEST gain.                 â•‘
  â•‘  It reduces impurity the most â†’ gives the best split.        â•‘
  â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

**Visual intuition:**

```
  Before split: all 15 examples mixed â†’ entropy = 0.971
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  â—‹ â—‹ â—‹ â—‹ â—‹ â—‹ â— â— â— â— â— â— â— â— â—        â”‚  9 Yes, 6 No
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  After split on Own House:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  Own House=true  â”‚     â”‚  Own House=false           â”‚
  â”‚  â— â— â— â— â— â—     â”‚     â”‚  â—‹ â—‹ â—‹ â—‹ â—‹ â—‹  â— â— â—        â”‚
  â”‚  6 Yes, 0 No     â”‚     â”‚  6 No, 3 Yes               â”‚
  â”‚  entropy = 0.000 â”‚     â”‚  entropy = 0.918           â”‚
  â”‚  PURE!           â”‚     â”‚  Still mixed, recurse      â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  Weighted after = (6/15)Ã—0.000 + (9/15)Ã—0.918 = 0.551
  Gain = 0.971 - 0.551 = 0.420  â† best of all 4 attributes
```

[ğŸ” Back to Top](#-table-of-contents)

---

## 14. Full Worked Example â€” Loan Dataset

> *Slide 32 â€” An example*

### Root Entropy

$$\text{entropy}(D) = -\frac{9}{15}\log_2\frac{9}{15} - \frac{6}{15}\log_2\frac{6}{15} = 0.971$$

```
  Pr(Yes) = 9/15 = 0.6  â†’ -(0.6 Ã— -0.737) = +0.442
  Pr(No)  = 6/15 = 0.4  â†’ -(0.4 Ã— -1.322) = +0.529
  entropy(D) = 0.442 + 0.529 = 0.971
```

---

### Gain for Age (3 values: young / middle / old)

| Age | Yes | No | Total | Entropy |
|---|---|---|---|---|
| young | 2 | 3 | 5 | 0.971 |
| middle | 3 | 2 | 5 | 0.971 |
| old | 4 | 1 | 5 | 0.722 |

$$\text{entropy}_\text{Age}(D) = \frac{5}{15}(0.971) + \frac{5}{15}(0.971) + \frac{5}{15}(0.722) = 0.888$$

$$\text{gain}(D,\text{Age}) = 0.971 - 0.888 = \mathbf{0.083}$$

---

### Gain for Own House (2 values: true / false)

| Own House | Yes | No | Total | Entropy |
|---|---|---|---|---|
| true | 6 | 0 | 6 | **0.000** (pure!) |
| false | 3 | 6 | 9 | 0.918 |

$$\text{entropy}_\text{OwnHouse}(D) = \frac{6}{15}(0) + \frac{9}{15}(0.918) = 0.551$$

$$\text{gain}(D,\text{OwnHouse}) = 0.971 - 0.551 = \mathbf{0.420}$$

---

### Gain for Has Job (2 values: true / false)

| Has Job | Yes | No | Total | Entropy |
|---|---|---|---|---|
| true | 5 | 0 | 5 | **0.000** (pure!) |
| false | 4 | 6 | 10 | 0.971 |

$$\text{entropy}_\text{HasJob}(D) = \frac{5}{15}(0) + \frac{10}{15}(0.971) = 0.647$$

$$\text{gain}(D,\text{HasJob}) = 0.971 - 0.647 = \mathbf{0.324}$$

---

### Gain for Credit Rating (3 values: fair / good / excellent)

| Credit Rating | Yes | No | Total | Entropy |
|---|---|---|---|---|
| fair | 1 | 4 | 5 | 0.722 |
| good | 4 | 2 | 6 | 0.918 |
| excellent | 4 | 0 | 4 | **0.000** (pure!) |

$$\text{entropy}_\text{Credit}(D) = \frac{5}{15}(0.722) + \frac{6}{15}(0.918) + \frac{4}{15}(0) = 0.608$$

$$\text{gain}(D,\text{CreditRating}) = 0.971 - 0.608 = \mathbf{0.363}$$

---

### Gain Comparison â€” from Slide 32

```
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  Attribute           â”‚  Weighted    â”‚  Gain        â”‚
  â”‚                      â”‚  Entropy     â”‚              â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚  Age                 â”‚  0.888       â”‚  0.083       â”‚
  â”‚  Has Job             â”‚  0.647       â”‚  0.324       â”‚
  â”‚  Credit Rating       â”‚  0.608       â”‚  0.363       â”‚
  â”‚  Own House           â”‚  0.551       â”‚  0.420  âœ…   â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  Slide conclusion: "Own house is the best choice for the root."
```

[ğŸ” Back to Top](#-table-of-contents)

---

## 15. Building the Final Tree

> *Slide 33 â€” We build the final tree*

After choosing Own House as root, recurse on each branch:

**Branch 1: Own House = true**
6 Yes, 0 No â†’ entropy = 0 â†’ LEAF: Yes (6/6)

**Branch 2: Own House = false**
3 Yes, 6 No â†’ entropy = 0.918 â†’ not pure â†’ recurse with {Age, Has Job, Credit Rating}
Among these 9 examples, Has Job wins â†’ splits perfectly:
- Has Job = true â†’ 5 Yes, 0 No â†’ LEAF: Yes (5/5)
- Has Job = false â†’ 0 Yes, 4 No â†’ LEAF: No (4/4)

**The final tree (from slide):**

```
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚  Own house?    â”‚   â† ROOT  (gain = 0.420)
                  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         true                          false
           â”‚                              â”‚
        âœ… Yes                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
         (6/6)                   â”‚   Has job?     â”‚
                                 â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        true                          false
                          â”‚                              â”‚
                       âœ… Yes                        âŒ No
                        (5/5)                         (4/4)

  Rules:
    Own house = true                       â†’ Yes  [conf=6/6=100%]
    Own house = false, Has job = true      â†’ Yes  [conf=5/5=100%]
    Own house = false, Has job = false     â†’ No   [conf=4/4=100%]

  Training accuracy: 15/15 = 100%
```

**Slide note:** "We can use information gain ratio to evaluate the impurity as well."

**Gain Ratio formula (C4.5):**

$$\text{GainRatio}(D, A_i) = \frac{\text{gain}(D, A_i)}{\text{SplitInfo}(A_i)}$$

$$\text{SplitInfo}(A_i) = -\sum_{j=1}^{v} \frac{|D_j|}{|D|} \cdot \log_2 \frac{|D_j|}{|D|}$$

[ğŸ” Back to Top](#-table-of-contents)

---

## 16. Handling Continuous Attributes

> *Slide 34 â€” Handling continuous attributes*

Continuous values (temperature, salary, age as a number) cannot have one branch per value. The solution: **binary threshold split**.

**Strategy:**

```
  Instead of: Temp=15 â†’ ..., Temp=18 â†’ ..., Temp=22 â†’ ... (impractical)

  Use a threshold:
    Temp â‰¤ 23.5  â†’  Left branch
    Temp  > 23.5  â†’  Right branch
```

**Algorithm to find best threshold:**

```
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  STEP 1: Sort all values in increasing order                    â”‚
  â”‚          {v1, v2, v3, ..., vr}                                  â”‚
  â”‚                                                                 â”‚
  â”‚  STEP 2: Candidate thresholds = midpoints between adjacent      â”‚
  â”‚          values: t = (vi + vi+1) / 2                            â”‚
  â”‚                                                                 â”‚
  â”‚  STEP 3: For each candidate threshold t:                        â”‚
  â”‚          Split into {x â‰¤ t}  and  {x > t}                       â”‚
  â”‚          Compute information gain (or gain ratio)               â”‚
  â”‚                                                                 â”‚
  â”‚  STEP 4: Choose threshold with MAXIMUM gain                     â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Why midpoints?**

```
  Values:     15   18   22   25   30   35
  Classes:    No   No   No  Yes  Yes  Yes

  Candidates: 16.5  20.0  23.5  27.5  32.5
                            â†‘
                    Best threshold!
                    Sits at the class boundary

  Left  (â‰¤23.5): {15,18,22} â†’ all No  â†’ entropy = 0 (pure!)
  Right  (>23.5): {25,30,35} â†’ all Yes â†’ entropy = 0 (pure!)
  Gain = 1.0 - 0 = 1.0  â† PERFECT SPLIT
```

[ğŸ” Back to Top](#-table-of-contents)

---

## 17. Continuous Attribute Example

> *Slide 35 â€” An example in a continuous space*

The slide shows a 2D dataset (features X, Y) and its decision tree:

```
  DATA SPACE (X-Y plot):                 RESULTING DECISION TREE:
  â–  = Class 1 (black squares)
  â—‹ = Class 2 (white circles)                   â”Œâ”€â”€â”€â”€â”€â”
                                                 â”‚  X  â”‚
  Yâ†‘                                             â””â”€â”€â”¬â”€â”€â”˜
  2.6â”‚â–  â–  â—‹  â—‹   â—‹  â—‹                        â‰¤2  â•±  â•² >2
  2.5â”‚â–  â–   â—‹   â—‹   â—‹                            â•±    â•²
    2â”‚â–  â–   â–   â—‹â—‹ â—‹                         â”Œâ”€â”€â”€â”      â”Œâ”€â”€â”€â”
     â”‚â–  â–  â–  â–  â–  â—‹ â—‹                        â”‚ Y â”‚      â”‚ Y â”‚
     â”‚     â–  â–  â–  â—‹â—‹                         â””â”€â”¬â”€â”˜      â””â”€â”¬â”€â”˜
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’X                   â‰¤2.5 >2.5    â‰¤2 >2
       0    2   3  4                      â–     ...    ...  â—‹

  Decision boundaries are AXIS-ALIGNED
  (horizontal and vertical lines only)
  Each rectangular region = one leaf node
```

**Key property:**

```
  Decision trees create RECTANGULAR regions in feature space.
  Advantage:    Very interpretable
  Disadvantage: Cannot capture diagonal decision boundaries
                naturally â€” may need many splits to approximate
```

[ğŸ” Back to Top](#-table-of-contents)

---

## 18. Avoiding Overfitting

> *Slide 36 â€” Avoid overfitting in classification*

**What is overfitting?**

```
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  A tree may OVERFIT the training data                           â”‚
  â”‚                                                                 â”‚
  â”‚  Symptom:  High accuracy on TRAINING data                       â”‚
  â”‚            Poor accuracy on TEST data / new examples            â”‚
  â”‚                                                                 â”‚
  â”‚  Signs:    Tree is too DEEP                                     â”‚
  â”‚            Too many BRANCHES                                    â”‚
  â”‚            Some branches reflect NOISE or OUTLIERS              â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Two strategies to avoid overfitting:**

```
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  APPROACH        â”‚  HOW IT WORKS                                 â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚  Pre-pruning     â”‚  STOP early before tree grows too deep        â”‚
  â”‚  (Early stop)    â”‚  â€¢ Stop if gain < threshold                   â”‚
  â”‚                  â”‚  â€¢ Stop if node has fewer than min examples   â”‚
  â”‚                  â”‚                                               â”‚
  â”‚                  â”‚  PROBLEM: Hard to decide when to stop.        â”‚
  â”‚                  â”‚  May miss a great split just ahead.           â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚  Post-pruning    â”‚  Grow FULL tree, then TRIM back               â”‚
  â”‚  (Most common)   â”‚  Remove branches that do not help             â”‚
  â”‚                  â”‚                                               â”‚
  â”‚                  â”‚  C4.5: uses statistical error estimation      â”‚
  â”‚                  â”‚  Alternative: use a VALIDATION SET            â”‚
  â”‚                  â”‚  to test which branches hurt accuracy         â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Trade-off:**

```
  Perfect fit on training:   100% train acc,  60% test acc  â† BAD
  Pruned tree:                90% train acc,  88% test acc  â† GOOD

  Moral: Being LESS perfect on training can make you
         MORE accurate on real unseen data.
```

[ğŸ” Back to Top](#-table-of-contents)

---

## 19. Overfitting Example

> *Slide 37 â€” An example (Likely to overfit the data)*

The slide shows the same 2D continuous dataset with two trees:

```
  OVERFITTED TREE                       PRUNED TREE
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€           â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Many levels, memorises noise          Fewer levels, generalises

         â”Œâ”€â”€â”€â”€â”€â”                               â”Œâ”€â”€â”€â”€â”€â”
         â”‚  X  â”‚                               â”‚  X  â”‚
         â””â”€â”€â”¬â”€â”€â”˜                               â””â”€â”€â”¬â”€â”€â”˜
      â‰¤2  â•±  â•² >2                          â‰¤2  â•±  â•² >2
      â”Œâ”€â”€â”€â”   â”Œâ”€â”€â”€â”                         â–       â”Œâ”€â”€â”€â”
      â”‚ Y â”‚   â”‚ Y â”‚                                â”‚ Y â”‚
      â””â”€â”¬â”€â”˜   â””â”€â”¬â”€â”˜                               â””â”€â”¬â”€â”˜
   â‰¤2.5 >2.5 â‰¤2  >2                            â‰¤2    >2
    â–    â”ŒYâ”  â”ŒXâ”  â—‹                           â”Œâ”€â”€â”€â”    â—‹
        2.6>  â‰¤3 >3                            â”‚ X â”‚
         â–     â–   â”ŒXâ”                          â””â”€â”¬â”€â”˜
               â‰¤4  >4                         â‰¤3  >3
               â—‹    â–                           â–    â”ŒXâ”
                                                  â‰¤4 >4
                                                  â—‹   â– 

  6 levels deep, fits every            3 levels deep, ignores
  noisy outlier perfectly              noise, generalises well
```

**Rule of thumb:**

```
  Prune branches where gain is due to NOISE (few examples, unusual)
  Keep branches where gain is due to SIGNAL (many examples, consistent)

  C4.5 uses statistical confidence intervals to decide which is which.
```

[ğŸ” Back to Top](#-table-of-contents)

---

## 20. Other Issues in Decision Tree Learning

> *Slide 38 â€” Other issues in decision tree learning*

```
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  ADVANCED TOPICS (beyond the core algorithm):               â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚                                                             â”‚
  â”‚  1. From tree to rules, and rule pruning                    â”‚
  â”‚     Remove redundant conditions from converted rules        â”‚
  â”‚                                                             â”‚
  â”‚  2. Handling missing values                                 â”‚
  â”‚     What if an attribute is unknown for an example?         â”‚
  â”‚     Strategies: skip, fill most common value, distribute    â”‚
  â”‚                                                             â”‚
  â”‚  3. Handling skewed distributions                           â”‚
  â”‚     When one class is very rare (e.g., fraud = 0.1%)        â”‚
  â”‚     Entropy dominated by majority â†’ need cost-sensitivity   â”‚
  â”‚                                                             â”‚
  â”‚  4. Attributes and classes with different costs             â”‚
  â”‚     Misclassifying cancer as benign is far worse than       â”‚
  â”‚     the reverse â†’ costs must be factored into splitting     â”‚
  â”‚                                                             â”‚
  â”‚  5. Attribute construction                                  â”‚
  â”‚     Create new derived attributes to improve splits         â”‚
  â”‚     e.g., income brackets, age groups, ratios               â”‚
  â”‚                                                             â”‚
  â”‚  6. Etc. â€” active research area!                            â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

[ğŸ” Back to Top](#-table-of-contents)

---

## 21. Cheat Sheet and Mnemonics

> Quick-recall reference for exams and revision

### The Big Picture â€” "SEGS"

```
  S â€” Split on best attribute (highest Information Gain)
  E â€” Entropy measures disorder  (0 = pure, 1 = max confusion)
  G â€” Gain = entropy BEFORE âˆ’ entropy AFTER split
  S â€” Stop when pure, no attributes left, or gain < threshold
```

---

### Entropy Rules â€” "PZML"

```
  P â€” Probabilities always sum to 1.0
  Z â€” Zero entropy = pure node â†’ STOP, make a leaf
  M â€” Maximum entropy = 1.0 (binary case) at 50/50
  L â€” Logâ‚‚ of any probability is NEGATIVE
      (formula has leading minus â†’ entropy is always POSITIVE)
```

---

### Logâ‚‚ Quick Reference Card

```
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  EXACT VALUES (memorise):                                â”‚
  â”‚    logâ‚‚(1.000) =  0.000   logâ‚‚(0.500) = -1.000           â”‚
  â”‚    logâ‚‚(0.250) = -2.000   logâ‚‚(0.125) = -3.000           â”‚
  â”‚                                                          â”‚
  â”‚  FORMULA (for everything else):                          â”‚
  â”‚    logâ‚‚(x) = log10(x) / 0.3010                           â”‚
  â”‚                                                          â”‚
  â”‚  COMMON EXAM VALUES:                                     â”‚
  â”‚    logâ‚‚(0.600) = -0.737      Pr = 9/15                   â”‚
  â”‚    logâ‚‚(0.400) = -1.322      Pr = 6/15                   â”‚
  â”‚    logâ‚‚(0.667) = -0.585      Pr = 2/3                    â”‚
  â”‚    logâ‚‚(0.333) = -1.585      Pr = 1/3                    â”‚
  â”‚    logâ‚‚(0.800) = -0.322      Pr = 4/5                    â”‚
  â”‚    logâ‚‚(0.200) = -2.322      Pr = 1/5                    â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### The Gain Calculation Flowchart

```
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  1. Count classes in D                                     â”‚
  â”‚     â†’ Pr(cj) = count(cj) / total                           â”‚
  â”‚                   â†“                                        â”‚
  â”‚  2. Compute entropy(D)                                     â”‚
  â”‚     = âˆ’Î£ Pr(cj) Ã— logâ‚‚(Pr(cj))                             â”‚
  â”‚                   â†“                                        â”‚
  â”‚  3. For each attribute Ai:                                 â”‚
  â”‚     a. Split D into subsets D1, D2, ..., Dv                â”‚
  â”‚     b. Compute entropy(Dj) for each subset                 â”‚
  â”‚     c. Weighted entropy = Î£ (|Dj|/|D|) Ã— entropy(Dj)       â”‚
  â”‚     d. gain = entropy(D) âˆ’ weighted entropy                â”‚
  â”‚                   â†“                                        â”‚
  â”‚  4. Pick Ai with MAXIMUM gain â†’ split here                 â”‚
  â”‚                   â†“                                        â”‚
  â”‚  5. Recurse on each subset (Ai removed from A)             â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Decision Tree Quick Facts

```
  Type:         Supervised learning, classification
  Algorithm:    Greedy, top-down, divide-and-conquer
  Best system:  C4.5 by Ross Quinlan (uses Gain Ratio)
  Python:       sklearn uses CART with Gini impurity
  Unique?       NO â€” same data can produce many valid trees
  Optimal?      Finding the globally best tree is NP-hard
  How?          Heuristics (locally optimal at each step)
  Overfit?      YES â€” must prune (pre or post)
  Rules:        Each root-to-leaf path = one IF-THEN rule
```

---

### Splitting Criteria Comparison

```
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  Criterion    â”‚  Formula             â”‚  Used by              â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚  Info Gain    â”‚  entropy(D)          â”‚  ID3 (original)       â”‚
  â”‚               â”‚  - entropy_Ai(D)     â”‚  biased to many vals  â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚  Gain Ratio   â”‚  Gain / SplitInfo    â”‚  C4.5 (recommended)   â”‚
  â”‚               â”‚  SplitInfo penalises â”‚  corrects the bias    â”‚
  â”‚               â”‚  wide/uneven splits  â”‚                       â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚  Gini         â”‚  1 âˆ’ Î£ Pr(cj)Â²      â”‚  CART / sklearn        â”‚
  â”‚               â”‚  No log needed       â”‚  fast to compute      â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Three Stopping Conditions

```
  â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
  â•‘  STOP 1 â€” All same class:   entropy = 0   â†’ leaf node         â•‘
  â•‘  STOP 2 â€” No attributes:    A = empty      â†’ majority vote    â•‘
  â•‘  STOP 3 â€” Gain too small:   gain < theta   â†’ pre-prune        â•‘
  â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

[ğŸ” Back to Top](#-table-of-contents)

---

## 22. Formula Quick Reference

> Every formula from the slides in one place

### Entropy

$$\text{entropy}(D) = -\sum_{j=1}^{|C|} \Pr(c_j) \cdot \log_2 \Pr(c_j)$$

**Key values:**

| Distribution | Entropy |
|---|---|
| All one class (100/0) | **0.000** |
| 80/20 split | **0.722** |
| 60/40 split (9 Yes, 6 No) | **0.971** |
| 50/50 split | **1.000** |
| Three equal classes (1/3 each) | **1.585** |

### Expected Entropy After Split

$$\text{entropy}_{A_i}(D) = \sum_{j=1}^{v} \frac{|D_j|}{|D|} \times \text{entropy}(D_j)$$

### Information Gain

$$\text{gain}(D, A_i) = \text{entropy}(D) - \text{entropy}_{A_i}(D)$$

### Gain Ratio (C4.5)

$$\text{GainRatio}(D, A_i) = \frac{\text{gain}(D, A_i)}{\text{SplitInfo}(A_i)}$$

$$\text{SplitInfo}(A_i) = -\sum_{j=1}^{v} \frac{|D_j|}{|D|} \cdot \log_2 \frac{|D_j|}{|D|}$$

### Gini Impurity (CART / sklearn)

$$\text{Gini}(D) = 1 - \sum_{j=1}^{|C|} \Pr(c_j)^2$$

### Change of Base Formula

$$\log_2(x) = \frac{\log_{10}(x)}{0.30103} = \frac{\ln(x)}{0.69315}$$

### Loan Dataset Final Gain Summary

```
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  Attribute       â”‚  Weighted    â”‚  Gain        â”‚
  â”‚                  â”‚  Entropy     â”‚              â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚  Age             â”‚  0.888       â”‚  0.083       â”‚
  â”‚  Has Job         â”‚  0.647       â”‚  0.324       â”‚
  â”‚  Credit Rating   â”‚  0.608       â”‚  0.363       â”‚
  â”‚  Own House       â”‚  0.551       â”‚  0.420  âœ…   â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  Root node: Own House (highest gain = 0.420)
```

---

> **ML Course:** Subhash Bhagat, Sir..
> **Topic:** Decision Tree Learning â€” Entropy and Information Gain

[ğŸ” Back to Top](#-table-of-contents)
