# ğŸŒ³ Decision Trees â€” Complete Study Guide
 
> **Topic:** Decision Tree Learning using Entropy & Information Gain  
> **Level:** Beginner â†’ Advanced

---

## ğŸ“‹ Table of Contents

1. [Introduction](#1-introduction)
2. [Core Concepts & Terminology](#2-core-concepts--terminology)
3. [Decision Tree Structure (Visual)](#3-decision-tree-structure-visual)
4. [How a Decision Tree Works](#4-how-a-decision-tree-works)
5. [Mathematics â€” Step by Step](#5-mathematics--step-by-step)
   - [5.1 Entropy](#51-entropy)
   - [5.2 Information Gain](#52-information-gain)
   - [5.3 Gain Ratio](#53-gain-ratio)
   - [5.4 Gini Impurity (CART)](#54-gini-impurity-cart)
6. [The Algorithm](#6-the-algorithm)
7. [Worked Example â€” Loan Dataset](#7-worked-example--loan-dataset)
8. [Tree â†’ Rules Conversion](#8-tree--rules-conversion)
9. [Handling Continuous Attributes](#9-handling-continuous-attributes)
10. [Overfitting & Pruning](#10-overfitting--pruning)
11. [Algorithm Variants](#11-algorithm-variants)
12. [Visualizations & Diagrams](#12-visualizations--diagrams)
13. [Mnemonics & Cheat Sheet](#13-mnemonics--cheat-sheet)
14. [Practice Problems](#14-practice-problems)
15. [Best Practices](#15-best-practices)
16. [Common Pitfalls](#16-common-pitfalls)

---

## 1. Introduction

A **Decision Tree** is a supervised machine learning algorithm used for **classification** and **regression**. It learns a model shaped like a flowchart-tree where:

- Every **internal node** = a question about a feature
- Every **branch** = an answer/outcome to that question
- Every **leaf node** = a final class label or prediction

```
                  [Is it raining?]
                  /             \
               Yes               No
              /                    \
    [Do you have an umbrella?]    â†’ Go outside â˜€ï¸
         /          \
       Yes           No
        |              |
   Stay dry â˜‚ï¸     Get wet ğŸŒ§ï¸
```

### Why Use Decision Trees?

| Advantage | Explanation |
|---|---|
| âœ… Interpretable | Humans can read the rules directly |
| âœ… No feature scaling needed | Works with raw values |
| âœ… Handles mixed data | Numerical + categorical features |
| âœ… Competitive accuracy | Often rivals complex models |
| âœ… Fast prediction | Just traverse the tree |
| âŒ Prone to overfitting | Can memorize training data |
| âŒ Instability | Small data change â†’ different tree |
| âŒ NP-hard to optimize | All algorithms are heuristic |

> ğŸ’¡ **Best-known system:** C4.5 by Ross Quinlan (downloadable, widely used in practice)

---

## 2. Core Concepts & Terminology

| Term | Meaning | Example |
|---|---|---|
| **Root Node** | Top-most node; first question asked | "Own House?" |
| **Decision Node** | Internal node that tests an attribute | "Has Job?" |
| **Leaf Node** | Terminal node; holds a class label | "Yes" / "No" |
| **Branch** | Edge connecting nodes; represents a value/outcome | "true" or "false" |
| **Depth** | Longest path from root to leaf | 3 levels deep |
| **Purity** | All examples in node are same class | 6 Yes, 0 No = pure |
| **Impurity** | Mix of classes in a node | 3 Yes, 3 No = max impure |
| **Entropy** | Mathematical measure of impurity/disorder | 0 = pure, 1 = max disorder |
| **Information Gain** | Reduction in entropy after a split | Higher = better attribute |
| **Pruning** | Removing branches to reduce overfitting | Pre/Post pruning |
| **Support** | Fraction of training data a rule covers | 6/15 = 40% |
| **Confidence** | Accuracy of the rule on covered examples | 6/6 = 100% |

---

## 3. Decision Tree Structure (Visual)

### Full Tree (from Loan Dataset)

```
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚    Age?     â”‚  â† Root Node (Decision Node)
                         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           Young             middle                old
              â”‚                 â”‚                   â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”
     â”‚   Has_Job?      â”‚  â”‚ Own_House? â”‚   â”‚ Credit_Rating?â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â”Œâ”€â”€â”€â”´â”€â”€â”€â”         â”Œâ”€â”€â”€â”´â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”
        true   false      true   false      fair  good  excellent
          â”‚       â”‚         â”‚       â”‚         â”‚     â”‚       â”‚
        Yes      No        Yes     No        No   Yes     Yes
       (2/2)   (3/3)      (3/3)  (2/2)     (1/1)(2/2)   (2/2)
         â†‘                                             â†‘
      Leaf Node                                    Leaf Node
```

### Simpler Equivalent Tree (Better!)

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Own_House?  â”‚  â† Root (highest info gain)
                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
                  true          false
                    â”‚               â”‚
               â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
               â”‚   Yes   â”‚    â”‚ Has_Job?â”‚
               â”‚  (6/6)  â”‚    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
                            true     false
                              â”‚          â”‚
                            Yes         No
                           (5/5)       (4/4)
```

> ğŸ¯ **Key insight:** The second tree is simpler AND equally accurate. Simpler trees generalize better!

---

## 4. How a Decision Tree Works

### Classification Flow (Step-by-step)

Given a new record: `Age=young, Has_Job=false, Own_house=false, Credit=good`

```
Step 1: Start at ROOT â†’ "Own_House?"
                â†“
        Own_House = false â†’ go RIGHT

Step 2: Reach "Has_Job?"
                â†“
        Has_Job = false â†’ go RIGHT

Step 3: Reach LEAF â†’ "No"
                â†“
        ğŸ¦ LOAN REJECTED
```

### Tree Building (High Level)

```
Start with ALL training data at root
         â”‚
         â–¼
  For each attribute, compute
  how much it reduces impurity
         â”‚
         â–¼
  Pick attribute with MAX gain
  â†’ Make it the current node
         â”‚
         â–¼
  Split data into subsets
  (one per attribute value)
         â”‚
         â–¼
  Recurse on each subset
  until stopping condition
```

### Stopping Conditions

```
STOP when any of these are true:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â‘  All examples in node = same class             â”‚
â”‚    â†’ Make leaf with that class                  â”‚
â”‚                                                 â”‚
â”‚ â‘¡ No attributes left to split on               â”‚
â”‚    â†’ Make leaf with MAJORITY class              â”‚
â”‚                                                 â”‚
â”‚ â‘¢ No examples left in this branch              â”‚
â”‚    â†’ Make leaf with PARENT'S majority class     â”‚
â”‚                                                 â”‚
â”‚ â‘£ Gain < threshold (pre-pruning)               â”‚
â”‚    â†’ Make leaf with majority class              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 5. Mathematics â€” Step by Step

---

### 5.0 â€” Pre-Requisite: How to Calculate logâ‚‚ (Base-2 Logarithm)

> ğŸ’¡ **Read this first.** Every entropy and gain calculation depends on computing logâ‚‚. Understanding this once makes all the math click.

---

#### ğŸ“ What Is a Logarithm?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DEFINITION                                                     â”‚
â”‚                                                                 â”‚
â”‚  log_b(x) = y    means    b^y = x                               â”‚
â”‚                                                                 â”‚
â”‚  In plain English:                                              â”‚
â”‚  "To what POWER must I raise b to get x?"                       â”‚
â”‚                                                                 â”‚
â”‚  b = base (we use base 2 in entropy)                            â”‚
â”‚  x = the number we're taking the log of                         â”‚
â”‚  y = the answer (the exponent)                                  â”‚
â”‚                                                                 â”‚
â”‚  EXAMPLES with base 2:                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ logâ‚‚(8)   = 3   because  2Â³ = 8                         â”‚   â”‚
â”‚  â”‚ logâ‚‚(4)   = 2   because  2Â² = 4                         â”‚   â”‚
â”‚  â”‚ logâ‚‚(2)   = 1   because  2Â¹ = 2                         â”‚   â”‚
â”‚  â”‚ logâ‚‚(1)   = 0   because  2â° = 1   â† always true!       â”‚   â”‚
â”‚  â”‚ logâ‚‚(0.5) = âˆ’1  because  2â»Â¹ = 0.5                     â”‚   â”‚
â”‚  â”‚ logâ‚‚(0.25)= âˆ’2  because  2â»Â² = 0.25                    â”‚   â”‚
â”‚  â”‚ logâ‚‚(0.125)=âˆ’3  because  2â»Â³ = 0.125                   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                 â”‚
â”‚  CRITICAL RULE:                                                 â”‚
â”‚  Any number between 0 and 1 (a fraction/probability)           â”‚
â”‚  ALWAYS has a NEGATIVE logarithm.                               â”‚
â”‚                                                                 â”‚
â”‚  WHY? Because to get a fraction, you need a negative exponent:  â”‚
â”‚    2â»Â¹ = 1/2 = 0.5    â†’  logâ‚‚(0.5) = âˆ’1                       â”‚
â”‚    2â»Â² = 1/4 = 0.25   â†’  logâ‚‚(0.25) = âˆ’2                      â”‚
â”‚    Probabilities are always between 0 and 1                     â”‚
â”‚    So logâ‚‚(probability) is ALWAYS negative!                     â”‚
â”‚    That's why entropy has a leading âˆ’ sign (to make it +ve)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

#### ğŸ“ The Change of Base Formula

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PROBLEM: Most calculators only have logâ‚â‚€ or ln buttons.      â”‚
â”‚           There is no logâ‚‚ button.                              â”‚
â”‚                                                                 â”‚
â”‚  SOLUTION: Change of Base Formula                               â”‚
â”‚                                                                 â”‚
â”‚              logâ‚â‚€(x)         ln(x)                            â”‚
â”‚  logâ‚‚(x) = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   =   â”€â”€â”€â”€â”€â”€â”€â”€â”€                          â”‚
â”‚              logâ‚â‚€(2)         ln(2)                             â”‚
â”‚                                                                 â”‚
â”‚  The FIXED denominator values:                                  â”‚
â”‚    logâ‚â‚€(2) = 0.30103   â† divide by this when using logâ‚â‚€      â”‚
â”‚    ln(2)    = 0.69315   â† divide by this when using ln         â”‚
â”‚                                                                 â”‚
â”‚  THE RECIPE (always the same 2 steps):                          â”‚
â”‚    Step A: Find logâ‚â‚€(x) on your calculator                     â”‚
â”‚    Step B: Divide that result by 0.30103                        â”‚
â”‚    â†’ Result = logâ‚‚(x)                                           â”‚
â”‚                                                                 â”‚
â”‚  WHY DOES THIS WORK?                                            â”‚
â”‚  log_b(x) = log_a(x) / log_a(b)   â† mathematical identity     â”‚
â”‚  Choosing a=10: logâ‚‚(x) = logâ‚â‚€(x) / logâ‚â‚€(2) = logâ‚â‚€(x)/0.301â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

#### ğŸ“ How to Find logâ‚â‚€ Without a Calculator

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CORE VALUES TO MEMORISE (only 3 needed!)                       â”‚
â”‚                                                                 â”‚
â”‚    logâ‚â‚€(2)  = 0.3010                                           â”‚
â”‚    logâ‚â‚€(3)  = 0.4771                                           â”‚
â”‚    logâ‚â‚€(10) = 1.0000   â† always!                              â”‚
â”‚                                                                 â”‚
â”‚  RULES TO DERIVE EVERYTHING ELSE:                               â”‚
â”‚                                                                 â”‚
â”‚  Rule 1: log(a Ã— b) = log(a) + log(b)                          â”‚
â”‚    logâ‚â‚€(6)  = logâ‚â‚€(2Ã—3) = 0.3010 + 0.4771 = 0.7781          â”‚
â”‚    logâ‚â‚€(4)  = logâ‚â‚€(2Ã—2) = 0.3010 + 0.3010 = 0.6021          â”‚
â”‚    logâ‚â‚€(8)  = logâ‚â‚€(2Ã—2Ã—2) = 3Ã—0.3010      = 0.9031          â”‚
â”‚                                                                 â”‚
â”‚  Rule 2: log(a/b) = log(a) âˆ’ log(b)                            â”‚
â”‚    logâ‚â‚€(0.6) = logâ‚â‚€(6/10) = logâ‚â‚€(6) âˆ’ logâ‚â‚€(10)            â”‚
â”‚               = 0.7781 âˆ’ 1.0000 = âˆ’0.2219                      â”‚
â”‚    logâ‚â‚€(0.4) = logâ‚â‚€(4/10) = logâ‚â‚€(4) âˆ’ logâ‚â‚€(10)            â”‚
â”‚               = 0.6021 âˆ’ 1.0000 = âˆ’0.3979                      â”‚
â”‚    logâ‚â‚€(0.5) = logâ‚â‚€(5/10) = logâ‚â‚€(5) âˆ’ 1.0000               â”‚
â”‚               logâ‚â‚€(5) = logâ‚â‚€(10/2) = 1âˆ’0.3010 = 0.6990      â”‚
â”‚               = 0.6990 âˆ’ 1.0000 = âˆ’0.3010                      â”‚
â”‚                                                                 â”‚
â”‚  Rule 3: log(aâ¿) = n Ã— log(a)                                  â”‚
â”‚    logâ‚â‚€(4) = logâ‚â‚€(2Â²) = 2 Ã— logâ‚â‚€(2) = 2 Ã— 0.3010 = 0.6021 â”‚
â”‚    logâ‚â‚€(0.25) = logâ‚â‚€(1/4) = logâ‚â‚€(1)âˆ’logâ‚â‚€(4) = 0âˆ’0.6021   â”‚
â”‚               = âˆ’0.6021                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

#### ğŸ“ Complete Worked Examples: logâ‚‚(0.6) and logâ‚‚(0.4)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  COMPUTING logâ‚‚(0.6) â€” every micro-step                        â”‚
â”‚                                                                 â”‚
â”‚  Sub-step 1: Recognise 0.6 = 6 Ã· 10                           â”‚
â”‚                                                                 â”‚
â”‚  Sub-step 2: Apply log division rule                            â”‚
â”‚              logâ‚â‚€(0.6) = logâ‚â‚€(6) âˆ’ logâ‚â‚€(10)                â”‚
â”‚                                                                 â”‚
â”‚  Sub-step 3: Find logâ‚â‚€(6)                                     â”‚
â”‚              logâ‚â‚€(6) = logâ‚â‚€(2 Ã— 3)                           â”‚
â”‚                       = logâ‚â‚€(2) + logâ‚â‚€(3)                    â”‚
â”‚                       = 0.3010    + 0.4771                      â”‚
â”‚                       = 0.7781                                  â”‚
â”‚                                                                 â”‚
â”‚  Sub-step 4: Find logâ‚â‚€(10) = 1.0000  (always)                 â”‚
â”‚                                                                 â”‚
â”‚  Sub-step 5: Subtract                                           â”‚
â”‚              logâ‚â‚€(0.6) = 0.7781 âˆ’ 1.0000 = âˆ’0.2219           â”‚
â”‚                                                                 â”‚
â”‚  Sub-step 6: Change base â€” divide by logâ‚â‚€(2)                  â”‚
â”‚              logâ‚‚(0.6) = âˆ’0.2219 Ã· 0.3010                      â”‚
â”‚                        = âˆ’0.7370  âœ…                             â”‚
â”‚                                                                 â”‚
â”‚  Sanity check: 2^(âˆ’0.737) = 1/2^(0.737) â‰ˆ 1/1.667 â‰ˆ 0.60 âœ…   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  COMPUTING logâ‚‚(0.4) â€” every micro-step                        â”‚
â”‚                                                                 â”‚
â”‚  Sub-step 1: Recognise 0.4 = 4 Ã· 10                           â”‚
â”‚                                                                 â”‚
â”‚  Sub-step 2: Apply log division rule                            â”‚
â”‚              logâ‚â‚€(0.4) = logâ‚â‚€(4) âˆ’ logâ‚â‚€(10)                â”‚
â”‚                                                                 â”‚
â”‚  Sub-step 3: Find logâ‚â‚€(4) using power rule                    â”‚
â”‚              logâ‚â‚€(4) = logâ‚â‚€(2Â²)                               â”‚
â”‚                       = 2 Ã— logâ‚â‚€(2)                            â”‚
â”‚                       = 2 Ã— 0.3010                              â”‚
â”‚                       = 0.6021                                  â”‚
â”‚                                                                 â”‚
â”‚  Sub-step 4: Find logâ‚â‚€(10) = 1.0000  (always)                 â”‚
â”‚                                                                 â”‚
â”‚  Sub-step 5: Subtract                                           â”‚
â”‚              logâ‚â‚€(0.4) = 0.6021 âˆ’ 1.0000 = âˆ’0.3979           â”‚
â”‚                                                                 â”‚
â”‚  Sub-step 6: Change base â€” divide by logâ‚â‚€(2)                  â”‚
â”‚              logâ‚‚(0.4) = âˆ’0.3979 Ã· 0.3010                      â”‚
â”‚                        = âˆ’1.3219  âœ…                             â”‚
â”‚                                                                 â”‚
â”‚  Sanity check: 2^(âˆ’1.322) = 1/2^(1.322) â‰ˆ 1/2.499 â‰ˆ 0.40 âœ…   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

#### ğŸ“Š Master Reference Table â€” logâ‚‚ of All Common Probabilities

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  x        â”‚ logâ‚â‚€(x)  â”‚ Ã· 0.3010  =  logâ‚‚(x)    â”‚ Notes        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1.000    â”‚  0.0000    â”‚  0.0000 / 0.3010 =  0.000â”‚ 2â°=1        â”‚
â”‚  0.900    â”‚ âˆ’0.0458    â”‚ âˆ’0.0458 / 0.3010 = âˆ’0.152â”‚              â”‚
â”‚  0.800    â”‚ âˆ’0.0969    â”‚ âˆ’0.0969 / 0.3010 = âˆ’0.322â”‚              â”‚
â”‚  0.750    â”‚ âˆ’0.1249    â”‚ âˆ’0.1249 / 0.3010 = âˆ’0.415â”‚ = 3/4        â”‚
â”‚  0.667    â”‚ âˆ’0.1761    â”‚ âˆ’0.1761 / 0.3010 = âˆ’0.585â”‚ = 2/3        â”‚
â”‚  0.625    â”‚ âˆ’0.2041    â”‚ âˆ’0.2041 / 0.3010 = âˆ’0.678â”‚ = 5/8        â”‚
â”‚  0.600    â”‚ âˆ’0.2219    â”‚ âˆ’0.2219 / 0.3010 = âˆ’0.737â”‚ â† 9/15      â”‚
â”‚  0.500    â”‚ âˆ’0.3010    â”‚ âˆ’0.3010 / 0.3010 = âˆ’1.000â”‚ 2â»Â¹=1/2     â”‚
â”‚  0.400    â”‚ âˆ’0.3979    â”‚ âˆ’0.3979 / 0.3010 = âˆ’1.322â”‚ â† 6/15      â”‚
â”‚  0.375    â”‚ âˆ’0.4260    â”‚ âˆ’0.4260 / 0.3010 = âˆ’1.415â”‚ = 3/8        â”‚
â”‚  0.333    â”‚ âˆ’0.4771    â”‚ âˆ’0.4771 / 0.3010 = âˆ’1.585â”‚ = 1/3        â”‚
â”‚  0.300    â”‚ âˆ’0.5229    â”‚ âˆ’0.5229 / 0.3010 = âˆ’1.737â”‚              â”‚
â”‚  0.250    â”‚ âˆ’0.6021    â”‚ âˆ’0.6021 / 0.3010 = âˆ’2.000â”‚ 2â»Â²=1/4     â”‚
â”‚  0.200    â”‚ âˆ’0.6990    â”‚ âˆ’0.6990 / 0.3010 = âˆ’2.322â”‚ = 1/5        â”‚
â”‚  0.167    â”‚ âˆ’0.7782    â”‚ âˆ’0.7782 / 0.3010 = âˆ’2.585â”‚ = 1/6        â”‚
â”‚  0.143    â”‚ âˆ’0.8451    â”‚ âˆ’0.8451 / 0.3010 = âˆ’2.807â”‚ = 1/7        â”‚
â”‚  0.125    â”‚ âˆ’0.9031    â”‚ âˆ’0.9031 / 0.3010 = âˆ’3.000â”‚ 2â»Â³=1/8     â”‚
â”‚  0.100    â”‚ âˆ’1.0000    â”‚ âˆ’1.0000 / 0.3010 = âˆ’3.322â”‚ = 1/10       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

PATTERN to remember:
  Probabilities of simple fractions 1/2â¿ give exact integers:
  1/2 â†’ âˆ’1,   1/4 â†’ âˆ’2,   1/8 â†’ âˆ’3,   1/16 â†’ âˆ’4
  Everything else falls in between.
```

---

#### ğŸ§  Shortcut: How to Estimate logâ‚‚ Mentally

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Anchor points to memorise:                                     â”‚
â”‚    logâ‚‚(1.0)  =  0       logâ‚‚(0.5) = âˆ’1                        â”‚
â”‚    logâ‚‚(0.25) = âˆ’2       logâ‚‚(0.125) = âˆ’3                      â”‚
â”‚                                                                 â”‚
â”‚  Interpolation trick: 0.6 is between 0.5 and 1.0               â”‚
â”‚    logâ‚‚(0.5) = âˆ’1,  logâ‚‚(1.0) = 0                              â”‚
â”‚    0.6 is 20% of the way from 0.5â†’1.0                           â”‚
â”‚    Rough estimate: âˆ’1 + 0.20Ã—1 = âˆ’0.80                          â”‚
â”‚    Actual: âˆ’0.737  (close enough for sanity checks!)            â”‚
â”‚                                                                 â”‚
â”‚  Interpolation trick: 0.4 is between 0.25 and 0.5              â”‚
â”‚    logâ‚‚(0.25) = âˆ’2,  logâ‚‚(0.5) = âˆ’1                            â”‚
â”‚    0.4 is 60% of the way from 0.25â†’0.5                          â”‚
â”‚    Rough estimate: âˆ’2 + 0.60Ã—1 = âˆ’1.40                          â”‚
â”‚    Actual: âˆ’1.322  (close enough!)                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 5.1 Entropy

#### What is Entropy?

Entropy measures **how mixed/disordered** a dataset is.

```
High Entropy = Mixed Classes = Uncertain = More info needed
Low Entropy  = Pure Classes  = Certain   = Less info needed

Real-world analogy:
â€¢ A bag with ALL red balls  â†’ entropy = 0   (you know exactly what you'll pick)
â€¢ A bag with HALF red, HALF blue â†’ entropy = 1  (total surprise every time)
â€¢ A bag with mostly red, few blue â†’ entropy is between 0 and 1

Coin example:
â€¢ Fair coin   (50/50):  entropy = 1.0  â†’ you know nothing!
â€¢ Biased coin (99/1):   entropy â‰ˆ 0.08 â†’ pretty predictable
â€¢ Fixed coin  (100/0):  entropy = 0    â†’ you know everything!
```

---

#### ğŸ“ The Entropy Formula

$$\text{entropy}(D) = -\sum_{j=1}^{|C|} \Pr(c_j) \cdot \log_2 \Pr(c_j)$$

**Breaking down every symbol:**

| Symbol | Full Name | Meaning | Example |
|---|---|---|---|
| `D` | Dataset | The set of training examples at a node | All 15 loan records |
| `C` | Class set | The set of all possible class labels | {Yes, No} |
| `\|C\|` | Class count | How many distinct classes exist | 2 |
| `j` | Class index | Counts from 1 to \|C\| | j=1 â†’ Yes, j=2 â†’ No |
| `Pr(câ±¼)` | Class probability | Fraction of examples belonging to class câ±¼ | Pr(Yes) = 9/15 |
| `logâ‚‚` | Log base 2 | Logarithm with base 2 (measures bits) | logâ‚‚(0.5) = -1 |
| `âˆ’` | Negation | Makes the result positive (logs of fractions are negative) | âˆ’(âˆ’0.971) = +0.971 |
| `Î£` | Summation | Add up the term for every class | Sum over Yes and No |

> âš ï¸ **Special convention:** `0 Ã— logâ‚‚(0) = 0` by definition (not undefined). Pure nodes don't contribute.

> ğŸ’¡ **Why log base 2?** It measures information in *bits*. 1 bit = 1 yes/no question. A fair coin needs exactly 1 bit to describe its outcome.

---

#### ğŸ”¢ Step-by-Step: Entropy Calculation (Example 1)

**Given:** Dataset D has **9 Yes** and **6 No** out of **15 total** examples.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 1 â€” Count the classes                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Total examples  = 15                                           â”‚
â”‚  Yes count       = 9                                            â”‚
â”‚  No  count       = 6                                            â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 2 â€” Compute class probabilities                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Pr(Yes) = count(Yes) / total = 9 / 15 = 0.6                   â”‚
â”‚  Pr(No)  = count(No)  / total = 6 / 15 = 0.4                   â”‚
â”‚                                                                 â”‚
â”‚  âœ… Check: 0.6 + 0.4 = 1.0  (must sum to 1)                    â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 3 â€” Write out the entropy formula                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  entropy(D) = âˆ’ [Pr(Yes) Ã— logâ‚‚(Pr(Yes))]                      â”‚
â”‚             âˆ’ [Pr(No)  Ã— logâ‚‚(Pr(No))]                         â”‚
â”‚                                                                 â”‚
â”‚  entropy(D) = âˆ’ [0.6 Ã— logâ‚‚(0.6)]                              â”‚
â”‚             âˆ’ [0.4 Ã— logâ‚‚(0.4)]                                 â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 4 â€” Calculate each logâ‚‚ value                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—  â”‚
â”‚  â•‘  PART A â€” WHAT IS A LOGARITHM?                           â•‘  â”‚
â”‚  â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  â”‚
â”‚                                                                 â”‚
â”‚  Definition:                                                    â”‚
â”‚    log_b(x) = y   â†â†’   b^y = x                                 â”‚
â”‚                                                                 â”‚
â”‚  Read as: "log base b of x equals y"                           â”‚
â”‚  Meaning: "b raised to the power y gives x"                    â”‚
â”‚           or "what exponent on b produces x?"                  â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  EXACT examples with base 2 (logâ‚‚):                     â”‚  â”‚
â”‚  â”‚                                                          â”‚  â”‚
â”‚  â”‚  logâ‚‚(8)    = 3   â† because 2Â³    = 8                   â”‚  â”‚
â”‚  â”‚  logâ‚‚(4)    = 2   â† because 2Â²    = 4                   â”‚  â”‚
â”‚  â”‚  logâ‚‚(2)    = 1   â† because 2Â¹    = 2                   â”‚  â”‚
â”‚  â”‚  logâ‚‚(1)    = 0   â† because 2â°    = 1  (always!)        â”‚  â”‚
â”‚  â”‚  logâ‚‚(0.5)  = âˆ’1  â† because 2â»Â¹   = 1/2  = 0.5         â”‚  â”‚
â”‚  â”‚  logâ‚‚(0.25) = âˆ’2  â† because 2â»Â²   = 1/4  = 0.25        â”‚  â”‚
â”‚  â”‚  logâ‚‚(0.125)= âˆ’3  â† because 2â»Â³   = 1/8  = 0.125       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                 â”‚
â”‚  ğŸ”‘ KEY INSIGHT: Probabilities are always between 0 and 1.     â”‚
â”‚     To get a number < 1 from 2^y, you need y to be NEGATIVE.   â”‚
â”‚     Therefore logâ‚‚(any probability) is ALWAYS NEGATIVE.        â”‚
â”‚     The âˆ’ sign in the entropy formula flips it to positive.    â”‚
â”‚                                                                 â”‚
â”‚  WHY BASE 2? It measures information in bits.                  â”‚
â”‚    1 bit = 1 yes/no question with completely unknown answer     â”‚
â”‚    Fair coin â†’ 1 bit of uncertainty â†’ entropy = 1.0            â”‚
â”‚    Sure outcome â†’ 0 bits of uncertainty â†’ entropy = 0.0        â”‚
â”‚                                                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—  â”‚
â”‚  â•‘  PART B â€” THE 3 LOG RULES YOU NEED                       â•‘  â”‚
â”‚  â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  â”‚
â”‚                                                                 â”‚
â”‚  Rule 1 â€” Product Rule:  log(a Ã— b) = log(a) + log(b)          â”‚
â”‚    logâ‚â‚€(6) = logâ‚â‚€(2Ã—3) = logâ‚â‚€(2) + logâ‚â‚€(3)                â”‚
â”‚                           = 0.3010   + 0.4771  = 0.7781        â”‚
â”‚                                                                 â”‚
â”‚  Rule 2 â€” Quotient Rule: log(a/b) = log(a) âˆ’ log(b)            â”‚
â”‚    logâ‚â‚€(0.6) = logâ‚â‚€(6/10) = logâ‚â‚€(6) âˆ’ logâ‚â‚€(10)            â”‚
â”‚                             = 0.7781  âˆ’ 1.0000  = âˆ’0.2219      â”‚
â”‚                                                                 â”‚
â”‚  Rule 3 â€” Power Rule:    log(aâ¿) = n Ã— log(a)                  â”‚
â”‚    logâ‚â‚€(4) = logâ‚â‚€(2Â²) = 2 Ã— logâ‚â‚€(2) = 2 Ã— 0.3010 = 0.6021 â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  CORE VALUES TO MEMORISE (only 3!)                       â”‚  â”‚
â”‚  â”‚                                                          â”‚  â”‚
â”‚  â”‚  logâ‚â‚€(2)  = 0.3010   â† most important!                 â”‚  â”‚
â”‚  â”‚  logâ‚â‚€(3)  = 0.4771                                     â”‚  â”‚
â”‚  â”‚  logâ‚â‚€(10) = 1.0000   â† always true                     â”‚  â”‚
â”‚  â”‚                                                          â”‚  â”‚
â”‚  â”‚  Derive everything else using the 3 rules above          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                 â”‚
â”‚  Derived values (using the 3 rules):                           â”‚
â”‚    logâ‚â‚€(4) = 2Ã—logâ‚â‚€(2)        = 2Ã—0.3010       = 0.6021    â”‚
â”‚    logâ‚â‚€(5) = logâ‚â‚€(10/2)       = 1.0000âˆ’0.3010  = 0.6990    â”‚
â”‚    logâ‚â‚€(6) = logâ‚â‚€(2)+logâ‚â‚€(3) = 0.3010+0.4771  = 0.7781    â”‚
â”‚    logâ‚â‚€(8) = 3Ã—logâ‚â‚€(2)        = 3Ã—0.3010        = 0.9031    â”‚
â”‚    logâ‚â‚€(9) = 2Ã—logâ‚â‚€(3)        = 2Ã—0.4771        = 0.9542    â”‚
â”‚                                                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—  â”‚
â”‚  â•‘  PART C â€” CHANGE OF BASE FORMULA                         â•‘  â”‚
â”‚  â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  â”‚
â”‚                                                                 â”‚
â”‚  Problem: calculators have logâ‚â‚€ and ln, NOT logâ‚‚.            â”‚
â”‚                                                                 â”‚
â”‚  The Change of Base Formula:                                    â”‚
â”‚                                                                 â”‚
â”‚             logâ‚â‚€(x)          ln(x)                            â”‚
â”‚  logâ‚‚(x) = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   =   â”€â”€â”€â”€â”€â”€â”€â”€â”€                          â”‚
â”‚             logâ‚â‚€(2)          ln(2)                             â”‚
â”‚                                                                 â”‚
â”‚             logâ‚â‚€(x)          ln(x)                            â”‚
â”‚           = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   =   â”€â”€â”€â”€â”€â”€â”€â”€â”€                          â”‚
â”‚              0.30103          0.69315                            â”‚
â”‚                                                                 â”‚
â”‚  Why does this work?                                            â”‚
â”‚    General form:  log_b(x) = log_a(x) / log_a(b)               â”‚
â”‚    Set a=10:      logâ‚‚(x) = logâ‚â‚€(x) / logâ‚â‚€(2)               â”‚
â”‚    Substitute:    logâ‚‚(x) = logâ‚â‚€(x) / 0.30103                 â”‚
â”‚                                                                 â”‚
â”‚  THE 2-STEP RECIPE (same every time):                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Step A: Compute logâ‚â‚€(x)   [use calculator or rules]   â”‚  â”‚
â”‚  â”‚  Step B: Divide by 0.30103                               â”‚  â”‚
â”‚  â”‚  â†’ Answer is logâ‚‚(x)   âœ…                                â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—  â”‚
â”‚  â•‘  PART D â€” COMPUTING logâ‚‚(0.6) â€” EVERY MICRO-STEP         â•‘  â”‚
â”‚  â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  â”‚
â”‚                                                                 â”‚
â”‚  We need: logâ‚‚(0.6)                                            â”‚
â”‚                                                                 â”‚
â”‚  â”€â”€ Sub-step 1: Rewrite as a fraction â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚                                                                 â”‚
â”‚    0.6 = 6 Ã· 10 = 6/10                                         â”‚
â”‚                                                                 â”‚
â”‚  â”€â”€ Sub-step 2: Apply Quotient Rule to logâ‚â‚€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚                                                                 â”‚
â”‚    logâ‚â‚€(6/10) = logâ‚â‚€(6) âˆ’ logâ‚â‚€(10)                         â”‚
â”‚                                                                 â”‚
â”‚  â”€â”€ Sub-step 3: Find logâ‚â‚€(6) using Product Rule â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚                                                                 â”‚
â”‚    6 = 2 Ã— 3                                                    â”‚
â”‚    logâ‚â‚€(6) = logâ‚â‚€(2) + logâ‚â‚€(3)                              â”‚
â”‚             = 0.3010   + 0.4771                                 â”‚
â”‚             = 0.7781                                            â”‚
â”‚                                                                 â”‚
â”‚  â”€â”€ Sub-step 4: Look up logâ‚â‚€(10) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚                                                                 â”‚
â”‚    logâ‚â‚€(10) = 1.0000   â† always, by definition                â”‚
â”‚                                                                 â”‚
â”‚  â”€â”€ Sub-step 5: Subtract to get logâ‚â‚€(0.6) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚                                                                 â”‚
â”‚    logâ‚â‚€(0.6) = logâ‚â‚€(6) âˆ’ logâ‚â‚€(10)                          â”‚
â”‚               = 0.7781  âˆ’ 1.0000                                â”‚
â”‚               = âˆ’0.2219                                         â”‚
â”‚                                                                 â”‚
â”‚    Note: negative because 0.6 < 1                               â”‚
â”‚                                                                 â”‚
â”‚  â”€â”€ Sub-step 6: Apply Change of Base (divide by 0.30103) â”€â”€â”€â”€â”€ â”‚
â”‚                                                                 â”‚
â”‚    logâ‚‚(0.6) = logâ‚â‚€(0.6) Ã· logâ‚â‚€(2)                          â”‚
â”‚              = âˆ’0.2219    Ã·  0.30103                            â”‚
â”‚                                                                 â”‚
â”‚    How to do the division:                                      â”‚
â”‚      âˆ’0.2219 / 0.30103                                          â”‚
â”‚      = âˆ’(0.2219 / 0.30103)   â† handle sign separately          â”‚
â”‚                                                                 â”‚
â”‚      0.2219 / 0.30103 = ?                                       â”‚
â”‚      Try: 0.30103 Ã— 0.7 = 0.21072                               â”‚
â”‚      Try: 0.30103 Ã— 0.73 = 0.21975                              â”‚
â”‚      Try: 0.30103 Ã— 0.737 = 0.22186  â† very close to 0.2219!  â”‚
â”‚      Try: 0.30103 Ã— 0.7370 = 0.22186 âœ…                         â”‚
â”‚                                                                 â”‚
â”‚    logâ‚‚(0.6) = âˆ’0.7370   âœ…                                     â”‚
â”‚                                                                 â”‚
â”‚  â”€â”€ Sub-step 7: Sanity check â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚                                                                 â”‚
â”‚    Verify: does 2^(âˆ’0.7370) â‰ˆ 0.6?                             â”‚
â”‚    2^(âˆ’0.737) = 1 / 2^(0.737)                                   â”‚
â”‚    2^(0.737) â‰ˆ 2^(0.5) Ã— 2^(0.237) â‰ˆ 1.414 Ã— 1.179 â‰ˆ 1.667   â”‚
â”‚    1 / 1.667 â‰ˆ 0.600  âœ…  Correct!                              â”‚
â”‚                                                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—  â”‚
â”‚  â•‘  PART E â€” COMPUTING logâ‚‚(0.4) â€” EVERY MICRO-STEP         â•‘  â”‚
â”‚  â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  â”‚
â”‚                                                                 â”‚
â”‚  We need: logâ‚‚(0.4)                                            â”‚
â”‚                                                                 â”‚
â”‚  â”€â”€ Sub-step 1: Rewrite as a fraction â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚                                                                 â”‚
â”‚    0.4 = 4 Ã· 10 = 4/10                                         â”‚
â”‚                                                                 â”‚
â”‚  â”€â”€ Sub-step 2: Apply Quotient Rule to logâ‚â‚€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚                                                                 â”‚
â”‚    logâ‚â‚€(4/10) = logâ‚â‚€(4) âˆ’ logâ‚â‚€(10)                         â”‚
â”‚                                                                 â”‚
â”‚  â”€â”€ Sub-step 3: Find logâ‚â‚€(4) using Power Rule â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚                                                                 â”‚
â”‚    4 = 2Â²                                                       â”‚
â”‚    logâ‚â‚€(4) = logâ‚â‚€(2Â²) = 2 Ã— logâ‚â‚€(2)                        â”‚
â”‚             = 2 Ã— 0.3010                                        â”‚
â”‚             = 0.6021  (just double logâ‚â‚€(2)!)                  â”‚
â”‚                                                                 â”‚
â”‚  â”€â”€ Sub-step 4: Look up logâ‚â‚€(10) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚                                                                 â”‚
â”‚    logâ‚â‚€(10) = 1.0000                                          â”‚
â”‚                                                                 â”‚
â”‚  â”€â”€ Sub-step 5: Subtract to get logâ‚â‚€(0.4) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚                                                                 â”‚
â”‚    logâ‚â‚€(0.4) = logâ‚â‚€(4) âˆ’ logâ‚â‚€(10)                          â”‚
â”‚               = 0.6021  âˆ’ 1.0000                                â”‚
â”‚               = âˆ’0.3979                                         â”‚
â”‚                                                                 â”‚
â”‚    Note: negative because 0.4 < 1                               â”‚
â”‚    Note: more negative than logâ‚â‚€(0.6)=âˆ’0.2219 âœ“ (0.4<0.6)   â”‚
â”‚                                                                 â”‚
â”‚  â”€â”€ Sub-step 6: Apply Change of Base (divide by 0.30103) â”€â”€â”€â”€â”€ â”‚
â”‚                                                                 â”‚
â”‚    logâ‚‚(0.4) = logâ‚â‚€(0.4) Ã· logâ‚â‚€(2)                          â”‚
â”‚              = âˆ’0.3979    Ã·  0.30103                            â”‚
â”‚                                                                 â”‚
â”‚    How to do the division:                                      â”‚
â”‚      âˆ’0.3979 / 0.30103                                          â”‚
â”‚      = âˆ’(0.3979 / 0.30103)   â† handle sign separately          â”‚
â”‚                                                                 â”‚
â”‚      0.3979 / 0.30103 = ?                                       â”‚
â”‚      Try: 0.30103 Ã— 1.0 = 0.30103                               â”‚
â”‚      Try: 0.30103 Ã— 1.3 = 0.39134                               â”‚
â”‚      Try: 0.30103 Ã— 1.32 = 0.39736  â† getting close            â”‚
â”‚      Try: 0.30103 Ã— 1.322 = 0.39796  â‰ˆ 0.3979  âœ…              â”‚
â”‚                                                                 â”‚
â”‚    logâ‚‚(0.4) = âˆ’1.3219   âœ…                                     â”‚
â”‚                                                                 â”‚
â”‚  â”€â”€ Sub-step 7: Sanity check â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚                                                                 â”‚
â”‚    Verify: does 2^(âˆ’1.3219) â‰ˆ 0.4?                             â”‚
â”‚    2^(âˆ’1.3219) = 1 / 2^(1.3219)                                 â”‚
â”‚    We know 2^1 = 2 and 2^1.5 = 2.828                           â”‚
â”‚    2^(1.322) â‰ˆ 2 Ã— 2^(0.322) â‰ˆ 2 Ã— 1.250 â‰ˆ 2.500              â”‚
â”‚    1 / 2.500 = 0.400  âœ…  Correct!                              â”‚
â”‚                                                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—  â”‚
â”‚  â•‘  PART F â€” COMPLETE REFERENCE TABLE (all exam values)     â•‘  â”‚
â”‚  â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  â”‚
â”‚                                                                 â”‚
â”‚  Prob  â”‚How logâ‚â‚€ is derived          â”‚logâ‚â‚€(x) â”‚logâ‚‚(x)      â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”‚
â”‚  1.000 â”‚logâ‚â‚€(1)= 0 always           â”‚  0.0000 â”‚  0.000       â”‚
â”‚  0.900 â”‚logâ‚â‚€(9/10)=logâ‚â‚€(9)âˆ’1       â”‚ âˆ’0.0458 â”‚ âˆ’0.152       â”‚
â”‚  0.800 â”‚logâ‚â‚€(8/10)=3Ã—0.301âˆ’1        â”‚ âˆ’0.0969 â”‚ âˆ’0.322       â”‚
â”‚  0.750 â”‚logâ‚â‚€(3/4)=0.4771âˆ’0.6021     â”‚ âˆ’0.1249 â”‚ âˆ’0.415       â”‚
â”‚  0.667 â”‚logâ‚â‚€(2/3)=0.3010âˆ’0.4771     â”‚ âˆ’0.1761 â”‚ âˆ’0.585       â”‚
â”‚  0.625 â”‚logâ‚â‚€(5/8)=0.699âˆ’0.903       â”‚ âˆ’0.2041 â”‚ âˆ’0.678       â”‚
â”‚  0.600 â”‚logâ‚â‚€(6/10)=0.7781âˆ’1.000     â”‚ âˆ’0.2219 â”‚ âˆ’0.737  â†   â”‚
â”‚  0.500 â”‚logâ‚â‚€(1/2)=0âˆ’0.3010          â”‚ âˆ’0.3010 â”‚ âˆ’1.000       â”‚
â”‚  0.400 â”‚logâ‚â‚€(4/10)=0.6021âˆ’1.000     â”‚ âˆ’0.3979 â”‚ âˆ’1.322  â†   â”‚
â”‚  0.375 â”‚logâ‚â‚€(3/8)=0.4771âˆ’0.9031     â”‚ âˆ’0.4260 â”‚ âˆ’1.415       â”‚
â”‚  0.333 â”‚logâ‚â‚€(1/3)=0âˆ’0.4771          â”‚ âˆ’0.4771 â”‚ âˆ’1.585       â”‚
â”‚  0.300 â”‚logâ‚â‚€(3/10)=0.4771âˆ’1.000     â”‚ âˆ’0.5229 â”‚ âˆ’1.737       â”‚
â”‚  0.250 â”‚logâ‚â‚€(1/4)=0âˆ’0.6021          â”‚ âˆ’0.6021 â”‚ âˆ’2.000       â”‚
â”‚  0.200 â”‚logâ‚â‚€(2/10)=0.3010âˆ’1.000     â”‚ âˆ’0.6990 â”‚ âˆ’2.322       â”‚
â”‚  0.167 â”‚logâ‚â‚€(1/6)=0âˆ’0.7781          â”‚ âˆ’0.7781 â”‚ âˆ’2.585       â”‚
â”‚  0.143 â”‚logâ‚â‚€(1/7)â‰ˆâˆ’0.8451           â”‚ âˆ’0.8451 â”‚ âˆ’2.807       â”‚
â”‚  0.125 â”‚logâ‚â‚€(1/8)=0âˆ’3Ã—0.3010        â”‚ âˆ’0.9031 â”‚ âˆ’3.000       â”‚
â”‚  0.100 â”‚logâ‚â‚€(1/10)=0âˆ’1.000          â”‚ âˆ’1.0000 â”‚ âˆ’3.322       â”‚
â”‚                                                                 â”‚
â”‚  â† marks the two values used in our loan dataset example       â”‚
â”‚                                                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—  â”‚
â”‚  â•‘  PART G â€” QUICK MENTAL ESTIMATION TRICK                  â•‘  â”‚
â”‚  â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  â”‚
â”‚                                                                 â”‚
â”‚  Anchor points (exact, memorise 4):                            â”‚
â”‚    logâ‚‚(1.000) =  0    logâ‚‚(0.500) = âˆ’1                        â”‚
â”‚    logâ‚‚(0.250) = âˆ’2    logâ‚‚(0.125) = âˆ’3                        â”‚
â”‚                                                                 â”‚
â”‚  For any probability p between two anchors, interpolate:       â”‚
â”‚                                                                 â”‚
â”‚  Estimate logâ‚‚(0.6):                                           â”‚
â”‚    0.6 is between 0.5 (log=âˆ’1) and 1.0 (log=0)                â”‚
â”‚    0.6 is 20% of the way from 0.5â†’1.0                          â”‚
â”‚    [ (0.6âˆ’0.5)/(1.0âˆ’0.5) = 0.1/0.5 = 0.2 ]                   â”‚
â”‚    Estimate = âˆ’1 + 0.2Ã—(0âˆ’(âˆ’1)) = âˆ’1 + 0.2 = âˆ’0.80            â”‚
â”‚    Actual = âˆ’0.737  (estimate was good, within 8%)             â”‚
â”‚                                                                 â”‚
â”‚  Estimate logâ‚‚(0.4):                                           â”‚
â”‚    0.4 is between 0.25 (log=âˆ’2) and 0.5 (log=âˆ’1)              â”‚
â”‚    0.4 is 60% of the way from 0.25â†’0.5                         â”‚
â”‚    [ (0.4âˆ’0.25)/(0.5âˆ’0.25) = 0.15/0.25 = 0.6 ]               â”‚
â”‚    Estimate = âˆ’2 + 0.6Ã—(âˆ’1âˆ’(âˆ’2)) = âˆ’2 + 0.6 = âˆ’1.40           â”‚
â”‚    Actual = âˆ’1.322  (estimate was good, within 6%)             â”‚
â”‚                                                                 â”‚
â”‚  Use estimates for sanity checks â€” not for final answers!      â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 5 â€” Multiply probability Ã— log value                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  For class Yes:  0.6  Ã— (âˆ’0.7370) = âˆ’0.4422                    â”‚
â”‚  For class No:   0.4  Ã— (âˆ’1.3219) = âˆ’0.5288                    â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 6 â€” Apply the negative sign (the âˆ’ in front of Î£)        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  âˆ’ (âˆ’0.4422) = +0.4422                                          â”‚
â”‚  âˆ’ (âˆ’0.5288) = +0.5288                                          â”‚
â”‚                                                                 â”‚
â”‚  ğŸ’¡ The double negative makes entropy always POSITIVE           â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 7 â€” Sum all terms                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  entropy(D) = 0.4422 + 0.5288                                   â”‚
â”‚                                                                 â”‚
â”‚  entropy(D) = 0.9710  â‰ˆ 0.971  âœ…                               â”‚
â”‚                                                                 â”‚
â”‚  Interpretation: Close to 1.0 â†’ dataset is quite mixed         â”‚
â”‚  (9 Yes vs 6 No â€” not too far from 50/50)                      â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

#### ğŸ”¢ Step-by-Step: Entropy Calculation (Example 2 â€” Pure Node)

**Given:** A subset with **6 Yes** and **0 No** (Own\_House = true branch).

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 1 â€” Count the classes                                     â”‚
â”‚  Yes = 6,  No = 0,  Total = 6                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  STEP 2 â€” Compute probabilities                                 â”‚
â”‚  Pr(Yes) = 6/6 = 1.0                                            â”‚
â”‚  Pr(No)  = 0/6 = 0.0                                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  STEP 3 â€” Apply formula                                         â”‚
â”‚  entropy = âˆ’ [1.0 Ã— logâ‚‚(1.0)] âˆ’ [0.0 Ã— logâ‚‚(0.0)]            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  STEP 4 â€” Calculate logs                                        â”‚
â”‚  logâ‚‚(1.0) = 0     (because 2â° = 1)                            â”‚
â”‚  0.0 Ã— logâ‚‚(0.0)  = 0  (special convention: 0 Ã— âˆ’âˆ = 0)        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  STEP 5 â€” Final sum                                             â”‚
â”‚  entropy = âˆ’ [1.0 Ã— 0] âˆ’ [0] = 0 + 0 = 0.000  âœ…               â”‚
â”‚                                                                 â”‚
â”‚  Interpretation: 0 = perfectly pure â†’ no uncertainty at all!   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

#### ğŸ”¢ Step-by-Step: Entropy Calculation (Example 3 â€” Subset entropy)

**Given:** A subset with **3 Yes** and **6 No** (Own\_House = false branch, total = 9).

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 1 â€” Count                                                 â”‚
â”‚  Yes = 3,  No = 6,  Total = 9                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  STEP 2 â€” Probabilities                                         â”‚
â”‚  Pr(Yes) = 3/9 = 0.333                                          â”‚
â”‚  Pr(No)  = 6/9 = 0.667                                          â”‚
â”‚  Check:  0.333 + 0.667 = 1.0  âœ…                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  STEP 3 â€” Logs                                                  â”‚
â”‚  logâ‚‚(0.333) = logâ‚â‚€(0.333) / logâ‚â‚€(2)                         â”‚
â”‚              = (âˆ’0.4771) / (0.3010) = âˆ’1.585                    â”‚
â”‚                                                                 â”‚
â”‚  logâ‚‚(0.667) = logâ‚â‚€(0.667) / logâ‚â‚€(2)                         â”‚
â”‚              = (âˆ’0.1761) / (0.3010) = âˆ’0.585                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  STEP 4 â€” Multiply                                              â”‚
â”‚  0.333 Ã— (âˆ’1.585) = âˆ’0.528                                      â”‚
â”‚  0.667 Ã— (âˆ’0.585) = âˆ’0.390                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  STEP 5 â€” Negate and sum                                        â”‚
â”‚  entropy = âˆ’(âˆ’0.528) + âˆ’(âˆ’0.390)                                â”‚
â”‚          = 0.528 + 0.390                                        â”‚
â”‚          = 0.918  âœ…                                             â”‚
â”‚                                                                 â”‚
â”‚  Interpretation: 0.918 is quite high â†’ still very mixed        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

#### ğŸ“Š Entropy Intuition Table

| Pr(positive) | Pr(negative) | Calculation | Entropy | Meaning |
|---|---|---|---|---|
| 1/2 = 0.500 | 1/2 = 0.500 | âˆ’0.5Ã—(âˆ’1) âˆ’ 0.5Ã—(âˆ’1) | **1.000** | Maximum confusion |
| 2/10 = 0.200 | 8/10 = 0.800 | âˆ’0.2Ã—(âˆ’2.322) âˆ’ 0.8Ã—(âˆ’0.322) | **0.722** | Leaning negative |
| 9/10 = 0.900 | 1/10 = 0.100 | âˆ’0.9Ã—(âˆ’0.152) âˆ’ 0.1Ã—(âˆ’3.322) | **0.469** | Mostly positive |
| 1.000 | 0.000 | âˆ’1Ã—0 âˆ’ 0Ã—(âˆ’âˆ) | **0.000** | Perfectly pure |

#### ğŸ“ˆ Entropy Curve

```
Entropy
  1.0 â”¤         â—         â† Maximum (50/50 split)
      â”‚      â—     â—
  0.8 â”¤    â—         â—
      â”‚  â—             â—
  0.6 â”¤â—                 â—
      â”‚                    â—
  0.4 â”¤                      â—
      â”‚                        â—
  0.2 â”¤                          â—
      â”‚                             â—
  0.0 â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—  â† Zero (pure node)
      0   0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1.0
                              Proportion of Positive Class

Key takeaway:
  entropy = 0    â†’ data is 100% one class   â†’ leaf node, stop splitting!
  entropy = 1    â†’ data is 50/50 split      â†’ most impure, must split!
  entropy = 0.97 â†’ close to 1               â†’ still quite mixed (our root node)
```

---

### 5.2 Information Gain

#### What is Information Gain?

Information Gain tells us **how much an attribute reduces uncertainty** in the data. We always pick the attribute with the **highest gain**.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CONCEPT: Gain = disorder BEFORE âˆ’ disorder AFTER split      â”‚
â”‚                                                              â”‚
â”‚  Think of it as:                                             â”‚
â”‚  "If I split the data on attribute A, how much cleaner       â”‚
â”‚   do the resulting groups become?"                           â”‚
â”‚                                                              â”‚
â”‚  Higher gain = attribute creates purer sub-groups            â”‚
â”‚  Lower  gain = attribute barely helps                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

#### ğŸ“ The Three Formulas

**Formula 1 â€” Entropy of the current node (before split):**

$$\text{entropy}(D) = -\sum_{j=1}^{|C|} \Pr(c_j) \cdot \log_2 \Pr(c_j)$$

**Formula 2 â€” Expected (weighted) entropy after splitting on attribute Aáµ¢:**

$$\text{entropy}_{A_i}(D) = \sum_{j=1}^{v} \frac{|D_j|}{|D|} \times \text{entropy}(D_j)$$

**Formula 3 â€” Information Gain:**

$$\text{gain}(D, A_i) = \text{entropy}(D) - \text{entropy}_{A_i}(D)$$

**Complete Notation Reference:**

| Symbol | Meaning | Example |
|---|---|---|
| `D` | Full dataset at current node | All 15 records |
| `Aáµ¢` | Attribute being evaluated | "Age" or "Own House" |
| `v` | Number of distinct values of Aáµ¢ | Age has 3: young/middle/old |
| `j` | Index for each subset | j=1 (young), j=2 (middle), j=3 (old) |
| `Dâ±¼` | Subset of D where Aáµ¢ = its j-th value | Dâ‚ = all young examples |
| `\|Dâ±¼\|` | Size (count) of subset Dâ±¼ | 5 young examples |
| `\|D\|` | Total size of current dataset | 15 total |
| `\|Dâ±¼\|/\|D\|` | Weight = proportion of data in this subset | 5/15 = 0.333 |
| `entropy(Dâ±¼)` | Entropy of the j-th subset | entropy of young group |

---

#### ğŸ”¢ Step-by-Step: Information Gain for Own House

**Split: Own\_House = true (6 examples) vs Own\_House = false (9 examples)**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 1 â€” We already know entropy(D) = 0.971  (computed above) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 2 â€” Identify the subsets created by this split           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Subset Dâ‚ (Own House = true):                                  â”‚
â”‚    Yes = 6,  No = 0,  Size = 6                                  â”‚
â”‚                                                                 â”‚
â”‚  Subset Dâ‚‚ (Own House = false):                                 â”‚
â”‚    Yes = 3,  No = 6,  Size = 9                                  â”‚
â”‚                                                                 â”‚
â”‚  Check: |Dâ‚| + |Dâ‚‚| = 6 + 9 = 15 = |D|  âœ…                    â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 3 â€” Compute entropy of each subset                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  entropy(Dâ‚) â€” Own House = true branch:                        â”‚
â”‚    Pr(Yes) = 6/6 = 1.0,  Pr(No) = 0/6 = 0.0                   â”‚
â”‚    entropy = âˆ’(1.0 Ã— logâ‚‚1.0) âˆ’ (0.0 Ã— logâ‚‚0.0)               â”‚
â”‚            = âˆ’(1.0 Ã— 0) âˆ’ 0                                    â”‚
â”‚            = 0.000  â† PURE! âœ…                                  â”‚
â”‚                                                                 â”‚
â”‚  entropy(Dâ‚‚) â€” Own House = false branch:                       â”‚
â”‚    Pr(Yes) = 3/9 = 0.333,  Pr(No) = 6/9 = 0.667               â”‚
â”‚    logâ‚‚(0.333) = âˆ’1.585,  logâ‚‚(0.667) = âˆ’0.585                 â”‚
â”‚    entropy = âˆ’(0.333 Ã— âˆ’1.585) âˆ’ (0.667 Ã— âˆ’0.585)              â”‚
â”‚            = âˆ’(âˆ’0.528) âˆ’ (âˆ’0.390)                               â”‚
â”‚            = 0.528 + 0.390                                      â”‚
â”‚            = 0.918                                              â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 4 â€” Compute the weights for each subset                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Weight of Dâ‚ = |Dâ‚| / |D| = 6 / 15 = 0.400                   â”‚
â”‚  Weight of Dâ‚‚ = |Dâ‚‚| / |D| = 9 / 15 = 0.600                   â”‚
â”‚                                                                 â”‚
â”‚  Check: 0.400 + 0.600 = 1.0  âœ…                                 â”‚
â”‚                                                                 â”‚
â”‚  Why weights? Because larger subsets matter more!               â”‚
â”‚  A 14-example pure subset is more impressive than a            â”‚
â”‚  1-example pure subset.                                         â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 5 â€” Compute the weighted (expected) entropy              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  entropy_OwnHouse(D) = (6/15) Ã— entropy(Dâ‚)                    â”‚
â”‚                      + (9/15) Ã— entropy(Dâ‚‚)                    â”‚
â”‚                                                                 â”‚
â”‚                      = (6/15) Ã— 0.000                           â”‚
â”‚                      + (9/15) Ã— 0.918                           â”‚
â”‚                                                                 â”‚
â”‚                      = 0.400 Ã— 0.000                            â”‚
â”‚                      + 0.600 Ã— 0.918                            â”‚
â”‚                                                                 â”‚
â”‚                      = 0.000 + 0.551                            â”‚
â”‚                                                                 â”‚
â”‚                      = 0.551                                    â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 6 â€” Compute Information Gain                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  gain(D, Own House) = entropy(D) âˆ’ entropy_OwnHouse(D)         â”‚
â”‚                     = 0.971     âˆ’  0.551                        â”‚
â”‚                     = 0.420   âœ…  â† HIGHEST GAIN               â”‚
â”‚                                                                 â”‚
â”‚  Meaning: Splitting on Own House reduces entropy               â”‚
â”‚           from 0.971 â†’ 0.551 (a drop of 0.420)                 â”‚
â”‚           One branch becomes perfectly pure!                    â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 5.3 Gain Ratio

#### Why Gain Ratio Is Needed

Information Gain has a bias problem. Consider a useless attribute like "Customer ID" â€” it has 15 unique values for 15 examples, creating 15 perfectly pure subsets of 1 example each. Gain would be maximum (0.971), but this attribute is **completely useless** for prediction!

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PROBLEM: Gain is biased toward high-cardinality attributes      â”‚
â”‚                                                                  â”‚
â”‚  Customer ID: 15 values â†’ 15 subsets of 1 â†’ each is pure        â”‚
â”‚               Gain = 0.971  (looks amazing but is useless!)     â”‚
â”‚                                                                  â”‚
â”‚  Own House:   2 values â†’ 2 subsets â†’ one is pure                â”‚
â”‚               Gain = 0.420  (actually useful!)                  â”‚
â”‚                                                                  â”‚
â”‚  FIX: Divide gain by how "wide" the split was â†’ Gain Ratio      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

#### ğŸ“ The Gain Ratio Formulas

**Formula 1 â€” Split Information (measures how wide/even the split is):**

$$\text{SplitInfo}(A_i) = -\sum_{j=1}^{v} \frac{|D_j|}{|D|} \cdot \log_2 \frac{|D_j|}{|D|}$$

> ğŸ’¡ SplitInfo is **exactly entropy applied to the split proportions** (not the class labels). It penalises attributes that split data into many or very unequal parts.

**Formula 2 â€” Gain Ratio:**

$$\text{GainRatio}(D, A_i) = \frac{\text{gain}(D, A_i)}{\text{SplitInfo}(A_i)}$$

---

#### ğŸ”¢ Step-by-Step: Gain Ratio for Own House

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 1 â€” We already know:                                      â”‚
â”‚    gain(D, Own House) = 0.420                                   â”‚
â”‚    Split: 6 examples in Dâ‚, 9 examples in Dâ‚‚, total = 15       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 2 â€” Compute split proportions                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  |Dâ‚|/|D| = 6/15 = 0.400                                       â”‚
â”‚  |Dâ‚‚|/|D| = 9/15 = 0.600                                       â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 3 â€” Compute logs of split proportions                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  logâ‚‚(0.400) = logâ‚â‚€(0.400) / logâ‚â‚€(2)                         â”‚
â”‚              = (âˆ’0.3979) / (0.3010) = âˆ’1.3219                   â”‚
â”‚                                                                 â”‚
â”‚  logâ‚‚(0.600) = logâ‚â‚€(0.600) / logâ‚â‚€(2)                         â”‚
â”‚              = (âˆ’0.2218) / (0.3010) = âˆ’0.7370                   â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 4 â€” Compute SplitInfo                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  SplitInfo(Own House) = âˆ’ [0.400 Ã— (âˆ’1.3219)]                   â”‚
â”‚                        âˆ’ [0.600 Ã— (âˆ’0.7370)]                   â”‚
â”‚                                                                 â”‚
â”‚                       = âˆ’ (âˆ’0.5288) âˆ’ (âˆ’0.4422)                 â”‚
â”‚                       = 0.5288 + 0.4422                         â”‚
â”‚                       = 0.9710                                  â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 5 â€” Compute Gain Ratio                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  GainRatio(D, Own House) = gain / SplitInfo                     â”‚
â”‚                          = 0.420 / 0.971                        â”‚
â”‚                          = 0.432  âœ…                             â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

#### ğŸ”¢ Step-by-Step: Gain Ratio for Age (to see penalty in action)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 1 â€” We already know:                                      â”‚
â”‚    gain(D, Age) = 0.083                                         â”‚
â”‚    Split: 5 young, 5 middle, 5 old  (perfectly equal thirds)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 2 â€” Split proportions                                    â”‚
â”‚    5/15 = 5/15 = 5/15 = 0.333 each                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 3 â€” Log of split proportion                              â”‚
â”‚    logâ‚‚(0.333) = logâ‚â‚€(0.333)/logâ‚â‚€(2) = âˆ’0.4771/0.3010       â”‚
â”‚               = âˆ’1.585                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 4 â€” SplitInfo for Age                                    â”‚
â”‚    = âˆ’[0.333Ã—(âˆ’1.585)] âˆ’ [0.333Ã—(âˆ’1.585)] âˆ’ [0.333Ã—(âˆ’1.585)]  â”‚
â”‚    = 3 Ã— [0.333 Ã— 1.585]                                        â”‚
â”‚    = 3 Ã— 0.528 = 1.585                                          â”‚
â”‚                                                                 â”‚
â”‚  Note: 1.585 = logâ‚‚(3) â€” 3 equal branches always gives this    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 5 â€” Gain Ratio for Age                                   â”‚
â”‚    = gain / SplitInfo = 0.083 / 1.585 = 0.052                  â”‚
â”‚                                                                 â”‚
â”‚  Compare:                                                       â”‚
â”‚    GainRatio(Own House) = 0.432   â† wins by a huge margin!     â”‚
â”‚    GainRatio(Age)       = 0.052   â† penalised for 3 branches   â”‚
â”‚                                                                 â”‚
â”‚  Conclusion: Own House is STILL the best root  âœ…               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 5.4 Gini Impurity (CART)

Used by the **CART** algorithm (sklearn's default in Python).

#### ğŸ“ The Formula

$$\text{Gini}(D) = 1 - \sum_{j=1}^{|C|} \Pr(c_j)^2$$

**Breaking down every symbol:**

| Symbol | Meaning |
|---|---|
| `1` | We start from 1 (maximum) and subtract purity |
| `Pr(câ±¼)Â²` | Square of each class probability |
| `Î£ Pr(câ±¼)Â²` | Sum of squared probabilities = measure of "how pure" |
| `1 âˆ’ Î£ Pr(câ±¼)Â²` | 1 minus purity = impurity |

> ğŸ’¡ **Intuition:** If all examples are one class, Pr = 1.0 â†’ 1Â² = 1 â†’ Gini = 1 âˆ’ 1 = 0 (pure). If 50/50 split â†’ (0.5Â² + 0.5Â²) = 0.5 â†’ Gini = 1 âˆ’ 0.5 = 0.5 (maximum impurity).

---

#### ğŸ”¢ Step-by-Step: Gini for the Root Node (9 Yes, 6 No)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 1 â€” Class probabilities                                   â”‚
â”‚    Pr(Yes) = 9/15 = 0.600                                       â”‚
â”‚    Pr(No)  = 6/15 = 0.400                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 2 â€” Square each probability                              â”‚
â”‚    Pr(Yes)Â² = 0.600Â² = 0.360                                    â”‚
â”‚    Pr(No)Â²  = 0.400Â² = 0.160                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 3 â€” Sum the squares                                      â”‚
â”‚    Î£ Pr(câ±¼)Â² = 0.360 + 0.160 = 0.520                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 4 â€” Subtract from 1                                      â”‚
â”‚    Gini(D) = 1 âˆ’ 0.520 = 0.480  âœ…                              â”‚
â”‚                                                                 â”‚
â”‚  Compare: Entropy was 0.971 â†’ different scale but same meaning â”‚
â”‚  Both tell us the data is impure and needs splitting            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

#### ğŸ“Š Entropy vs Gini â€” Side-by-Side Comparison

| Scenario | Pr(+) | Pr(âˆ’) | Entropy Calculation | Entropy | Gini Calculation | Gini |
|---|---|---|---|---|---|---|
| Equal split | 0.5 | 0.5 | âˆ’0.5logâ‚‚0.5âˆ’0.5logâ‚‚0.5 | **1.000** | 1âˆ’(0.25+0.25) | **0.500** |
| 80/20 split | 0.8 | 0.2 | âˆ’0.8logâ‚‚0.8âˆ’0.2logâ‚‚0.2 | **0.722** | 1âˆ’(0.64+0.04) | **0.320** |
| 90/10 split | 0.9 | 0.1 | âˆ’0.9logâ‚‚0.9âˆ’0.1logâ‚‚0.1 | **0.469** | 1âˆ’(0.81+0.01) | **0.180** |
| Pure node   | 1.0 | 0.0 | âˆ’1Ã—0 âˆ’ 0 | **0.000** | 1âˆ’(1.00+0.00) | **0.000** |

**Key difference:** Entropy is more sensitive to class imbalance (uses log), Gini is faster to compute (no log). Both produce the same tree most of the time.

---

## 6. The Algorithm

```
Algorithm: decisionTree(D, A, T)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
INPUT:
  D = set of training examples
  A = set of available attributes
  T = current tree node

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1.  IF all examples in D belong to same class cáµ¢:
        â†’ make T a LEAF node labeled cáµ¢
        â†’ RETURN

2.  ELSE IF A is empty (no attributes left):
        â†’ make T a LEAF node labeled with MAJORITY class in D
        â†’ RETURN

3.  ELSE:  (D has mixed classes, attributes available)

    a. Compute pâ‚€ = entropy(D)      â† baseline impurity

    b. FOR each attribute Aáµ¢ in A:
           compute páµ¢ = entropy_Aáµ¢(D)   â† post-split impurity

    c. Select Ag = attribute with MAXIMUM (pâ‚€ - páµ¢)
                                         â† best gain

    d. IF (pâ‚€ - p_Ag) < threshold:       â† gain too small
           â†’ make T a LEAF with majority class
           â†’ RETURN   (Pre-pruning)

    e. ELSE:
           â†’ make T a DECISION NODE on Ag

           FOR each value vâ±¼ of Ag:
               Dâ±¼ = subset of D where Ag = vâ±¼
               IF Dâ±¼ â‰  empty:
                   create child node Tâ±¼
                   decisionTree(Dâ±¼, A - {Ag}, Tâ±¼)  â† recurse!
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

### Key Properties

```
TYPE:       Greedy, top-down, recursive divide-and-conquer
GUARANTEE:  Locally optimal (not globally optimal)
COMPLEXITY: Finding BEST tree is NP-hard â†’ use heuristics
UNIQUENESS: Decision trees are NOT unique for the same data
```

---

## 7. Worked Example â€” Loan Dataset

### The Data (15 examples, 4 features)

| ID | Age | Has\_Job | Own\_House | Credit\_Rating | Class |
|---|---|---|---|---|---|
| 1 | young | false | false | fair | **No** |
| 2 | young | false | false | good | **No** |
| 3 | young | true | false | good | **Yes** |
| 4 | young | true | true | fair | **Yes** |
| 5 | young | false | false | fair | **No** |
| 6 | middle | false | false | fair | **No** |
| 7 | middle | false | false | good | **No** |
| 8 | middle | true | true | good | **Yes** |
| 9 | middle | false | true | excellent | **Yes** |
| 10 | middle | false | true | excellent | **Yes** |
| 11 | old | false | true | excellent | **Yes** |
| 12 | old | false | true | good | **Yes** |
| 13 | old | true | false | good | **Yes** |
| 14 | old | true | false | excellent | **Yes** |
| 15 | old | false | false | fair | **No** |

**Summary:** 9 Yes, 6 No out of 15 total

---

### â–¶ STEP 1 â€” Compute Root Entropy

**Goal:** Find entropy(D) for the entire dataset before any split.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Count classes:                                                 â”‚
â”‚    Yes = 9  (IDs: 3,4,8,9,10,11,12,13,14)                      â”‚
â”‚    No  = 6  (IDs: 1,2,5,6,7,15)                                 â”‚
â”‚    Total = 15                                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Compute probabilities:                                         â”‚
â”‚    Pr(Yes) = 9/15 = 0.600                                       â”‚
â”‚    Pr(No)  = 6/15 = 0.400                                       â”‚
â”‚    Check:  0.600 + 0.400 = 1.0  âœ…                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Compute logs:                                                  â”‚
â”‚    logâ‚‚(0.600) = logâ‚â‚€(0.600)/logâ‚â‚€(2) = âˆ’0.2218/0.3010       â”‚
â”‚                = âˆ’0.7370                                        â”‚
â”‚    logâ‚‚(0.400) = logâ‚â‚€(0.400)/logâ‚â‚€(2) = âˆ’0.3979/0.3010       â”‚
â”‚                = âˆ’1.3219                                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Multiply and negate:                                           â”‚
â”‚    Yes term: âˆ’(0.600 Ã— âˆ’0.7370) = âˆ’(âˆ’0.4422) = +0.4422         â”‚
â”‚    No  term: âˆ’(0.400 Ã— âˆ’1.3219) = âˆ’(âˆ’0.5288) = +0.5288         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Sum:                                                           â”‚
â”‚    entropy(D) = 0.4422 + 0.5288 = 0.9710 â‰ˆ 0.971  âœ…           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

$$\text{entropy}(D) = -\frac{9}{15}\log_2\frac{9}{15} - \frac{6}{15}\log_2\frac{6}{15} = \mathbf{0.971}$$

---

### â–¶ STEP 2 â€” Compute Gain for Every Attribute

We compute gain for all 4 attributes and pick the winner.

---

#### ğŸ“Œ Attribute A: Age (3 values: young / middle / old)

**First, separate the data into 3 groups:**

```
Group young  (IDs 1â€“5):  Yes=2, No=3, Total=5
Group middle (IDs 6â€“10): Yes=3, No=2, Total=5
Group old    (IDs 11â€“15): Yes=4, No=1, Total=5
```

**Compute entropy for each group:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ entropy(D_young) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Yes=2, No=3, Total=5                                        â”‚
â”‚  Pr(Yes) = 2/5 = 0.400,  Pr(No) = 3/5 = 0.600               â”‚
â”‚                                                              â”‚
â”‚  logâ‚‚(0.400) = âˆ’1.3219                                       â”‚
â”‚  logâ‚‚(0.600) = âˆ’0.7370                                       â”‚
â”‚                                                              â”‚
â”‚  entropy = âˆ’(0.400 Ã— âˆ’1.3219) âˆ’ (0.600 Ã— âˆ’0.7370)           â”‚
â”‚          = 0.5288 + 0.4422                                   â”‚
â”‚          = 0.9710 â‰ˆ 0.971                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ entropy(D_middle) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Yes=3, No=2, Total=5                                        â”‚
â”‚  Pr(Yes) = 3/5 = 0.600,  Pr(No) = 2/5 = 0.400               â”‚
â”‚                                                              â”‚
â”‚  Same probabilities as young (just swapped) â†’ same entropy  â”‚
â”‚  entropy = 0.971                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ entropy(D_old) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Yes=4, No=1, Total=5                                        â”‚
â”‚  Pr(Yes) = 4/5 = 0.800,  Pr(No) = 1/5 = 0.200               â”‚
â”‚                                                              â”‚
â”‚  logâ‚‚(0.800) = logâ‚â‚€(0.800)/logâ‚â‚€(2) = âˆ’0.0969/0.3010       â”‚
â”‚              = âˆ’0.3219                                       â”‚
â”‚  logâ‚‚(0.200) = logâ‚â‚€(0.200)/logâ‚â‚€(2) = âˆ’0.6990/0.3010       â”‚
â”‚              = âˆ’2.3219                                       â”‚
â”‚                                                              â”‚
â”‚  entropy = âˆ’(0.800 Ã— âˆ’0.3219) âˆ’ (0.200 Ã— âˆ’2.3219)           â”‚
â”‚          = 0.2575 + 0.4644                                   â”‚
â”‚          = 0.7219 â‰ˆ 0.722                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Compute weighted entropy for Age:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  entropy_Age(D) = (|D_young|/|D|) Ã— entropy(D_young)           â”‚
â”‚                + (|D_middle|/|D|) Ã— entropy(D_middle)          â”‚
â”‚                + (|D_old|/|D|)   Ã— entropy(D_old)              â”‚
â”‚                                                                 â”‚
â”‚                = (5/15) Ã— 0.971                                 â”‚
â”‚                + (5/15) Ã— 0.971                                 â”‚
â”‚                + (5/15) Ã— 0.722                                 â”‚
â”‚                                                                 â”‚
â”‚                = 0.3333 Ã— 0.971                                 â”‚
â”‚                + 0.3333 Ã— 0.971                                 â”‚
â”‚                + 0.3333 Ã— 0.722                                 â”‚
â”‚                                                                 â”‚
â”‚                = 0.3237 + 0.3237 + 0.2407                      â”‚
â”‚                                                                 â”‚
â”‚                = 0.888                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Compute Gain for Age:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  gain(D, Age) = entropy(D) âˆ’ entropy_Age(D)                    â”‚
â”‚              = 0.971 âˆ’ 0.888                                    â”‚
â”‚              = 0.083                                            â”‚
â”‚                                                                 â”‚
â”‚  Interpretation: Age reduces entropy by only 0.083 â€” not much! â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

| Age group | Yes | No | Total | Weight | Entropy | Weighted |
|---|---|---|---|---|---|---|
| young | 2 | 3 | 5 | 5/15=0.333 | 0.971 | 0.324 |
| middle | 3 | 2 | 5 | 5/15=0.333 | 0.971 | 0.324 |
| old | 4 | 1 | 5 | 5/15=0.333 | 0.722 | 0.241 |
| **Total** | | | | | **entropy\_Age** | **0.888** |

$$\text{gain}(D, \text{Age}) = 0.971 - 0.888 = \mathbf{0.083}$$

---

#### ğŸ“Œ Attribute B: Own House (2 values: true / false)

**Separate data:**

```
Group true  (IDs 4,8,9,10,11,12): Yes=6, No=0, Total=6
Group false (IDs 1,2,3,5,6,7,13,14,15): Yes=3, No=6, Total=9
```

**Compute entropy for each group:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ entropy(D_true) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Yes=6, No=0, Total=6                                        â”‚
â”‚  Pr(Yes) = 6/6 = 1.000,  Pr(No) = 0/6 = 0.000               â”‚
â”‚                                                              â”‚
â”‚  logâ‚‚(1.000) = 0   (because 2â° = 1)                         â”‚
â”‚  0.000 Ã— logâ‚‚(0.000) = 0  (convention)                      â”‚
â”‚                                                              â”‚
â”‚  entropy = âˆ’(1.000 Ã— 0) âˆ’ (0)                                â”‚
â”‚          = 0.000  â† PERFECTLY PURE âœ…                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ entropy(D_false) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Yes=3, No=6, Total=9                                        â”‚
â”‚  Pr(Yes) = 3/9 = 0.333,  Pr(No) = 6/9 = 0.667               â”‚
â”‚                                                              â”‚
â”‚  logâ‚‚(0.333) = logâ‚â‚€(0.333)/logâ‚â‚€(2) = âˆ’0.4771/0.3010       â”‚
â”‚              = âˆ’1.5850                                       â”‚
â”‚  logâ‚‚(0.667) = logâ‚â‚€(0.667)/logâ‚â‚€(2) = âˆ’0.1761/0.3010       â”‚
â”‚              = âˆ’0.5850                                       â”‚
â”‚                                                              â”‚
â”‚  entropy = âˆ’(0.333 Ã— âˆ’1.5850) âˆ’ (0.667 Ã— âˆ’0.5850)           â”‚
â”‚          = 0.5278 + 0.3902                                   â”‚
â”‚          = 0.9180 â‰ˆ 0.918                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Compute weighted entropy for Own House:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  entropy_OwnHouse(D) = (6/15) Ã— entropy(D_true)                â”‚
â”‚                      + (9/15) Ã— entropy(D_false)               â”‚
â”‚                                                                 â”‚
â”‚                      = (6/15) Ã— 0.000                           â”‚
â”‚                      + (9/15) Ã— 0.918                           â”‚
â”‚                                                                 â”‚
â”‚                      = 0.400 Ã— 0.000                            â”‚
â”‚                      + 0.600 Ã— 0.918                            â”‚
â”‚                                                                 â”‚
â”‚                      = 0.000 + 0.551                            â”‚
â”‚                                                                 â”‚
â”‚                      = 0.551                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Compute Gain for Own House:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  gain(D, Own House) = entropy(D) âˆ’ entropy_OwnHouse(D)         â”‚
â”‚                     = 0.971 âˆ’ 0.551                             â”‚
â”‚                     = 0.420  âœ…  â† HIGHEST GAIN!               â”‚
â”‚                                                                 â”‚
â”‚  Why so high? Because the "true" branch is perfectly pure      â”‚
â”‚  (all 6 examples are Yes â†’ entropy = 0)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

| Own House | Yes | No | Total | Weight | Entropy | Weighted |
|---|---|---|---|---|---|---|
| true | 6 | 0 | 6 | 6/15=0.400 | 0.000 | 0.000 |
| false | 3 | 6 | 9 | 9/15=0.600 | 0.918 | 0.551 |
| **Total** | | | | | **entropy\_OwnHouse** | **0.551** |

$$\text{gain}(D, \text{OwnHouse}) = 0.971 - 0.551 = \mathbf{0.420}$$ âœ… **HIGHEST!**

---

#### ğŸ“Œ Attribute C: Has Job (2 values: true / false)

**Separate data:**

```
Group true  (IDs 3,4,8,13,14): Yes=5, No=0, Total=5
Group false (IDs 1,2,5,6,7,9,10,11,12,15): Yes=4, No=6, Total=10
```

**Compute entropy for each group:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ entropy(D_true) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Yes=5, No=0, Total=5                                        â”‚
â”‚  Pr(Yes) = 5/5 = 1.000,  Pr(No) = 0/5 = 0.000               â”‚
â”‚  entropy = 0.000  â† PURE âœ…                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ entropy(D_false) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Yes=4, No=6, Total=10                                       â”‚
â”‚  Pr(Yes) = 4/10 = 0.400,  Pr(No) = 6/10 = 0.600             â”‚
â”‚                                                              â”‚
â”‚  logâ‚‚(0.400) = âˆ’1.3219                                       â”‚
â”‚  logâ‚‚(0.600) = âˆ’0.7370                                       â”‚
â”‚                                                              â”‚
â”‚  entropy = âˆ’(0.400 Ã— âˆ’1.3219) âˆ’ (0.600 Ã— âˆ’0.7370)           â”‚
â”‚          = 0.5288 + 0.4422                                   â”‚
â”‚          = 0.971                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Compute weighted entropy for Has Job:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  entropy_HasJob(D) = (5/15) Ã— entropy(D_true)                  â”‚
â”‚                   + (10/15) Ã— entropy(D_false)                  â”‚
â”‚                                                                 â”‚
â”‚                   = (5/15) Ã— 0.000                              â”‚
â”‚                   + (10/15) Ã— 0.971                             â”‚
â”‚                                                                 â”‚
â”‚                   = 0.3333 Ã— 0.000                              â”‚
â”‚                   + 0.6667 Ã— 0.971                              â”‚
â”‚                                                                 â”‚
â”‚                   = 0.000 + 0.647                               â”‚
â”‚                                                                 â”‚
â”‚                   = 0.647                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Compute Gain for Has Job:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  gain(D, Has Job) = entropy(D) âˆ’ entropy_HasJob(D)             â”‚
â”‚                  = 0.971 âˆ’ 0.647                                â”‚
â”‚                  = 0.324                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

| Has Job | Yes | No | Total | Weight | Entropy | Weighted |
|---|---|---|---|---|---|---|
| true | 5 | 0 | 5 | 5/15=0.333 | 0.000 | 0.000 |
| false | 4 | 6 | 10 | 10/15=0.667 | 0.971 | 0.647 |
| **Total** | | | | | **entropy\_HasJob** | **0.647** |

$$\text{gain}(D, \text{HasJob}) = 0.971 - 0.647 = \mathbf{0.324}$$

---

#### ğŸ“Œ Attribute D: Credit Rating (3 values: fair / good / excellent)

**Separate data:**

```
Group fair      (IDs 1,4,5,6,15): Yes=1, No=4, Total=5
Group good      (IDs 2,3,7,8,12,13): Yes=4, No=2, Total=6
Group excellent (IDs 9,10,11,14): Yes=4, No=0, Total=4
```

**Compute entropy for each group:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ entropy(D_fair) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Yes=1, No=4, Total=5                                        â”‚
â”‚  Pr(Yes) = 1/5 = 0.200,  Pr(No) = 4/5 = 0.800               â”‚
â”‚                                                              â”‚
â”‚  logâ‚‚(0.200) = logâ‚â‚€(0.200)/logâ‚â‚€(2) = âˆ’0.6990/0.3010       â”‚
â”‚              = âˆ’2.3219                                       â”‚
â”‚  logâ‚‚(0.800) = logâ‚â‚€(0.800)/logâ‚â‚€(2) = âˆ’0.0969/0.3010       â”‚
â”‚              = âˆ’0.3219                                       â”‚
â”‚                                                              â”‚
â”‚  entropy = âˆ’(0.200 Ã— âˆ’2.3219) âˆ’ (0.800 Ã— âˆ’0.3219)           â”‚
â”‚          = 0.4644 + 0.2575                                   â”‚
â”‚          = 0.7219 â‰ˆ 0.722                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ entropy(D_good) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Yes=4, No=2, Total=6                                        â”‚
â”‚  Pr(Yes) = 4/6 = 0.667,  Pr(No) = 2/6 = 0.333               â”‚
â”‚                                                              â”‚
â”‚  logâ‚‚(0.667) = âˆ’0.5850                                       â”‚
â”‚  logâ‚‚(0.333) = âˆ’1.5850                                       â”‚
â”‚                                                              â”‚
â”‚  entropy = âˆ’(0.667 Ã— âˆ’0.5850) âˆ’ (0.333 Ã— âˆ’1.5850)           â”‚
â”‚          = 0.3902 + 0.5278                                   â”‚
â”‚          = 0.9180 â‰ˆ 0.918                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ entropy(D_excellent) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Yes=4, No=0, Total=4                                        â”‚
â”‚  Pr(Yes) = 4/4 = 1.000,  Pr(No) = 0/4 = 0.000               â”‚
â”‚  entropy = 0.000  â† PURE âœ…                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Compute weighted entropy for Credit Rating:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  entropy_CreditRating(D) = (5/15) Ã— entropy(D_fair)            â”‚
â”‚                          + (6/15) Ã— entropy(D_good)            â”‚
â”‚                          + (4/15) Ã— entropy(D_excellent)       â”‚
â”‚                                                                 â”‚
â”‚                          = (5/15) Ã— 0.722                       â”‚
â”‚                          + (6/15) Ã— 0.918                       â”‚
â”‚                          + (4/15) Ã— 0.000                       â”‚
â”‚                                                                 â”‚
â”‚                          = 0.3333 Ã— 0.722                       â”‚
â”‚                          + 0.4000 Ã— 0.918                       â”‚
â”‚                          + 0.2667 Ã— 0.000                       â”‚
â”‚                                                                 â”‚
â”‚                          = 0.2407 + 0.3672 + 0.000             â”‚
â”‚                                                                 â”‚
â”‚                          = 0.6079 â‰ˆ 0.608                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Compute Gain for Credit Rating:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  gain(D, Credit Rating) = entropy(D) âˆ’ entropy_CreditRating(D) â”‚
â”‚                         = 0.971 âˆ’ 0.608                         â”‚
â”‚                         = 0.363                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

| Credit Rating | Yes | No | Total | Weight | Entropy | Weighted |
|---|---|---|---|---|---|---|
| fair | 1 | 4 | 5 | 5/15=0.333 | 0.722 | 0.241 |
| good | 4 | 2 | 6 | 6/15=0.400 | 0.918 | 0.367 |
| excellent | 4 | 0 | 4 | 4/15=0.267 | 0.000 | 0.000 |
| **Total** | | | | | **entropy\_CreditRating** | **0.608** |

$$\text{gain}(D, \text{CreditRating}) = 0.971 - 0.608 = \mathbf{0.363}$$

---

### â–¶ STEP 3 â€” Compare All Gains and Pick the Root

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           GAIN COMPARISON SUMMARY                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Attribute          â”‚ Weighted Entropy â”‚ Gain       â”‚ Rank      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Age                â”‚ 0.888            â”‚ 0.083      â”‚ 4th (worst)â”‚
â”‚ Has Job            â”‚ 0.647            â”‚ 0.324      â”‚ 3rd       â”‚
â”‚ Credit Rating      â”‚ 0.608            â”‚ 0.363      â”‚ 2nd       â”‚
â”‚ Own House          â”‚ 0.551            â”‚ 0.420      â”‚ 1st âœ…    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

WINNER: Own House (gain = 0.420)
â†’ Own House becomes the ROOT NODE of the tree!

Why? Because splitting on Own House reduces entropy
from 0.971 â†’ 0.551 â€” the biggest reduction of all 4 attributes.
One branch (Own House = true) becomes perfectly pure: all 6 are Yes!
```

---

### â–¶ STEP 4 â€” Recurse on Each Branch

#### Branch 1: Own House = true â†’ 6 Yes, 0 No

```
entropy(D_true) = 0.000  â† Pure!
â†’ STOP. Make a LEAF node: Class = Yes  (6/6 correct)
```

#### Branch 2: Own House = false â†’ 3 Yes, 6 No (9 examples)

This branch is still impure (entropy = 0.918), so we recurse.
Compute gain for remaining attributes on these 9 examples:

**Re-compute root entropy for this sub-problem:**

```
Pr(Yes) = 3/9 = 0.333,  Pr(No) = 6/9 = 0.667
entropy(D_false) = âˆ’(0.333Ã—logâ‚‚0.333) âˆ’ (0.667Ã—logâ‚‚0.667)
                 = 0.528 + 0.390 = 0.918
```

**Evaluate Has Job on the 9 "false" examples:**

```
Among Own House = false records:
  Has Job = true:  IDs 3,13,14 â†’ Yes=3, No=0 â†’ Total=3 â†’ entropy=0.000
  Has Job = false: IDs 1,2,5,6,7,15 â†’ Yes=0, No=6 â†’ Total=6 â†’ entropy=0.000

entropy_HasJob(D_false) = (3/9)Ã—0.000 + (6/9)Ã—0.000 = 0.000

gain(D_false, Has Job) = 0.918 âˆ’ 0.000 = 0.918  â† PERFECT!
```

```
â†’ Has Job splits perfectly within the "false" branch:
    Has Job = true  â†’ 3 Yes, 0 No â†’ LEAF: Yes  (3/3)
    Has Job = false â†’ 0 Yes, 6 No â†’ LEAF: No   (6/6)
```

---

### â–¶ FINAL TREE

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Own_House?      â”‚  â† Root (gain=0.420)
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             true                         false
               â”‚                              â”‚
          âœ… Yes                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          (6/6)                     â”‚     Has_Job?        â”‚
                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              true                        false
                                â”‚                              â”‚
                            âœ… Yes                        âŒ No
                             (5/5)                         (4/4)

Rules extracted:
  Own House = true               â†’ Yes  [conf=6/6=100%]
  Own House = false, Has Job = true  â†’ Yes  [conf=5/5=100%]
  Own House = false, Has Job = false â†’ No   [conf=4/4=100%]

Overall accuracy: 15/15 = 100% on training data
```

---

## 8. Tree â†’ Rules Conversion

Each path from root to leaf = one IF-THEN rule

```
Own_house = true
    â†’ Class = Yes    [support=6/15=40%, confidence=6/6=100%]

Own_house = false AND Has_job = true
    â†’ Class = Yes    [support=5/15=33%, confidence=5/5=100%]

Own_house = false AND Has_job = false
    â†’ Class = No     [support=4/15=27%, confidence=4/4=100%]
```

### Rule Metrics

| Metric | Formula | Meaning |
|---|---|---|
| **Support** | (# examples covered by rule) / (total examples) | How often the rule fires |
| **Confidence** | (# correctly classified) / (# covered) | How accurate the rule is |
| **Coverage** | Same as support (alternate name) | â€” |
| **Lift** | Confidence / Pr(class) | Improvement over random guessing |

---

## 9. Handling Continuous Attributes

### The Challenge

Continuous features (like Age=22, 35, 47â€¦) can't be used directly for categorical splits.

### Solution: Binary Threshold Split

```
Approach:
1. Sort all values of continuous attribute: vâ‚ < vâ‚‚ < ... < váµ£
2. Candidate thresholds = midpoints between adjacent values
   â†’ tâ‚ = (vâ‚+vâ‚‚)/2, tâ‚‚ = (vâ‚‚+vâ‚ƒ)/2, ...
3. For each threshold táµ¢, create 2 branches:
   LEFT: examples where attribute â‰¤ táµ¢
   RIGHT: examples where attribute > táµ¢
4. Compute gain for each threshold
5. Pick threshold with MAXIMUM gain
```

### Example

```
Age values: [22, 25, 28, 35, 40]
Classes:    [No, No, Yes, Yes, Yes]

Candidate thresholds: 23.5, 26.5, 31.5, 37.5

Threshold 26.5:
  Left  (â‰¤26.5): 22, 25  â†’ [No, No]  â†’ entropy = 0
  Right (>26.5): 28,35,40 â†’ [Yes,Yes,Yes] â†’ entropy = 0
  â†’ gain = max possible!  âœ… Best threshold = 26.5
```

### Visual (from slides)

```
Y-axis
  2.6 â”‚ â–  â–  â—‹ â—‹ â—‹              Corresponding Decision Tree:
  2.5 â”‚ â–  â—‹ â—‹ â—‹ â—‹                        [X â‰¤ 2?]
  2.0 â”‚ â–  â–  â—‹â—‹â–  â–                         /       \
      â”‚ â–  â–  â–  â– â—‹ â–               [Y â‰¤ 2.5]        [Y â‰¤ 2]
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ X-axis
         2   3   4
         
â–  = Class 1,  â—‹ = Class 2
Horizontal/vertical splits partition the space
```

> âš ï¸ Note: The SAME continuous attribute can be reused at different nodes with different thresholds!

---

## 10. Overfitting & Pruning

### What is Overfitting?

```
Overfitting = Tree memorizes training data
              â†’ Perfect training accuracy
              â†’ Poor test/real-world accuracy

Signs:
  â€¢ Tree is very deep (many levels)
  â€¢ Many branches with very few examples
  â€¢ Leaf nodes with 1-2 examples
  â€¢ Training accuracy >> Test accuracy
```

```
Training Data          Overfitted Tree        Generalized Tree
    â— â—‹ â—                   â”Œâ”€â”                    â”Œâ”€â”€â”€â”€â”€â”
  â—‹ â— â—‹ â—‹            â”Œâ”€â”   â”‚ â”‚   â”Œâ”€â”              â”‚     â”‚
    â— â—‹ â—            â”‚ â”‚  ...â”‚...  â”‚ â”‚         â”Œâ”€â”€â”€â”˜     â””â”€â”€â”€â”
                  (very deep, fits noise)    (simpler, robust)
```

### Two Types of Pruning

#### Pre-Pruning (Early Stopping)

Stop growing the tree **before** it becomes too complex.

```
Conditions to stop:
  â€¢ Gain < threshold (e.g., gain < 0.01)
  â€¢ Node has < min_samples (e.g., < 5 examples)
  â€¢ Tree depth > max_depth
  â€¢ Statistical test shows improvement is not significant

PRO: Fast
CON: May stop too early (underfitting risk)
     Hard to know what you might miss
```

#### Post-Pruning (Preferred)

Grow a full tree FIRST, then trim it back.

```
Process:
  1. Grow full tree (no stopping conditions)
  2. Evaluate each subtree
  3. Compare: leaf_node vs full_subtree
     â€¢ Estimate error of subtree on validation data
     â€¢ If leaf â‰¥ subtree in accuracy â†’ replace subtree with leaf
  4. Repeat bottom-up until no improvement

Methods:
  â€¢ C4.5: Pessimistic error estimation
  â€¢ Reduced Error Pruning: Use held-out validation set
  â€¢ Cost-Complexity Pruning (sklearn): Î± parameter

PRO: More reliable, empirically better
CON: Slightly slower (build full tree first)
```

### Visual: Before and After Pruning

```
BEFORE PRUNING (Overfitted):        AFTER PRUNING (Cleaner):

         [X]                                [X]
        /   \                              /   \
      â‰¤2     >2                          â‰¤2     >2
      /       \                          â–        [Y]
   [Y]         [Y]                              /   \
   / \         / \                           â‰¤2      >2
 â‰¤2.5 >2.5   â‰¤2  >2                         [X]      â—‹
 /    [Y]    [X]  â—‹                         / \
â–      /\    / \                           â‰¤3   >3
   â‰¤2.6 >2.6 â‰¤3 >3                        â–    [X]
   â—‹    â–     â–   [X]                           / \
                /\                           â‰¤4  >4
              â‰¤4  >4                         â—‹   â– 
              â—‹    â– 
```

---

## 11. Algorithm Variants

| Algorithm | Year | Split Criterion | Splits | Handles Continuous | Missing Values |
|---|---|---|---|---|---|
| **ID3** | 1986 | Information Gain | Multi-way | âŒ No | âŒ No |
| **C4.5** | 1993 | Gain Ratio | Multi-way | âœ… Yes | âœ… Yes |
| **CART** | 1984 | Gini Impurity | Binary only | âœ… Yes | âœ… Yes |
| **CHAID** | 1980 | Chi-Square | Multi-way | âœ… Yes | âœ… Yes |
| **C5.0** | 2000s | Gain Ratio | Multi-way | âœ… Yes | âœ… Yes |

### ID3 vs C4.5 vs CART

```
ID3:                    C4.5:                   CART:
â€¢ Simple, classic       â€¢ Most popular          â€¢ sklearn default
â€¢ Gain only             â€¢ Gain Ratio            â€¢ Gini Impurity
â€¢ Categorical only      â€¢ Continuous ok         â€¢ Binary splits only
â€¢ Prone to bias         â€¢ Handles missing       â€¢ Regression too
                        â€¢ Post-pruning          â€¢ Produces 2-way splits
```

### Decision Trees in Ensembles

```
Single Tree          Random Forest           Gradient Boosting
    [T]              [Tâ‚][Tâ‚‚]...[Tâ‚™]         Tâ‚â†’Tâ‚‚â†’Tâ‚ƒâ†’...â†’Tâ‚™
     â”‚               Each trained on          Each tree corrects
     â†“               random data subset       errors of previous
  Prediction         + random features
                     Average predictions
                     
â€¢ High variance     â€¢ Low variance           â€¢ Low bias
â€¢ Interpretable     â€¢ Not interpretable      â€¢ Not interpretable
â€¢ Fast              â€¢ Slower                 â€¢ Slowest
```

---

## 12. Visualizations & Diagrams

### Entropy as a Function of Class Balance

```
      Entropy
  1.0 â”¤         â—         â† Most impure (50/50)
      â”‚      â—     â—
  0.8 â”¤    â—         â—
      â”‚  â—             â—
  0.6 â”¤â—                 â—
      â”‚                    â—
  0.4 â”¤                      â—
      â”‚                        â—
  0.2 â”¤                          â—
      â”‚                             â—
  0.0 â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—  â† Pure (0/100 or 100/0)
      0   0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1.0
                              Proportion of Positive Class
```

### Information Gain = Area Saved

```
Before Split:          After Split on Attr A:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â—‹ â— â— â—‹ â— â—‹ â”‚  â†’   â”‚ â— â— â”‚  â”‚ â—‹ â—‹ â—‹ â—‹ â”‚
â”‚ â—‹ â—‹ â— â— â—‹ â— â”‚  â†’   â”‚ â— â— â”‚  â”‚ â—‹ â—‹ â—‹   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
entropy = 0.971         entropy=0  entropy=0
                       
Gain = 0.971 - (6/15Ã—0 + 9/15Ã—0) = 0.971 (perfect split!)
```

### Decision Boundary in 2D Space

```
Y
2.6 â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â—‹ â—‹ â—‹
2.5 â”‚    â–      â”‚       â—‹    â† Decision tree creates
    â”‚          â”‚              axis-aligned rectangular
2.0 â”‚    â–      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    boundaries in the feature
    â”‚    â–      â”‚ â—‹ â—‹  â–        space
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    0    2     3    4    X

Each vertical/horizontal line = one decision node split
```

### Overfitting Visualization

```
Model Complexity (Tree Depth)
         â†‘ Accuracy
    100% â”‚       â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â—  â† Training accuracy (keeps rising)
         â”‚     â—         â—
     80% â”‚   â—         â—
         â”‚  â—         â—
     60% â”‚ â—       â—         â† Test accuracy (peaks then drops)
         â”‚         
     40% â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’
         0    2    4    6    8   Max Depth

        Underfitting â†” Sweet Spot â†” Overfitting
```

---

## 13. Mnemonics & Cheat Sheet

### ğŸ§  Remember: "SELF" to Build a Tree

```
S â†’ Select best attribute (max information gain)
E â†’ Expand the node (make it a decision node)
L â†’ Leaf if stopping condition is met
F â†’ Further recurse on each subset
```

### ğŸ§  Remember: Entropy Extremes

```
"No Mix = Zero, Equal Mix = One"
Pure dataset   â†’ entropy = 0  (you know everything, no new info needed)
50/50 split    â†’ entropy = 1  (you know nothing, maximum uncertainty)
```

### ğŸ§  Remember: Gain Formula in Words

```
GAIN = (Disorder BEFORE split) âˆ’ (Disorder AFTER split)
     = What we started with   âˆ’  What we're left with
     = How much cleaner it got
```

### ğŸ§  Remember: Which Algorithm Uses What

```
"ID3 Gains, C4.5 Ratios, CART Grins (Gini)"
```

### ğŸ“‹ Quick Reference Card

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   DECISION TREE QUICK REFERENCE                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Formula              â”‚ Description                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ entropy(D)           â”‚ -Î£ Pr(câ±¼) Ã— logâ‚‚ Pr(câ±¼)               â”‚
â”‚ entropy_A(D)         â”‚ Î£ (|Dâ±¼|/|D|) Ã— entropy(Dâ±¼)            â”‚
â”‚ gain(D, A)           â”‚ entropy(D) - entropy_A(D)               â”‚
â”‚ gainRatio(D, A)      â”‚ gain(D, A) / SplitInfo(A)               â”‚
â”‚ SplitInfo(A)         â”‚ -Î£ (|Dâ±¼|/|D|) Ã— logâ‚‚(|Dâ±¼|/|D|)       â”‚
â”‚ Gini(D)              â”‚ 1 - Î£ Pr(câ±¼)Â²                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Entropy range        â”‚ [0, logâ‚‚(|C|)]  â†’ [0, 1] for binary    â”‚
â”‚ Gain range           â”‚ [0, entropy(D)]                          â”‚
â”‚ GainRatio range      â”‚ [0, 1]                                   â”‚
â”‚ Gini range           â”‚ [0, 0.5] for binary                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Best attribute       â”‚ MAX gain (or max gain ratio)             â”‚
â”‚ Stop condition       â”‚ Pure node OR no attributes OR no data    â”‚
â”‚ Overfitting fix      â”‚ Pruning (post > pre)                     â”‚
â”‚ Continuous feature   â”‚ Try all midpoint thresholds              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 14. Practice Problems

### Problem 1 â€” Basic Entropy (Easy)

A dataset has 10 examples: 7 positive, 3 negative.  
**Calculate entropy(D).**

<details>
<summary>â–¶ Full Step-by-Step Solution</summary>

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 1 â€” Count the classes                                     â”‚
â”‚    Positive = 7,  Negative = 3,  Total = 10                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 2 â€” Compute class probabilities                           â”‚
â”‚    Pr(positive) = 7/10 = 0.700                                  â”‚
â”‚    Pr(negative) = 3/10 = 0.300                                  â”‚
â”‚    Check: 0.700 + 0.300 = 1.0  âœ…                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 3 â€” Compute logâ‚‚ of each probability                     â”‚
â”‚    logâ‚‚(0.700) = logâ‚â‚€(0.700)/logâ‚â‚€(2)                         â”‚
â”‚                = (âˆ’0.1549) / (0.3010)                           â”‚
â”‚                = âˆ’0.5146                                        â”‚
â”‚                                                                 â”‚
â”‚    logâ‚‚(0.300) = logâ‚â‚€(0.300)/logâ‚â‚€(2)                         â”‚
â”‚                = (âˆ’0.5229) / (0.3010)                           â”‚
â”‚                = âˆ’1.7370                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 4 â€” Multiply probability Ã— log                           â”‚
â”‚    positive term:  0.700 Ã— (âˆ’0.5146) = âˆ’0.3602                 â”‚
â”‚    negative term:  0.300 Ã— (âˆ’1.7370) = âˆ’0.5211                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 5 â€” Apply the negative sign from the formula             â”‚
â”‚    âˆ’ (âˆ’0.3602) = +0.3602                                        â”‚
â”‚    âˆ’ (âˆ’0.5211) = +0.5211                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 6 â€” Sum the terms                                         â”‚
â”‚    entropy(D) = 0.3602 + 0.5211 = 0.8813 â‰ˆ 0.881  âœ…           â”‚
â”‚                                                                 â”‚
â”‚  Interpretation: 0.881 is fairly close to 1.0 (max disorder)   â”‚
â”‚  Dataset is unbalanced but not pure.                            â”‚
â”‚  If it were 50/50, entropy would be 1.0                        â”‚
â”‚  If it were 100/0, entropy would be 0.0                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
</details>

---

### Problem 2 â€” Which Split is Better? (Medium)

| Split | Left Branch | Right Branch |
|---|---|---|
| **A** | 4 Yes, 0 No | 1 Yes, 5 No |
| **B** | 3 Yes, 2 No | 2 Yes, 3 No |

Original dataset: 5 Yes, 5 No. Total = 10 examples.  
**Which split gives higher information gain?**

<details>
<summary>â–¶ Full Step-by-Step Solution</summary>

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 1 â€” Compute root entropy (before any split)              â”‚
â”‚    Pr(Yes) = 5/10 = 0.500,  Pr(No) = 5/10 = 0.500              â”‚
â”‚    logâ‚‚(0.500) = logâ‚â‚€(0.5)/logâ‚â‚€(2) = âˆ’0.3010/0.3010 = âˆ’1.0  â”‚
â”‚                                                                 â”‚
â”‚    entropy(D) = âˆ’(0.5 Ã— âˆ’1.0) âˆ’ (0.5 Ã— âˆ’1.0)                  â”‚
â”‚              = 0.500 + 0.500 = 1.000  (maximum disorder)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
  EVALUATING SPLIT A
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 2A â€” Left branch: 4 Yes, 0 No, Total=4                   â”‚
â”‚    Pr(Yes) = 4/4 = 1.0,  Pr(No) = 0/4 = 0.0                   â”‚
â”‚    entropy(Left_A) = âˆ’(1.0 Ã— logâ‚‚1.0) âˆ’ 0                     â”‚
â”‚                    = âˆ’(1.0 Ã— 0) = 0.000  â† PURE âœ…             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 3A â€” Right branch: 1 Yes, 5 No, Total=6                  â”‚
â”‚    Pr(Yes) = 1/6 = 0.1667,  Pr(No) = 5/6 = 0.8333              â”‚
â”‚                                                                 â”‚
â”‚    logâ‚‚(0.1667) = logâ‚â‚€(0.1667)/0.3010 = âˆ’0.7782/0.3010        â”‚
â”‚                 = âˆ’2.5850                                       â”‚
â”‚    logâ‚‚(0.8333) = logâ‚â‚€(0.8333)/0.3010 = âˆ’0.0792/0.3010        â”‚
â”‚                 = âˆ’0.2630                                       â”‚
â”‚                                                                 â”‚
â”‚    entropy(Right_A) = âˆ’(0.1667 Ã— âˆ’2.5850) âˆ’ (0.8333 Ã— âˆ’0.2630) â”‚
â”‚                     = 0.4309 + 0.2192                           â”‚
â”‚                     = 0.6500                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 4A â€” Weighted entropy for Split A                        â”‚
â”‚    Weight of Left  = 4/10 = 0.400                               â”‚
â”‚    Weight of Right = 6/10 = 0.600                               â”‚
â”‚                                                                 â”‚
â”‚    entropy_A = (0.400 Ã— 0.000) + (0.600 Ã— 0.650)               â”‚
â”‚              = 0.000 + 0.390                                    â”‚
â”‚              = 0.390                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 5A â€” Gain for Split A                                    â”‚
â”‚    gain_A = entropy(D) âˆ’ entropy_A                              â”‚
â”‚           = 1.000 âˆ’ 0.390 = 0.610  âœ…                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
  EVALUATING SPLIT B
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 2B â€” Left branch: 3 Yes, 2 No, Total=5                   â”‚
â”‚    Pr(Yes) = 3/5 = 0.600,  Pr(No) = 2/5 = 0.400                â”‚
â”‚    logâ‚‚(0.600) = âˆ’0.7370,  logâ‚‚(0.400) = âˆ’1.3219               â”‚
â”‚                                                                 â”‚
â”‚    entropy(Left_B) = âˆ’(0.600 Ã— âˆ’0.7370) âˆ’ (0.400 Ã— âˆ’1.3219)   â”‚
â”‚                    = 0.4422 + 0.5288 = 0.9710                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 3B â€” Right branch: 2 Yes, 3 No, Total=5                  â”‚
â”‚    Pr(Yes) = 2/5 = 0.400,  Pr(No) = 3/5 = 0.600                â”‚
â”‚    Same as Left_B with classes swapped â†’ same entropy           â”‚
â”‚    entropy(Right_B) = 0.971                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 4B â€” Weighted entropy for Split B                        â”‚
â”‚    Weight of Left  = 5/10 = 0.500                               â”‚
â”‚    Weight of Right = 5/10 = 0.500                               â”‚
â”‚                                                                 â”‚
â”‚    entropy_B = (0.500 Ã— 0.971) + (0.500 Ã— 0.971)               â”‚
â”‚              = 0.4855 + 0.4855 = 0.971                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 5B â€” Gain for Split B                                    â”‚
â”‚    gain_B = entropy(D) âˆ’ entropy_B                              â”‚
â”‚           = 1.000 âˆ’ 0.971 = 0.029  (very small!)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
  FINAL COMPARISON
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Split        â”‚ Weighted    â”‚ Gain             â”‚ Winner?  â”‚
â”‚              â”‚ Entropy     â”‚                  â”‚          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ A            â”‚ 0.390       â”‚ 1.000âˆ’0.390=0.610â”‚  âœ… YES  â”‚
â”‚ B            â”‚ 0.971       â”‚ 1.000âˆ’0.971=0.029â”‚  âŒ NO   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

WINNER: Split A (gain=0.610 >> 0.029)
WHY? Split A creates a perfectly PURE left branch (entropy=0)
     Split B leaves both branches almost as mixed as before!
```
</details>

---

### Problem 3 â€” Build a Mini Tree (Medium-Hard)

**Dataset (Weather â†’ Play Tennis):**

| Weather | Wind | Humidity | Play |
|---|---|---|---|
| Sunny | Weak | High | No |
| Sunny | Strong | High | No |
| Overcast | Weak | High | Yes |
| Rainy | Weak | High | Yes |
| Rainy | Weak | Normal | Yes |
| Rainy | Strong | Normal | No |
| Overcast | Strong | Normal | Yes |
| Sunny | Weak | Normal | Yes |

Total: 5 Yes, 3 No

**Task:** Compute entropy and information gain for each attribute. Which is the root?

<details>
<summary>â–¶ Full Step-by-Step Solution</summary>

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 1 â€” Root entropy                                          â”‚
â”‚    Yes=5, No=3, Total=8                                         â”‚
â”‚    Pr(Yes) = 5/8 = 0.625,  Pr(No) = 3/8 = 0.375                â”‚
â”‚                                                                 â”‚
â”‚    logâ‚‚(0.625) = logâ‚â‚€(0.625)/0.3010 = âˆ’0.2041/0.3010 = âˆ’0.678 â”‚
â”‚    logâ‚‚(0.375) = logâ‚â‚€(0.375)/0.3010 = âˆ’0.4260/0.3010 = âˆ’1.415 â”‚
â”‚                                                                 â”‚
â”‚    entropy(D) = âˆ’(0.625Ã—âˆ’0.678) âˆ’ (0.375Ã—âˆ’1.415)               â”‚
â”‚              = 0.424 + 0.531 = 0.954                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
  ATTRIBUTE: Weather (3 values: Sunny / Overcast / Rainy)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

  Group Sunny    (rows 1,2,8): Yes=1, No=2, Total=3
  Group Overcast (rows 3,7):   Yes=2, No=0, Total=2
  Group Rainy    (rows 4,5,6): Yes=2, No=1, Total=3

â”Œâ”€â”€â”€ entropy(Sunny) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Pr(Yes)=1/3=0.333, Pr(No)=2/3=0.667                           â”‚
â”‚  logâ‚‚(0.333)=âˆ’1.585,  logâ‚‚(0.667)=âˆ’0.585                       â”‚
â”‚  entropy = âˆ’(0.333Ã—âˆ’1.585)âˆ’(0.667Ã—âˆ’0.585) = 0.528+0.390 = 0.918â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€ entropy(Overcast) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Pr(Yes)=2/2=1.0, Pr(No)=0/2=0.0                               â”‚
â”‚  entropy = 0.000  â† PURE âœ…                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€ entropy(Rainy) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Pr(Yes)=2/3=0.667, Pr(No)=1/3=0.333                           â”‚
â”‚  Same as Sunny (just swapped) â†’ entropy = 0.918                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  entropy_Weather = (3/8)Ã—0.918 + (2/8)Ã—0.000 + (3/8)Ã—0.918
                  = 0.375Ã—0.918 + 0 + 0.375Ã—0.918
                  = 0.344 + 0 + 0.344 = 0.688

  gain(Weather) = 0.954 âˆ’ 0.688 = 0.266  âœ…

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
  ATTRIBUTE: Wind (2 values: Weak / Strong)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

  Group Weak   (rows 1,3,4,5,8): Yes=4, No=1, Total=5
  Group Strong (rows 2,6,7):     Yes=1, No=2, Total=3

â”Œâ”€â”€â”€ entropy(Weak) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Pr(Yes)=4/5=0.800, Pr(No)=1/5=0.200                           â”‚
â”‚  logâ‚‚(0.800)=âˆ’0.322,  logâ‚‚(0.200)=âˆ’2.322                       â”‚
â”‚  entropy = âˆ’(0.800Ã—âˆ’0.322)âˆ’(0.200Ã—âˆ’2.322) = 0.258+0.464 = 0.722â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€ entropy(Strong) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Pr(Yes)=1/3=0.333, Pr(No)=2/3=0.667                           â”‚
â”‚  entropy = 0.918  (same calculation as Sunny above)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  entropy_Wind = (5/8)Ã—0.722 + (3/8)Ã—0.918
               = 0.625Ã—0.722 + 0.375Ã—0.918
               = 0.451 + 0.344 = 0.795

  gain(Wind) = 0.954 âˆ’ 0.795 = 0.159

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
  ATTRIBUTE: Humidity (2 values: High / Normal)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

  Group High   (rows 1,2,3,4): Yes=2, No=2, Total=4
  Group Normal (rows 5,6,7,8): Yes=3, No=1, Total=4

â”Œâ”€â”€â”€ entropy(High) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Pr(Yes)=2/4=0.500, Pr(No)=2/4=0.500                           â”‚
â”‚  entropy = 1.000  â† maximum disorder!                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€ entropy(Normal) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Pr(Yes)=3/4=0.750, Pr(No)=1/4=0.250                           â”‚
â”‚  logâ‚‚(0.750) = logâ‚â‚€(0.750)/0.3010 = âˆ’0.1249/0.3010 = âˆ’0.4150  â”‚
â”‚  logâ‚‚(0.250) = logâ‚â‚€(0.250)/0.3010 = âˆ’0.6021/0.3010 = âˆ’2.0000  â”‚
â”‚  entropy = âˆ’(0.750Ã—âˆ’0.415)âˆ’(0.250Ã—âˆ’2.000) = 0.311+0.500 = 0.811â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  entropy_Humidity = (4/8)Ã—1.000 + (4/8)Ã—0.811
                   = 0.500Ã—1.000 + 0.500Ã—0.811
                   = 0.500 + 0.406 = 0.906

  gain(Humidity) = 0.954 âˆ’ 0.906 = 0.048

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
  FINAL COMPARISON
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Attribute    â”‚ Weighted        â”‚ Gain     â”‚ Winner?  â”‚
â”‚              â”‚ Entropy         â”‚          â”‚          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Weather      â”‚ 0.688           â”‚ 0.266    â”‚  âœ… YES  â”‚
â”‚ Wind         â”‚ 0.795           â”‚ 0.159    â”‚  âŒ NO   â”‚
â”‚ Humidity     â”‚ 0.906           â”‚ 0.048    â”‚  âŒ NO   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

WINNER: Weather (gain=0.266) â†’ becomes ROOT NODE
Note: Overcast branch is already PURE â†’ becomes a leaf (Yes) immediately!
```
</details>

---

### Problem 4 â€” Continuous Threshold (Medium)

Feature "Temperature" with values and classes:

| Temp | 15 | 18 | 22 | 25 | 30 | 35 |
|---|---|---|---|---|---|---|
| Class | No | No | No | Yes | Yes | Yes |

**Find the best binary split threshold.**

<details>
<summary>â–¶ Full Step-by-Step Solution</summary>

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 1 â€” Root entropy                                          â”‚
â”‚    Yes=3, No=3, Total=6                                         â”‚
â”‚    Pr(Yes) = Pr(No) = 0.500                                     â”‚
â”‚    entropy(D) = âˆ’(0.5Ã—âˆ’1.0) âˆ’ (0.5Ã—âˆ’1.0) = 1.000              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 2 â€” Identify candidate thresholds                        â”‚
â”‚    Sort values: 15, 18, 22, 25, 30, 35                         â”‚
â”‚    Midpoints between adjacent values:                           â”‚
â”‚      (15+18)/2 = 16.5                                           â”‚
â”‚      (18+22)/2 = 20.0                                           â”‚
â”‚      (22+25)/2 = 23.5   â† class boundary here (Noâ†’Yes)        â”‚
â”‚      (25+30)/2 = 27.5                                           â”‚
â”‚      (30+35)/2 = 32.5                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 3 â€” Evaluate each threshold                              â”‚
â”‚                                                                 â”‚
â”‚  Threshold 16.5: (â‰¤16.5 vs >16.5)                              â”‚
â”‚    Left : [15]       â†’ 0 Yes, 1 No  â†’ entropy=0.000  size=1    â”‚
â”‚    Right: [18,22,25,30,35] â†’ 3 Yes, 2 No â†’ entropy=0.971 size=5â”‚
â”‚    weighted = (1/6)Ã—0 + (5/6)Ã—0.971 = 0.809                    â”‚
â”‚    gain = 1.000 âˆ’ 0.809 = 0.191                                 â”‚
â”‚                                                                 â”‚
â”‚  Threshold 20.0: (â‰¤20 vs >20)                                   â”‚
â”‚    Left : [15,18]   â†’ 0 Yes, 2 No  â†’ entropy=0.000  size=2     â”‚
â”‚    Right: [22,25,30,35] â†’ 3 Yes, 1 No â†’ entropy=0.811 size=4   â”‚
â”‚    weighted = (2/6)Ã—0 + (4/6)Ã—0.811 = 0.541                    â”‚
â”‚    gain = 1.000 âˆ’ 0.541 = 0.459                                 â”‚
â”‚                                                                 â”‚
â”‚  Threshold 23.5: (â‰¤23.5 vs >23.5)  â† CLASS BOUNDARY           â”‚
â”‚    Left : [15,18,22] â†’ 0 Yes, 3 No â†’ entropy=0.000  size=3     â”‚
â”‚    Right: [25,30,35] â†’ 3 Yes, 0 No â†’ entropy=0.000  size=3     â”‚
â”‚    weighted = (3/6)Ã—0 + (3/6)Ã—0 = 0.000                        â”‚
â”‚    gain = 1.000 âˆ’ 0.000 = 1.000  â† PERFECT SPLIT! âœ…           â”‚
â”‚                                                                 â”‚
â”‚  (No need to check 27.5 or 32.5 since 1.0 is max possible)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  RESULT SUMMARY                                                 â”‚
â”‚                                                                 â”‚
â”‚  Threshold â”‚  Left (â‰¤t)  â”‚  Right (>t) â”‚  Gain                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€                 â”‚
â”‚   16.5     â”‚ 0Y, 1N, E=0 â”‚ 3Y, 2N, 0.971â”‚  0.191              â”‚
â”‚   20.0     â”‚ 0Y, 2N, E=0 â”‚ 3Y, 1N, 0.811â”‚  0.459              â”‚
â”‚   23.5     â”‚ 0Y, 3N, E=0 â”‚ 3Y, 0N, E=0 â”‚  1.000  âœ… BEST      â”‚
â”‚                                                                 â”‚
â”‚  Best threshold = 23.5                                          â”‚
â”‚  Split rule: Temperature â‰¤ 23.5 â†’ No,  Temperature > 23.5 â†’ Yesâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
</details>

---

### Problem 5 â€” Gain Ratio (Hard)

Using the same dataset as the worked example (15 loans):  
**Compute Gain Ratio for Own\_House and Age.**

<details>
<summary>â–¶ Full Step-by-Step Solution</summary>

```
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
  GAIN RATIO for Own House
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Known from earlier: gain(D, Own House) = 0.420                 â”‚
â”‚  Split: 6 examples in true-branch, 9 in false-branch           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 1 â€” Compute split proportions                            â”‚
â”‚    |D_true|/|D|  = 6/15 = 0.400                                 â”‚
â”‚    |D_false|/|D| = 9/15 = 0.600                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 2 â€” Compute logâ‚‚ of split proportions                    â”‚
â”‚    logâ‚‚(0.400) = logâ‚â‚€(0.400)/0.3010 = âˆ’0.3979/0.3010          â”‚
â”‚                = âˆ’1.3219                                        â”‚
â”‚                                                                 â”‚
â”‚    logâ‚‚(0.600) = logâ‚â‚€(0.600)/0.3010 = âˆ’0.2218/0.3010          â”‚
â”‚                = âˆ’0.7370                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 3 â€” Compute SplitInfo (entropy of the split)             â”‚
â”‚    SplitInfo(Own House) = âˆ’ (0.400 Ã— âˆ’1.3219)                   â”‚
â”‚                         âˆ’ (0.600 Ã— âˆ’0.7370)                    â”‚
â”‚                         = 0.5288 + 0.4422                       â”‚
â”‚                         = 0.9710 â‰ˆ 0.971                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 4 â€” Compute Gain Ratio                                   â”‚
â”‚    GainRatio(Own House) = gain / SplitInfo                      â”‚
â”‚                         = 0.420 / 0.971                         â”‚
â”‚                         = 0.432  âœ…                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
  GAIN RATIO for Age
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Known: gain(D, Age) = 0.083                                    â”‚
â”‚  Split: 5 young + 5 middle + 5 old (perfectly equal thirds)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 1 â€” Split proportions                                    â”‚
â”‚    |D_young|/|D|  = 5/15 = 0.3333                               â”‚
â”‚    |D_middle|/|D| = 5/15 = 0.3333                               â”‚
â”‚    |D_old|/|D|    = 5/15 = 0.3333                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 2 â€” Log of split proportion (same for all 3)             â”‚
â”‚    logâ‚‚(0.3333) = logâ‚â‚€(0.3333)/0.3010 = âˆ’0.4771/0.3010        â”‚
â”‚                 = âˆ’1.5850                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 3 â€” SplitInfo for Age                                    â”‚
â”‚    SplitInfo(Age) = âˆ’ (0.3333 Ã— âˆ’1.5850)                        â”‚
â”‚                   âˆ’ (0.3333 Ã— âˆ’1.5850)                          â”‚
â”‚                   âˆ’ (0.3333 Ã— âˆ’1.5850)                          â”‚
â”‚                                                                 â”‚
â”‚                 = 3 Ã— (0.3333 Ã— 1.5850)                         â”‚
â”‚                 = 3 Ã— 0.5283                                    â”‚
â”‚                 = 1.585                                          â”‚
â”‚                                                                 â”‚
â”‚  Note: logâ‚‚(3) = 1.585. This is not a coincidence!             â”‚
â”‚  Equal splits always give SplitInfo = logâ‚‚(number of branches) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 4 â€” Gain Ratio for Age                                   â”‚
â”‚    GainRatio(Age) = gain / SplitInfo                            â”‚
â”‚                   = 0.083 / 1.585                               â”‚
â”‚                   = 0.052                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
  CONCLUSION
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Attribute    â”‚ Gain       â”‚ SplitInfo    â”‚ Gain Ratio   â”‚ Winner?    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Own House    â”‚ 0.420      â”‚ 0.971        â”‚ 0.432        â”‚  âœ… YES    â”‚
â”‚ Age          â”‚ 0.083      â”‚ 1.585        â”‚ 0.052        â”‚  âŒ NO     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Own House still wins, but notice:
â€¢ Age was penalised because it splits into 3 branches (SplitInfo=1.585)
â€¢ Own House was barely penalised (SplitInfo=0.971, only 2 branches)
â€¢ Gain Ratio correctly identifies Own House as the better split
```
</details>

---

## 15. Best Practices

### 1. Choose the Right Split Criterion

```
Use Gain Ratio (C4.5) when:
  â†’ You have attributes with many distinct values
  â†’ You want to avoid bias toward high-cardinality features

Use Gini (CART) when:
  â†’ You need speed (no log computation)
  â†’ You want compatibility with sklearn

Use Information Gain (ID3) when:
  â†’ Learning the concept (simpler math)
  â†’ All attributes have similar cardinality
```

### 2. Control Tree Complexity

```python
# sklearn example â€” hyperparameters to tune
DecisionTreeClassifier(
    max_depth=5,           # Limit tree depth
    min_samples_split=10,  # Min samples to split a node
    min_samples_leaf=5,    # Min samples in each leaf
    max_features='sqrt',   # Use random subset of features
    ccp_alpha=0.01         # Cost-complexity pruning strength
)
```

### 3. Validation Strategy

```
ALWAYS evaluate on held-out test data:

Option A: Train/Test Split (simple)
  70% training â†’ build tree
  30% testing  â†’ evaluate

Option B: K-Fold Cross Validation (robust)
  Split data into k folds
  Train on k-1, test on 1
  Repeat k times, average the scores

Option C: Separate Pruning Set (for post-pruning)
  60% training â†’ build full tree
  20% validation â†’ pruning decisions
  20% testing  â†’ final evaluation
```

### 4. Feature Engineering

```
Decision trees benefit from:
  âœ… Creating interaction features (A AND B)
  âœ… Binning continuous features carefully
  âœ… Encoding ordinal categories as ordered numbers
  âœ… Removing irrelevant/redundant features (reduces tree size)

Decision trees DON'T need:
  âŒ Feature scaling (not distance-based)
  âŒ Normalization
  âŒ Handling of correlated features (tree handles it)
```

### 5. Handling Class Imbalance

```
Problem: 95% Class A, 5% Class B
         Tree predicts A for everything â†’ 95% accuracy but useless!

Solutions:
  â€¢ class_weight='balanced' in sklearn
  â€¢ Oversample minority class (SMOTE)
  â€¢ Undersample majority class
  â€¢ Use precision/recall/F1 instead of accuracy
```

---

## 16. Common Pitfalls

### âŒ Pitfall 1: Overfitting

```
Problem: Tree grows too deep, fits noise
         Training: 99% accuracy
         Test: 62% accuracy

Fix:
  â€¢ Post-pruning (preferred)
  â€¢ Set max_depth=5 or 10
  â€¢ Require min_samples_leaf â‰¥ 5
```

### âŒ Pitfall 2: Information Gain Bias

```
Problem: Attribute "Customer_ID" has 1000 unique values
         ID3 always picks it (perfect split, gain=max!)
         But it's USELESS for prediction

Fix:
  â€¢ Use Gain Ratio (C4.5) instead of raw Gain
  â€¢ Remove ID/timestamp columns before training
```

### âŒ Pitfall 3: Instability

```
Problem: Remove one training example â†’ completely different tree
         Decision trees have HIGH variance

Fix:
  â€¢ Use ensemble methods: Random Forest, Gradient Boosting
  â€¢ Average predictions across many trees = stability
```

### âŒ Pitfall 4: Axis-Aligned Boundaries Only

```
Problem: Trees can only make horizontal/vertical splits
         Diagonal or curved boundaries require many nodes

Example: Data separated by diagonal line y = x
         Tree needs many steps to approximate it:
         
         Real boundary: /    Tree boundary: â”
                                           â”‚ â”
                                             â”‚ â”

Fix:
  â€¢ Create diagonal features (e.g., feature1 - feature2)
  â€¢ Use SVM or neural nets for diagonal boundaries
```

### âŒ Pitfall 5: Missing Values

```
Problem: ID3 can't handle missing values at all

Fixes:
  â€¢ C4.5: Distributes example with weight across all branches
  â€¢ CART: Surrogate splits (use second-best attribute as backup)
  â€¢ Imputation: Fill missing values before training
  â€¢ sklearn: Does NOT support missing values natively â†’ impute first
```

### âŒ Pitfall 6: Assuming the Tree is Optimal

```
REMEMBER: Finding the globally optimal tree is NP-HARD

All algorithms are greedy heuristics:
  â€¢ Make locally optimal choice at each step
  â€¢ May miss globally better tree structure
  â€¢ The tree YOU get â‰  the BEST possible tree

This is WHY ensembles (Random Forests) outperform single trees
```

---

## ğŸ“š Summary Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DECISION TREE LEARNING â€” OVERVIEW                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                       â”‚
â”‚  DATA â†’ [Compute Entropy] â†’ [Compute Gain per Attribute]             â”‚
â”‚                â†“                         â†“                            â”‚
â”‚           entropy(D)            gain(D, Aáµ¢) = entropy(D)             â”‚
â”‚       = -Î£ Pr(c)logâ‚‚Pr(c)       - Î£(|Dâ±¼|/|D|)Ã—entropy(Dâ±¼)         â”‚
â”‚                                                                       â”‚
â”‚  [Pick MAX gain attribute] â†’ [Split data] â†’ [Recurse on subsets]     â”‚
â”‚                                                                       â”‚
â”‚  STOPPING: Pure node | No attributes | Gain < threshold               â”‚
â”‚                                                                       â”‚
â”‚  OVERFITTING: Prune! (Post-pruning preferred)                         â”‚
â”‚                                                                       â”‚
â”‚  CONTINUOUS: Try all midpoint thresholds, pick max gain               â”‚
â”‚                                                                       â”‚
â”‚  RULES: Each rootâ†’leaf path = one IF-THEN rule                        â”‚
â”‚                                                                       â”‚
â”‚  VARIANTS: ID3 (Gain) | C4.5 (GainRatio) | CART (Gini)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---
