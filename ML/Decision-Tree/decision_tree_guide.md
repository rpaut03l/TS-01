# ğŸŒ³ Decision Trees â€” Complete Study Guide

> **Reference:** CS583, Bing Liu, UIC | Subhash Bhagat Sir ML Slides  
> **Topic:** Decision Tree Learning using Entropy & Information Gain  
> **Level:** Beginner â†’ Advanced

---

## ğŸ“‹ Table of Contents

1. [Introduction](#1-introduction)
2. [Core Concepts & Terminology](#2-core-concepts--terminology)
3. [Decision Tree Structure (Visual)](#3-decision-tree-structure-visual)
4. [How a Decision Tree Works](#4-how-a-decision-tree-works)
5. [Mathematics â€” Step by Step](#5-mathematics--step-by-step)
   - [5.1 Entropy](#51-entropy)
   - [5.2 Information Gain](#52-information-gain)
   - [5.3 Gain Ratio](#53-gain-ratio)
   - [5.4 Gini Impurity (CART)](#54-gini-impurity-cart)
6. [The Algorithm](#6-the-algorithm)
7. [Worked Example â€” Loan Dataset](#7-worked-example--loan-dataset)
8. [Tree â†’ Rules Conversion](#8-tree--rules-conversion)
9. [Handling Continuous Attributes](#9-handling-continuous-attributes)
10. [Overfitting & Pruning](#10-overfitting--pruning)
11. [Algorithm Variants](#11-algorithm-variants)
12. [Visualizations & Diagrams](#12-visualizations--diagrams)
13. [Mnemonics & Cheat Sheet](#13-mnemonics--cheat-sheet)
14. [Practice Problems](#14-practice-problems)
15. [Best Practices](#15-best-practices)
16. [Common Pitfalls](#16-common-pitfalls)

---

## 1. Introduction

A **Decision Tree** is a supervised machine learning algorithm used for **classification** and **regression**. It learns a model shaped like a flowchart-tree where:

- Every **internal node** = a question about a feature
- Every **branch** = an answer/outcome to that question
- Every **leaf node** = a final class label or prediction

```
                  [Is it raining?]
                  /             \
               Yes               No
              /                    \
    [Do you have an umbrella?]    â†’ Go outside â˜€ï¸
         /          \
       Yes           No
        |              |
   Stay dry â˜‚ï¸     Get wet ğŸŒ§ï¸
```

### Why Use Decision Trees?

| Advantage | Explanation |
|---|---|
| âœ… Interpretable | Humans can read the rules directly |
| âœ… No feature scaling needed | Works with raw values |
| âœ… Handles mixed data | Numerical + categorical features |
| âœ… Competitive accuracy | Often rivals complex models |
| âœ… Fast prediction | Just traverse the tree |
| âŒ Prone to overfitting | Can memorize training data |
| âŒ Instability | Small data change â†’ different tree |
| âŒ NP-hard to optimize | All algorithms are heuristic |

> ğŸ’¡ **Best-known system:** C4.5 by Ross Quinlan (downloadable, widely used in practice)

---

## 2. Core Concepts & Terminology

| Term | Meaning | Example |
|---|---|---|
| **Root Node** | Top-most node; first question asked | "Own House?" |
| **Decision Node** | Internal node that tests an attribute | "Has Job?" |
| **Leaf Node** | Terminal node; holds a class label | "Yes" / "No" |
| **Branch** | Edge connecting nodes; represents a value/outcome | "true" or "false" |
| **Depth** | Longest path from root to leaf | 3 levels deep |
| **Purity** | All examples in node are same class | 6 Yes, 0 No = pure |
| **Impurity** | Mix of classes in a node | 3 Yes, 3 No = max impure |
| **Entropy** | Mathematical measure of impurity/disorder | 0 = pure, 1 = max disorder |
| **Information Gain** | Reduction in entropy after a split | Higher = better attribute |
| **Pruning** | Removing branches to reduce overfitting | Pre/Post pruning |
| **Support** | Fraction of training data a rule covers | 6/15 = 40% |
| **Confidence** | Accuracy of the rule on covered examples | 6/6 = 100% |

---

## 3. Decision Tree Structure (Visual)

### Full Tree (from Loan Dataset)

```
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚    Age?     â”‚  â† Root Node (Decision Node)
                         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           Young             middle                old
              â”‚                 â”‚                   â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”
     â”‚   Has_Job?      â”‚  â”‚ Own_House? â”‚   â”‚ Credit_Rating?â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â”Œâ”€â”€â”€â”´â”€â”€â”€â”         â”Œâ”€â”€â”€â”´â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”
        true   false      true   false      fair  good  excellent
          â”‚       â”‚         â”‚       â”‚         â”‚     â”‚       â”‚
        Yes      No        Yes     No        No   Yes     Yes
       (2/2)   (3/3)      (3/3)  (2/2)     (1/1)(2/2)   (2/2)
         â†‘                                             â†‘
      Leaf Node                                    Leaf Node
```

### Simpler Equivalent Tree (Better!)

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Own_House?  â”‚  â† Root (highest info gain)
                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
                  true          false
                    â”‚               â”‚
               â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
               â”‚   Yes   â”‚    â”‚ Has_Job?â”‚
               â”‚  (6/6)  â”‚    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
                            true     false
                              â”‚          â”‚
                            Yes         No
                           (5/5)       (4/4)
```

> ğŸ¯ **Key insight:** The second tree is simpler AND equally accurate. Simpler trees generalize better!

---

## 4. How a Decision Tree Works

### Classification Flow (Step-by-step)

Given a new record: `Age=young, Has_Job=false, Own_house=false, Credit=good`

```
Step 1: Start at ROOT â†’ "Own_House?"
                â†“
        Own_House = false â†’ go RIGHT

Step 2: Reach "Has_Job?"
                â†“
        Has_Job = false â†’ go RIGHT

Step 3: Reach LEAF â†’ "No"
                â†“
        ğŸ¦ LOAN REJECTED
```

### Tree Building (High Level)

```
Start with ALL training data at root
         â”‚
         â–¼
  For each attribute, compute
  how much it reduces impurity
         â”‚
         â–¼
  Pick attribute with MAX gain
  â†’ Make it the current node
         â”‚
         â–¼
  Split data into subsets
  (one per attribute value)
         â”‚
         â–¼
  Recurse on each subset
  until stopping condition
```

### Stopping Conditions

```
STOP when any of these are true:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â‘  All examples in node = same class             â”‚
â”‚    â†’ Make leaf with that class                  â”‚
â”‚                                                 â”‚
â”‚ â‘¡ No attributes left to split on               â”‚
â”‚    â†’ Make leaf with MAJORITY class              â”‚
â”‚                                                 â”‚
â”‚ â‘¢ No examples left in this branch              â”‚
â”‚    â†’ Make leaf with PARENT'S majority class     â”‚
â”‚                                                 â”‚
â”‚ â‘£ Gain < threshold (pre-pruning)               â”‚
â”‚    â†’ Make leaf with majority class              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 5. Mathematics â€” Step by Step

### 5.1 Entropy

#### What is Entropy?

Entropy measures **how mixed/disordered** a dataset is.

```
High Entropy = Mixed Classes = Uncertain = More info needed
Low Entropy  = Pure Classes  = Certain   = Less info needed

Coin example:
â€¢ Fair coin (50/50): entropy = 1.0  â†’ you know nothing!
â€¢ Biased coin (99/1): entropy â‰ˆ 0.08 â†’ pretty predictable
â€¢ One-sided coin (100/0): entropy = 0  â†’ you know everything!
```

#### Formula

$$\text{entropy}(D) = -\sum_{j=1}^{|C|} \Pr(c_j) \cdot \log_2 \Pr(c_j)$$

**Notation Key:**

| Symbol | Meaning |
|---|---|
| `D` | Dataset (set of examples) |
| `C` | Set of all class labels |
| `\|C\|` | Number of distinct classes |
| `Pr(câ±¼)` | Proportion of class câ±¼ in D |
| `logâ‚‚` | Logarithm base 2 |

> âš ï¸ Convention: `0 Ã— logâ‚‚(0) = 0` (not undefined)

#### Step-by-Step Entropy Calculation

**Example:** Dataset D has 9 Yes, 6 No (total 15 examples)

```
Step 1: Find class probabilities
        Pr(Yes) = 9/15 = 0.6
        Pr(No)  = 6/15 = 0.4

Step 2: Apply entropy formula
        entropy(D) = -(0.6 Ã— logâ‚‚0.6) - (0.4 Ã— logâ‚‚0.4)

Step 3: Calculate each log
        logâ‚‚(0.6) = log(0.6)/log(2) = -0.737
        logâ‚‚(0.4) = log(0.4)/log(2) = -1.322

Step 4: Multiply
        -(0.6 Ã— -0.737) = +0.442
        -(0.4 Ã— -1.322) = +0.529

Step 5: Sum
        entropy(D) = 0.442 + 0.529 = 0.971
```

#### Entropy Intuition Table

| Pr(positive) | Pr(negative) | Entropy | Meaning |
|---|---|---|---|
| 0.5 | 0.5 | **1.000** | Maximum confusion |
| 0.2 | 0.8 | **0.722** | Leaning negative |
| 0.9 | 0.1 | **0.469** | Mostly positive |
| 1.0 | 0.0 | **0.000** | Perfectly pure |

#### Entropy Curve

```
Entropy
  1.0 â”‚         â—
      â”‚      â—     â—
  0.7 â”‚    â—         â—
      â”‚  â—             â—
  0.4 â”‚â—                 â—
      â”‚                    â—
  0.0 â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—
      0   0.2  0.4  0.6  0.8  1.0
                              Pr(positive)

Peak at 0.5 â†’ maximum uncertainty
Drops to 0 at 0 or 1 â†’ completely certain
```

---

### 5.2 Information Gain

#### What is Information Gain?

Information Gain tells us **how much an attribute reduces uncertainty** in the data. We always pick the attribute with the **highest gain**.

```
Gain = entropy BEFORE split  âˆ’  weighted entropy AFTER split
     = How messy things were  âˆ’  How messy things are now
```

#### Formulas

**Step 1 â€” Entropy of current node:**
$$\text{entropy}(D) = -\sum_{j=1}^{|C|} \Pr(c_j) \cdot \log_2 \Pr(c_j)$$

**Step 2 â€” Expected entropy AFTER splitting on attribute Aáµ¢ (with v values):**
$$\text{entropy}_{A_i}(D) = \sum_{j=1}^{v} \frac{|D_j|}{|D|} \times \text{entropy}(D_j)$$

**Step 3 â€” Information Gain:**
$$\text{gain}(D, A_i) = \text{entropy}(D) - \text{entropy}_{A_i}(D)$$

**Notation Key:**

| Symbol | Meaning |
|---|---|
| `Aáµ¢` | Attribute being evaluated |
| `v` | Number of distinct values of Aáµ¢ |
| `Dâ±¼` | Subset of D where Aáµ¢ = vâ±¼ |
| `\|Dâ±¼\|` | Size of subset Dâ±¼ |
| `\|D\|` | Total size of dataset |
| `\|Dâ±¼\|/\|D\|` | Weight (fraction of data in this subset) |

---

### 5.3 Gain Ratio

#### Problem with Information Gain

Information Gain is **biased** toward attributes with many values (e.g., a unique ID column would always win, but gives useless splits).

#### Fix: Gain Ratio (used in C4.5)

$$\text{GainRatio}(D, A_i) = \frac{\text{gain}(D, A_i)}{\text{SplitInfo}(A_i)}$$

$$\text{SplitInfo}(A_i) = -\sum_{j=1}^{v} \frac{|D_j|}{|D|} \cdot \log_2 \frac{|D_j|}{|D|}$$

> ğŸ’¡ SplitInfo is the entropy of the split itself. Attributes that split data very unevenly get penalized.

**Example:** Attribute with 15 unique values â†’ SplitInfo â‰ˆ logâ‚‚(15) = 3.91 (large penalty). Attribute with 2 balanced values â†’ SplitInfo = 1.0 (small penalty).

---

### 5.4 Gini Impurity (CART)

Used by the **CART** algorithm (sklearn's default):

$$\text{Gini}(D) = 1 - \sum_{j=1}^{|C|} \Pr(c_j)^2$$

**Comparison:**

| Scenario | Entropy | Gini |
|---|---|---|
| 50/50 split | 1.000 | 0.500 |
| 80/20 split | 0.722 | 0.320 |
| 100/0 split | 0.000 | 0.000 |

Both measure impurity. Gini is slightly faster to compute (no log needed).

---

## 6. The Algorithm

```
Algorithm: decisionTree(D, A, T)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
INPUT:
  D = set of training examples
  A = set of available attributes
  T = current tree node

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1.  IF all examples in D belong to same class cáµ¢:
        â†’ make T a LEAF node labeled cáµ¢
        â†’ RETURN

2.  ELSE IF A is empty (no attributes left):
        â†’ make T a LEAF node labeled with MAJORITY class in D
        â†’ RETURN

3.  ELSE:  (D has mixed classes, attributes available)

    a. Compute pâ‚€ = entropy(D)      â† baseline impurity

    b. FOR each attribute Aáµ¢ in A:
           compute páµ¢ = entropy_Aáµ¢(D)   â† post-split impurity

    c. Select Ag = attribute with MAXIMUM (pâ‚€ - páµ¢)
                                         â† best gain

    d. IF (pâ‚€ - p_Ag) < threshold:       â† gain too small
           â†’ make T a LEAF with majority class
           â†’ RETURN   (Pre-pruning)

    e. ELSE:
           â†’ make T a DECISION NODE on Ag

           FOR each value vâ±¼ of Ag:
               Dâ±¼ = subset of D where Ag = vâ±¼
               IF Dâ±¼ â‰  empty:
                   create child node Tâ±¼
                   decisionTree(Dâ±¼, A - {Ag}, Tâ±¼)  â† recurse!
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

### Key Properties

```
TYPE:       Greedy, top-down, recursive divide-and-conquer
GUARANTEE:  Locally optimal (not globally optimal)
COMPLEXITY: Finding BEST tree is NP-hard â†’ use heuristics
UNIQUENESS: Decision trees are NOT unique for the same data
```

---

## 7. Worked Example â€” Loan Dataset

### The Data (15 examples, 4 features)

| ID | Age | Has_Job | Own_House | Credit_Rating | Class |
|---|---|---|---|---|---|
| 1 | young | false | false | fair | **No** |
| 2 | young | false | false | good | **No** |
| 3 | young | true | false | good | **Yes** |
| 4 | young | true | true | fair | **Yes** |
| 5 | young | false | false | fair | **No** |
| 6 | middle | false | false | fair | **No** |
| 7 | middle | false | false | good | **No** |
| 8 | middle | true | true | good | **Yes** |
| 9 | middle | false | true | excellent | **Yes** |
| 10 | middle | false | true | excellent | **Yes** |
| 11 | old | false | true | excellent | **Yes** |
| 12 | old | false | true | good | **Yes** |
| 13 | old | true | false | good | **Yes** |
| 14 | old | true | false | excellent | **Yes** |
| 15 | old | false | false | fair | **No** |

**Summary:** 9 Yes, 6 No out of 15 total

---

### Step 1: Compute Root Entropy

$$\text{entropy}(D) = -\frac{6}{15}\log_2\frac{6}{15} - \frac{9}{15}\log_2\frac{9}{15}$$

```
Pr(No)  = 6/15 = 0.4   â†’  -0.4 Ã— logâ‚‚(0.4) = -0.4 Ã— (-1.322) = 0.529
Pr(Yes) = 9/15 = 0.6   â†’  -0.6 Ã— logâ‚‚(0.6) = -0.6 Ã— (-0.737) = 0.442

entropy(D) = 0.529 + 0.442 = 0.971
```

---

### Step 2: Compute Gain for Each Attribute

#### Attribute: Age

Subsets after splitting on Age:

| Age Value | Yes | No | Total | Entropy of subset |
|---|---|---|---|---|
| young | 2 | 3 | 5 | âˆ’(2/5)logâ‚‚(2/5) âˆ’ (3/5)logâ‚‚(3/5) = **0.971** |
| middle | 3 | 2 | 5 | âˆ’(3/5)logâ‚‚(3/5) âˆ’ (2/5)logâ‚‚(2/5) = **0.971** |
| old | 4 | 1 | 5 | âˆ’(4/5)logâ‚‚(4/5) âˆ’ (1/5)logâ‚‚(1/5) = **0.722** |

$$\text{entropy}_{Age}(D) = \frac{5}{15}(0.971) + \frac{5}{15}(0.971) + \frac{5}{15}(0.722)$$
$$= 0.333 \times (0.971 + 0.971 + 0.722) = 0.888$$

$$\text{gain}(D, \text{Age}) = 0.971 - 0.888 = \mathbf{0.083}$$

---

#### Attribute: Own_House

| Own_House | Yes | No | Total | Entropy |
|---|---|---|---|---|
| true | 6 | 0 | 6 | **0.000** (pure!) |
| false | 3 | 6 | 9 | **0.918** |

$$\text{entropy}_{Own\_house}(D) = \frac{6}{15}(0) + \frac{9}{15}(0.918) = 0 + 0.551 = 0.551$$

$$\text{gain}(D, \text{Own\_House}) = 0.971 - 0.551 = \mathbf{0.420}$$ âœ… **HIGHEST!**

---

#### Attribute: Has_Job

| Has_Job | Yes | No | Total | Entropy |
|---|---|---|---|---|
| true | 5 | 0 | 5 | **0.000** (pure!) |
| false | 4 | 6 | 10 | **0.971** |

$$\text{entropy}_{Has\_Job}(D) = \frac{5}{15}(0) + \frac{10}{15}(0.971) = 0.647$$

$$\text{gain}(D, \text{Has\_Job}) = 0.971 - 0.647 = \mathbf{0.324}$$

---

#### Attribute: Credit_Rating

| Credit | Yes | No | Total | Entropy |
|---|---|---|---|---|
| fair | 1 | 4 | 5 | 0.722 |
| good | 4 | 2 | 6 | 0.918 |
| excellent | 4 | 0 | 4 | 0.000 |

$$\text{entropy}_{Credit}(D) = \frac{5}{15}(0.722) + \frac{6}{15}(0.918) + \frac{4}{15}(0) = 0.608$$

$$\text{gain}(D, \text{Credit\_Rating}) = 0.971 - 0.608 = \mathbf{0.363}$$

---

### Step 3: Compare All Gains

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Attribute          â”‚ Expected Entropy     â”‚ Gain         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Age                â”‚ 0.888                â”‚ 0.083        â”‚
â”‚ Has_Job            â”‚ 0.647                â”‚ 0.324        â”‚
â”‚ Own_House          â”‚ 0.551                â”‚ 0.420  â† MAX â”‚ âœ…
â”‚ Credit_Rating      â”‚ 0.608                â”‚ 0.363        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Winner: Own_House â†’ becomes ROOT NODE
```

---

### Step 4: Split and Recurse

**Branch 1: Own_House = true** â†’ 6 Yes, 0 No â†’ **PURE LEAF â†’ Yes**

**Branch 2: Own_House = false** â†’ 3 Yes, 6 No â†’ not pure, recurse!

For the "false" subset, compute gains for remaining attributes:
- Has_Job wins â†’ splits into Has_Job=true (5 Yes, 0 No â†’ **Yes**) and Has_Job=false (4 No, 0 Yes â†’ **No**)

---

### Final Tree

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Own_House?  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
                  true          false
                    â”‚               â”‚
               âœ… Yes           â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
               (6/6)            â”‚ Has_Job?â”‚
                                â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
                              â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”
                            true         false
                              â”‚               â”‚
                           âœ… Yes          âŒ No
                            (5/5)          (4/4)
```

---

## 8. Tree â†’ Rules Conversion

Each path from root to leaf = one IF-THEN rule

```
Own_house = true
    â†’ Class = Yes    [support=6/15=40%, confidence=6/6=100%]

Own_house = false AND Has_job = true
    â†’ Class = Yes    [support=5/15=33%, confidence=5/5=100%]

Own_house = false AND Has_job = false
    â†’ Class = No     [support=4/15=27%, confidence=4/4=100%]
```

### Rule Metrics

| Metric | Formula | Meaning |
|---|---|---|
| **Support** | (# examples covered by rule) / (total examples) | How often the rule fires |
| **Confidence** | (# correctly classified) / (# covered) | How accurate the rule is |
| **Coverage** | Same as support (alternate name) | â€” |
| **Lift** | Confidence / Pr(class) | Improvement over random guessing |

---

## 9. Handling Continuous Attributes

### The Challenge

Continuous features (like Age=22, 35, 47â€¦) can't be used directly for categorical splits.

### Solution: Binary Threshold Split

```
Approach:
1. Sort all values of continuous attribute: vâ‚ < vâ‚‚ < ... < váµ£
2. Candidate thresholds = midpoints between adjacent values
   â†’ tâ‚ = (vâ‚+vâ‚‚)/2, tâ‚‚ = (vâ‚‚+vâ‚ƒ)/2, ...
3. For each threshold táµ¢, create 2 branches:
   LEFT: examples where attribute â‰¤ táµ¢
   RIGHT: examples where attribute > táµ¢
4. Compute gain for each threshold
5. Pick threshold with MAXIMUM gain
```

### Example

```
Age values: [22, 25, 28, 35, 40]
Classes:    [No, No, Yes, Yes, Yes]

Candidate thresholds: 23.5, 26.5, 31.5, 37.5

Threshold 26.5:
  Left  (â‰¤26.5): 22, 25  â†’ [No, No]  â†’ entropy = 0
  Right (>26.5): 28,35,40 â†’ [Yes,Yes,Yes] â†’ entropy = 0
  â†’ gain = max possible!  âœ… Best threshold = 26.5
```

### Visual (from slides)

```
Y-axis
  2.6 â”‚ â–  â–  â—‹ â—‹ â—‹              Corresponding Decision Tree:
  2.5 â”‚ â–  â—‹ â—‹ â—‹ â—‹                        [X â‰¤ 2?]
  2.0 â”‚ â–  â–  â—‹â—‹â–  â–                         /       \
      â”‚ â–  â–  â–  â– â—‹ â–               [Y â‰¤ 2.5]        [Y â‰¤ 2]
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ X-axis
         2   3   4
         
â–  = Class 1,  â—‹ = Class 2
Horizontal/vertical splits partition the space
```

> âš ï¸ Note: The SAME continuous attribute can be reused at different nodes with different thresholds!

---

## 10. Overfitting & Pruning

### What is Overfitting?

```
Overfitting = Tree memorizes training data
              â†’ Perfect training accuracy
              â†’ Poor test/real-world accuracy

Signs:
  â€¢ Tree is very deep (many levels)
  â€¢ Many branches with very few examples
  â€¢ Leaf nodes with 1-2 examples
  â€¢ Training accuracy >> Test accuracy
```

```
Training Data          Overfitted Tree        Generalized Tree
    â— â—‹ â—                   â”Œâ”€â”                    â”Œâ”€â”€â”€â”€â”€â”
  â—‹ â— â—‹ â—‹            â”Œâ”€â”   â”‚ â”‚   â”Œâ”€â”              â”‚     â”‚
    â— â—‹ â—            â”‚ â”‚  ...â”‚...  â”‚ â”‚         â”Œâ”€â”€â”€â”˜     â””â”€â”€â”€â”
                  (very deep, fits noise)    (simpler, robust)
```

### Two Types of Pruning

#### Pre-Pruning (Early Stopping)

Stop growing the tree **before** it becomes too complex.

```
Conditions to stop:
  â€¢ Gain < threshold (e.g., gain < 0.01)
  â€¢ Node has < min_samples (e.g., < 5 examples)
  â€¢ Tree depth > max_depth
  â€¢ Statistical test shows improvement is not significant

PRO: Fast
CON: May stop too early (underfitting risk)
     Hard to know what you might miss
```

#### Post-Pruning (Preferred)

Grow a full tree FIRST, then trim it back.

```
Process:
  1. Grow full tree (no stopping conditions)
  2. Evaluate each subtree
  3. Compare: leaf_node vs full_subtree
     â€¢ Estimate error of subtree on validation data
     â€¢ If leaf â‰¥ subtree in accuracy â†’ replace subtree with leaf
  4. Repeat bottom-up until no improvement

Methods:
  â€¢ C4.5: Pessimistic error estimation
  â€¢ Reduced Error Pruning: Use held-out validation set
  â€¢ Cost-Complexity Pruning (sklearn): Î± parameter

PRO: More reliable, empirically better
CON: Slightly slower (build full tree first)
```

### Visual: Before and After Pruning

```
BEFORE PRUNING (Overfitted):        AFTER PRUNING (Cleaner):

         [X]                                [X]
        /   \                              /   \
      â‰¤2     >2                          â‰¤2     >2
      /       \                          â–        [Y]
   [Y]         [Y]                              /   \
   / \         / \                           â‰¤2      >2
 â‰¤2.5 >2.5   â‰¤2  >2                         [X]      â—‹
 /    [Y]    [X]  â—‹                         / \
â–      /\    / \                           â‰¤3   >3
   â‰¤2.6 >2.6 â‰¤3 >3                        â–    [X]
   â—‹    â–     â–   [X]                           / \
                /\                           â‰¤4  >4
              â‰¤4  >4                         â—‹   â– 
              â—‹    â– 
```

---

## 11. Algorithm Variants

| Algorithm | Year | Split Criterion | Splits | Handles Continuous | Missing Values |
|---|---|---|---|---|---|
| **ID3** | 1986 | Information Gain | Multi-way | âŒ No | âŒ No |
| **C4.5** | 1993 | Gain Ratio | Multi-way | âœ… Yes | âœ… Yes |
| **CART** | 1984 | Gini Impurity | Binary only | âœ… Yes | âœ… Yes |
| **CHAID** | 1980 | Chi-Square | Multi-way | âœ… Yes | âœ… Yes |
| **C5.0** | 2000s | Gain Ratio | Multi-way | âœ… Yes | âœ… Yes |

### ID3 vs C4.5 vs CART

```
ID3:                    C4.5:                   CART:
â€¢ Simple, classic       â€¢ Most popular          â€¢ sklearn default
â€¢ Gain only             â€¢ Gain Ratio            â€¢ Gini Impurity
â€¢ Categorical only      â€¢ Continuous ok         â€¢ Binary splits only
â€¢ Prone to bias         â€¢ Handles missing       â€¢ Regression too
                        â€¢ Post-pruning          â€¢ Produces 2-way splits
```

### Decision Trees in Ensembles

```
Single Tree          Random Forest           Gradient Boosting
    [T]              [Tâ‚][Tâ‚‚]...[Tâ‚™]         Tâ‚â†’Tâ‚‚â†’Tâ‚ƒâ†’...â†’Tâ‚™
     â”‚               Each trained on          Each tree corrects
     â†“               random data subset       errors of previous
  Prediction         + random features
                     Average predictions
                     
â€¢ High variance     â€¢ Low variance           â€¢ Low bias
â€¢ Interpretable     â€¢ Not interpretable      â€¢ Not interpretable
â€¢ Fast              â€¢ Slower                 â€¢ Slowest
```

---

## 12. Visualizations & Diagrams

### Entropy as a Function of Class Balance

```
      Entropy
  1.0 â”¤         â—         â† Most impure (50/50)
      â”‚      â—     â—
  0.8 â”¤    â—         â—
      â”‚  â—             â—
  0.6 â”¤â—                 â—
      â”‚                    â—
  0.4 â”¤                      â—
      â”‚                        â—
  0.2 â”¤                          â—
      â”‚                             â—
  0.0 â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—  â† Pure (0/100 or 100/0)
      0   0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1.0
                              Proportion of Positive Class
```

### Information Gain = Area Saved

```
Before Split:          After Split on Attr A:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â—‹ â— â— â—‹ â— â—‹ â”‚  â†’   â”‚ â— â— â”‚  â”‚ â—‹ â—‹ â—‹ â—‹ â”‚
â”‚ â—‹ â—‹ â— â— â—‹ â— â”‚  â†’   â”‚ â— â— â”‚  â”‚ â—‹ â—‹ â—‹   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
entropy = 0.971         entropy=0  entropy=0
                       
Gain = 0.971 - (6/15Ã—0 + 9/15Ã—0) = 0.971 (perfect split!)
```

### Decision Boundary in 2D Space

```
Y
2.6 â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â—‹ â—‹ â—‹
2.5 â”‚    â–      â”‚       â—‹    â† Decision tree creates
    â”‚          â”‚              axis-aligned rectangular
2.0 â”‚    â–      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    boundaries in the feature
    â”‚    â–      â”‚ â—‹ â—‹  â–        space
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    0    2     3    4    X

Each vertical/horizontal line = one decision node split
```

### Overfitting Visualization

```
Model Complexity (Tree Depth)
         â†‘ Accuracy
    100% â”‚       â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â—  â† Training accuracy (keeps rising)
         â”‚     â—         â—
     80% â”‚   â—         â—
         â”‚  â—         â—
     60% â”‚ â—       â—         â† Test accuracy (peaks then drops)
         â”‚         
     40% â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’
         0    2    4    6    8   Max Depth

        Underfitting â†” Sweet Spot â†” Overfitting
```

---

## 13. Mnemonics & Cheat Sheet

### ğŸ§  Remember: "SELF" to Build a Tree

```
S â†’ Select best attribute (max information gain)
E â†’ Expand the node (make it a decision node)
L â†’ Leaf if stopping condition is met
F â†’ Further recurse on each subset
```

### ğŸ§  Remember: Entropy Extremes

```
"No Mix = Zero, Equal Mix = One"
Pure dataset   â†’ entropy = 0  (you know everything, no new info needed)
50/50 split    â†’ entropy = 1  (you know nothing, maximum uncertainty)
```

### ğŸ§  Remember: Gain Formula in Words

```
GAIN = (Disorder BEFORE split) âˆ’ (Disorder AFTER split)
     = What we started with   âˆ’  What we're left with
     = How much cleaner it got
```

### ğŸ§  Remember: Which Algorithm Uses What

```
"ID3 Gains, C4.5 Ratios, CART Grins (Gini)"
```

### ğŸ“‹ Quick Reference Card

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   DECISION TREE QUICK REFERENCE                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Formula              â”‚ Description                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ entropy(D)           â”‚ -Î£ Pr(câ±¼) Ã— logâ‚‚ Pr(câ±¼)               â”‚
â”‚ entropy_A(D)         â”‚ Î£ (|Dâ±¼|/|D|) Ã— entropy(Dâ±¼)            â”‚
â”‚ gain(D, A)           â”‚ entropy(D) - entropy_A(D)               â”‚
â”‚ gainRatio(D, A)      â”‚ gain(D, A) / SplitInfo(A)               â”‚
â”‚ SplitInfo(A)         â”‚ -Î£ (|Dâ±¼|/|D|) Ã— logâ‚‚(|Dâ±¼|/|D|)       â”‚
â”‚ Gini(D)              â”‚ 1 - Î£ Pr(câ±¼)Â²                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Entropy range        â”‚ [0, logâ‚‚(|C|)]  â†’ [0, 1] for binary    â”‚
â”‚ Gain range           â”‚ [0, entropy(D)]                          â”‚
â”‚ GainRatio range      â”‚ [0, 1]                                   â”‚
â”‚ Gini range           â”‚ [0, 0.5] for binary                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Best attribute       â”‚ MAX gain (or max gain ratio)             â”‚
â”‚ Stop condition       â”‚ Pure node OR no attributes OR no data    â”‚
â”‚ Overfitting fix      â”‚ Pruning (post > pre)                     â”‚
â”‚ Continuous feature   â”‚ Try all midpoint thresholds              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 14. Practice Problems

### Problem 1 â€” Basic Entropy (Easy)

A dataset has 10 examples: 7 positive, 3 negative.  
**Calculate entropy(D).**

<details>
<summary>Solution</summary>

```
Pr(positive) = 7/10 = 0.7
Pr(negative) = 3/10 = 0.3

entropy(D) = -(0.7 Ã— logâ‚‚0.7) - (0.3 Ã— logâ‚‚0.3)
           = -(0.7 Ã— -0.515) - (0.3 Ã— -1.737)
           = 0.361 + 0.521
           = 0.882
```
</details>

---

### Problem 2 â€” Which Split is Better? (Medium)

| Split | Left Branch | Right Branch |
|---|---|---|
| **A** | 4 Yes, 0 No | 1 Yes, 5 No |
| **B** | 3 Yes, 2 No | 2 Yes, 3 No |

Original dataset: 5 Yes, 5 No. Total = 10 examples.  
**Which split gives higher information gain?**

<details>
<summary>Solution</summary>

```
entropy(D) = -(0.5 Ã— logâ‚‚0.5) - (0.5 Ã— logâ‚‚0.5) = 1.0

Split A:
  Left  (4 Yes, 0 No): entropy = 0          [size=4]
  Right (1 Yes, 5 No): -1/6Ã—logâ‚‚(1/6) - 5/6Ã—logâ‚‚(5/6)
                      = 0.650               [size=6]
  entropy_A = (4/10)Ã—0 + (6/10)Ã—0.650 = 0.390
  gain_A = 1.0 - 0.390 = 0.610

Split B:
  Left  (3 Yes, 2 No): -0.6Ã—logâ‚‚0.6 - 0.4Ã—logâ‚‚0.4 = 0.971  [size=5]
  Right (2 Yes, 3 No): same = 0.971                            [size=5]
  entropy_B = (5/10)Ã—0.971 + (5/10)Ã—0.971 = 0.971
  gain_B = 1.0 - 0.971 = 0.029

WINNER: Split A (gain=0.610 >> 0.029)
Reason: Split A creates a pure left node!
```
</details>

---

### Problem 3 â€” Build a Mini Tree (Medium-Hard)

**Dataset (Weather â†’ Play Tennis):**

| Weather | Wind | Humidity | Play |
|---|---|---|---|
| Sunny | Weak | High | No |
| Sunny | Strong | High | No |
| Overcast | Weak | High | Yes |
| Rainy | Weak | High | Yes |
| Rainy | Weak | Normal | Yes |
| Rainy | Strong | Normal | No |
| Overcast | Strong | Normal | Yes |
| Sunny | Weak | Normal | Yes |

Total: 5 Yes, 3 No

**Task:** Compute entropy and information gain for each attribute. Which is the root?

<details>
<summary>Solution</summary>

```
entropy(D) = -(5/8)logâ‚‚(5/8) - (3/8)logâ‚‚(3/8) = 0.954

Attribute: Weather
  Sunny    â†’ 1 Yes, 2 No â†’ entropy = 0.918   [size=3]
  Overcast â†’ 2 Yes, 0 No â†’ entropy = 0        [size=2]
  Rainy    â†’ 2 Yes, 1 No â†’ entropy = 0.918   [size=3]
  entropy_Weather = (3/8)(0.918) + (2/8)(0) + (3/8)(0.918)
                  = 0.344 + 0 + 0.344 = 0.688
  gain(Weather) = 0.954 - 0.688 = 0.266

Attribute: Wind
  Weak   â†’ 4 Yes, 1 No â†’ entropy = 0.722  [size=5]
  Strong â†’ 1 Yes, 2 No â†’ entropy = 0.918  [size=3]
  entropy_Wind = (5/8)(0.722) + (3/8)(0.918) = 0.451 + 0.344 = 0.795
  gain(Wind) = 0.954 - 0.795 = 0.159

Attribute: Humidity
  High   â†’ 2 Yes, 2 No â†’ entropy = 1.0   [size=4]
  Normal â†’ 3 Yes, 1 No â†’ entropy = 0.811  [size=4]
  entropy_Humidity = (4/8)(1.0) + (4/8)(0.811) = 0.500 + 0.406 = 0.906
  gain(Humidity) = 0.954 - 0.906 = 0.048

WINNER: Weather (gain=0.266) â†’ Root Node
```
</details>

---

### Problem 4 â€” Continuous Threshold (Medium)

Feature "Temperature" with values and classes:

| Temp | 15 | 18 | 22 | 25 | 30 | 35 |
|---|---|---|---|---|---|---|
| Class | No | No | No | Yes | Yes | Yes |

**Find the best binary split threshold.**

<details>
<summary>Solution</summary>

```
entropy(D): 3 Yes, 3 No â†’ entropy = 1.0

Candidate thresholds: 16.5, 20, 23.5, 27.5, 32.5

Threshold 23.5 (â‰¤23.5 vs >23.5):
  Left  (15,18,22 â†’ No,No,No): entropy = 0   [size=3]
  Right (25,30,35 â†’ Yes,Yes,Yes): entropy = 0 [size=3]
  expected entropy = (3/6)(0) + (3/6)(0) = 0
  gain = 1.0 - 0 = 1.0  â† PERFECT SPLIT! âœ…

Best threshold = 23.5
```
</details>

---

### Problem 5 â€” Gain Ratio (Hard)

Using the same dataset as the worked example (15 loans):  
**Compute Gain Ratio for Own_House and Age.**

<details>
<summary>Solution</summary>

```
SplitInfo(Own_House):
  P(true) = 6/15,  P(false) = 9/15
  SplitInfo = -(6/15)logâ‚‚(6/15) - (9/15)logâ‚‚(9/15)
            = -0.4Ã—(-1.322) - 0.6Ã—(-0.737) = 0.529 + 0.442 = 0.971

GainRatio(Own_House) = 0.420 / 0.971 = 0.432

SplitInfo(Age):
  P(young)=P(middle)=P(old) = 5/15 = 1/3 each
  SplitInfo = -3 Ã— (1/3)logâ‚‚(1/3) = -3 Ã— (1/3 Ã— -1.585) = 1.585

GainRatio(Age) = 0.083 / 1.585 = 0.052

CONCLUSION: Own_House still wins (0.432 >> 0.052)
Age gets penalized heavily for having 3 equal-sized groups
```
</details>

---

## 15. Best Practices

### 1. Choose the Right Split Criterion

```
Use Gain Ratio (C4.5) when:
  â†’ You have attributes with many distinct values
  â†’ You want to avoid bias toward high-cardinality features

Use Gini (CART) when:
  â†’ You need speed (no log computation)
  â†’ You want compatibility with sklearn

Use Information Gain (ID3) when:
  â†’ Learning the concept (simpler math)
  â†’ All attributes have similar cardinality
```

### 2. Control Tree Complexity

```python
# sklearn example â€” hyperparameters to tune
DecisionTreeClassifier(
    max_depth=5,           # Limit tree depth
    min_samples_split=10,  # Min samples to split a node
    min_samples_leaf=5,    # Min samples in each leaf
    max_features='sqrt',   # Use random subset of features
    ccp_alpha=0.01         # Cost-complexity pruning strength
)
```

### 3. Validation Strategy

```
ALWAYS evaluate on held-out test data:

Option A: Train/Test Split (simple)
  70% training â†’ build tree
  30% testing  â†’ evaluate

Option B: K-Fold Cross Validation (robust)
  Split data into k folds
  Train on k-1, test on 1
  Repeat k times, average the scores

Option C: Separate Pruning Set (for post-pruning)
  60% training â†’ build full tree
  20% validation â†’ pruning decisions
  20% testing  â†’ final evaluation
```

### 4. Feature Engineering

```
Decision trees benefit from:
  âœ… Creating interaction features (A AND B)
  âœ… Binning continuous features carefully
  âœ… Encoding ordinal categories as ordered numbers
  âœ… Removing irrelevant/redundant features (reduces tree size)

Decision trees DON'T need:
  âŒ Feature scaling (not distance-based)
  âŒ Normalization
  âŒ Handling of correlated features (tree handles it)
```

### 5. Handling Class Imbalance

```
Problem: 95% Class A, 5% Class B
         Tree predicts A for everything â†’ 95% accuracy but useless!

Solutions:
  â€¢ class_weight='balanced' in sklearn
  â€¢ Oversample minority class (SMOTE)
  â€¢ Undersample majority class
  â€¢ Use precision/recall/F1 instead of accuracy
```

---

## 16. Common Pitfalls

### âŒ Pitfall 1: Overfitting

```
Problem: Tree grows too deep, fits noise
         Training: 99% accuracy
         Test: 62% accuracy

Fix:
  â€¢ Post-pruning (preferred)
  â€¢ Set max_depth=5 or 10
  â€¢ Require min_samples_leaf â‰¥ 5
```

### âŒ Pitfall 2: Information Gain Bias

```
Problem: Attribute "Customer_ID" has 1000 unique values
         ID3 always picks it (perfect split, gain=max!)
         But it's USELESS for prediction

Fix:
  â€¢ Use Gain Ratio (C4.5) instead of raw Gain
  â€¢ Remove ID/timestamp columns before training
```

### âŒ Pitfall 3: Instability

```
Problem: Remove one training example â†’ completely different tree
         Decision trees have HIGH variance

Fix:
  â€¢ Use ensemble methods: Random Forest, Gradient Boosting
  â€¢ Average predictions across many trees = stability
```

### âŒ Pitfall 4: Axis-Aligned Boundaries Only

```
Problem: Trees can only make horizontal/vertical splits
         Diagonal or curved boundaries require many nodes

Example: Data separated by diagonal line y = x
         Tree needs many steps to approximate it:
         
         Real boundary: /    Tree boundary: â”
                                           â”‚ â”
                                             â”‚ â”

Fix:
  â€¢ Create diagonal features (e.g., feature1 - feature2)
  â€¢ Use SVM or neural nets for diagonal boundaries
```

### âŒ Pitfall 5: Missing Values

```
Problem: ID3 can't handle missing values at all

Fixes:
  â€¢ C4.5: Distributes example with weight across all branches
  â€¢ CART: Surrogate splits (use second-best attribute as backup)
  â€¢ Imputation: Fill missing values before training
  â€¢ sklearn: Does NOT support missing values natively â†’ impute first
```

### âŒ Pitfall 6: Assuming the Tree is Optimal

```
REMEMBER: Finding the globally optimal tree is NP-HARD

All algorithms are greedy heuristics:
  â€¢ Make locally optimal choice at each step
  â€¢ May miss globally better tree structure
  â€¢ The tree YOU get â‰  the BEST possible tree

This is WHY ensembles (Random Forests) outperform single trees
```

---

## ğŸ“š Summary Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DECISION TREE LEARNING â€” OVERVIEW                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                       â”‚
â”‚  DATA â†’ [Compute Entropy] â†’ [Compute Gain per Attribute]             â”‚
â”‚                â†“                         â†“                            â”‚
â”‚           entropy(D)            gain(D, Aáµ¢) = entropy(D)             â”‚
â”‚       = -Î£ Pr(c)logâ‚‚Pr(c)       - Î£(|Dâ±¼|/|D|)Ã—entropy(Dâ±¼)         â”‚
â”‚                                                                       â”‚
â”‚  [Pick MAX gain attribute] â†’ [Split data] â†’ [Recurse on subsets]     â”‚
â”‚                                                                       â”‚
â”‚  STOPPING: Pure node | No attributes | Gain < threshold               â”‚
â”‚                                                                       â”‚
â”‚  OVERFITTING: Prune! (Post-pruning preferred)                         â”‚
â”‚                                                                       â”‚
â”‚  CONTINUOUS: Try all midpoint thresholds, pick max gain               â”‚
â”‚                                                                       â”‚
â”‚  RULES: Each rootâ†’leaf path = one IF-THEN rule                        â”‚
â”‚                                                                       â”‚
â”‚  VARIANTS: ID3 (Gain) | C4.5 (GainRatio) | CART (Gini)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

*Last updated: February 2026 | Based on CS583 (Bing Liu, UIC) + Subhash Bhagat Sir ML Slides*
