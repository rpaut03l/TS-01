# K-Nearest Neighbors (K-NN) Classification
## Complete Learning Guide with Practice Problems

---

## Table of Contents
- [ğŸ Introduction - The Apple Store Analogy](#-introduction---the-apple-store-analogy)
- [ğŸ“ Understanding Distance](#-understanding-distance)
- [ğŸ”¢ The Mathematics](#-the-mathematics)
  - [Understanding (x, y) Points](#step-1-understanding-x-y-points)
  - [Calculating Distance](#step-2-calculating-distance-pythagorean-theorem)
- [ğŸ“ Breaking Down the Notation](#-breaking-down-the-notation)
  - [Distance Notation](#what-does-this-mean)
  - [Sigma Notation](#what-does-this-mean-1)
- [ğŸ¯ Complete Distance Formula](#-complete-distance-formula---step-by-step)
- [ğŸ—³ï¸ The Voting Formula](#ï¸-the-voting-formula)
- [ğŸ“ The Complete K-NN Algorithm](#-the-complete-k-nn-algorithm---super-simple)
- [ğŸ“Š Visualizing Different K Values](#-visualizing-k1-vs-k3-vs-k5)
- [ğŸ”§ Feature Scaling](#-feature-scaling---why-it-matters)
- [ğŸ§® Scaling Formulas](#-scaling-formula-explained)
  - [Min-Max Normalization](#min-max-normalization)
  - [Standardization (Z-score)](#standardization-z-score)
- [ğŸ¯ Choosing K](#-choosing-k---the-simple-rules)
- [ğŸ§ª Practice Problems](#-practice-problem---step-by-step)
- [âœ… Summary](#-summary---the-big-picture)

---

## ğŸ Introduction - The Apple Store Analogy

Imagine you work at a fruit store and need to identify if a new fruit is an **apple** or an **orange**.

```
Your Memory (Training Data):
ğŸ Red, Round, 3 inches     â†’ APPLE
ğŸ Red, Round, 2.5 inches   â†’ APPLE
ğŸŠ Orange, Round, 3 inches  â†’ ORANGE
ğŸŠ Orange, Round, 4 inches  â†’ ORANGE
ğŸ Red, Round, 2.8 inches   â†’ APPLE

New Fruit: Red, Round, 2.7 inches â†’ ??? 
```

**K-NN says:** "Look at the 3 most similar fruits you've seen before (K=3), and whatever most of them are, that's your answer!"

**Step 1:** Measure how similar the new fruit is to each fruit in memory  
**Step 2:** Pick the 3 closest matches  
**Step 3:** Count votes: 3 apples, 0 oranges  
**Step 4:** Answer: APPLE! ğŸ

[Back to Top](#table-of-contents)

---

## ğŸ“ Understanding Distance

### **What is "Distance"?**

Think of it like measuring how different two things are:

```
Person A: Height=5ft, Weight=120lbs
Person B: Height=6ft, Weight=180lbs

How different are they?
Height difference: 6-5 = 1 foot
Weight difference: 180-120 = 60 lbs

Total difference = somehow combine these two numbers
```

[Back to Top](#table-of-contents)

---

## ğŸ”¢ The Mathematics

### **Step 1: Understanding (x, y) Points**

```
Imagine a graph paper:

      y (height)
      â†‘
    5 |     â€¢ Point A (3, 5)
    4 |   
    3 |  â€¢ Point B (2, 3)
    2 |
    1 |
    0 +---â†’ x (width)
      0 1 2 3 4 5
```

**Point A** has:
- x-coordinate (width) = 3
- y-coordinate (height) = 5

**Point B** has:
- x = 2
- y = 3

[Back to Top](#table-of-contents)

---

### **Step 2: Calculating Distance (Pythagorean Theorem)**

Remember the triangle formula from school? Same thing!

```
Point A (3, 5)
Point B (2, 3)

Step 1: Find horizontal difference
x-difference = 3 - 2 = 1

Step 2: Find vertical difference  
y-difference = 5 - 3 = 2

Step 3: Use Pythagorean theorem
      Bâ€¢â”€â”€â”€â”€â”
       â”‚    â”‚ 2 (height)
       â”‚    â”‚
       â””â”€â”€â”€â”€â€¢A
         1
       (width)

Distance = âˆš(1Â² + 2Â²)
        = âˆš(1 + 4)
        = âˆš5
        = 2.24
```

**In words:** The distance is approximately 2.24 units.

[Back to Top](#table-of-contents)

---

## ğŸ“ Breaking Down the Notation

### **What does this mean?** 
$$||x^{(a)} - x^{(b)}||_2$$

Let me decode this scary-looking formula:

```
||  ||     â†’ These bars mean "length" or "distance"
x^(a)      â†’ Point A (the little (a) is just a label)
x^(b)      â†’ Point B (the little (b) is just a label)
-          â†’ Minus (subtract)
_2         â†’ The subscript 2 means "Euclidean" (straight-line distance)
```

**Translation:** "The straight-line distance between point A and point B"

[Back to Top](#table-of-contents)

---

### **What does this mean?**
$$\sum_{j=1}^{d}$$

This is called **Sigma** - it means "add up"

```
Î£     â†’ Greek letter Sigma = "Sum" = "Add up"
j=1   â†’ Start with j=1 (first dimension)
d     â†’ Go until d (last dimension)
```

**Example:**
If you have 3 dimensions (d=3):

```
Î£(j=1 to 3) means:
Do this for j=1, then j=2, then j=3, and ADD them all up

If calculating (xâ±¼)Â²:
j=1: (xâ‚)Â² = first dimension squared
j=2: (xâ‚‚)Â² = second dimension squared  
j=3: (xâ‚ƒ)Â² = third dimension squared
Total = (xâ‚)Â² + (xâ‚‚)Â² + (xâ‚ƒ)Â²
```

[Back to Top](#table-of-contents)

---

## ğŸ¯ Complete Distance Formula - Step by Step

$$||x^{(a)} - x^{(b)}||_2 = \sqrt{\sum_{j=1}^{d}(x_j^{(a)} - x_j^{(b)})^2}$$

### **Let's decode EVERY symbol:**

| Symbol | Meaning | Example |
|--------|---------|---------|
| $x^{(a)}$ | Point A (all coordinates) | (3, 5, 2) |
| $x^{(b)}$ | Point B (all coordinates) | (1, 4, 6) |
| $x_j^{(a)}$ | The j-th coordinate of point A | $x_1^{(a)}=3$, $x_2^{(a)}=5$ |
| $x_j^{(b)}$ | The j-th coordinate of point B | $x_1^{(b)}=1$, $x_2^{(b)}=4$ |
| $d$ | Total number of dimensions | 3 (we have 3 coordinates) |
| $\sum$ | Add everything up | + + + |
| $\sqrt{}$ | Square root | âˆš |

---

### **Full Example with Real Numbers:**

```
Point A: (3, 5, 2)   â† This is x^(a)
Point B: (1, 4, 6)   â† This is x^(b)
Dimensions: d = 3

Step 1: For j=1 (first coordinate)
(xâ‚^(a) - xâ‚^(b))Â² = (3 - 1)Â² = 2Â² = 4

Step 2: For j=2 (second coordinate)
(xâ‚‚^(a) - xâ‚‚^(b))Â² = (5 - 4)Â² = 1Â² = 1

Step 3: For j=3 (third coordinate)
(xâ‚ƒ^(a) - xâ‚ƒ^(b))Â² = (2 - 6)Â² = (-4)Â² = 16

Step 4: Sum them (Î£ means add)
Sum = 4 + 1 + 16 = 21

Step 5: Square root (âˆš)
Distance = âˆš21 = 4.58
```

**Answer:** Points A and B are **4.58 units apart**

[Back to Top](#table-of-contents)

---

## ğŸ—³ï¸ The Voting Formula

$$y = \arg\max_{t^{(z)}} \sum_{r=1}^{k} \delta(t^{(z)}, t^{(r)})$$

This looks SCARY! Let's break it down:

### **Simple Translation:**
"Pick the class that gets the most votes from the K neighbors"

---

### **Symbol by Symbol:**

| Symbol | Meaning | Plain English |
|--------|---------|---------------|
| $y$ | The prediction | "Our answer" |
| $\arg\max$ | "Pick the one with maximum value" | "Choose the winner" |
| $t^{(z)}$ | A possible class (like "apple" or "orange") | "Option Z" |
| $\sum_{r=1}^{k}$ | Add up for all K neighbors | "Count all K votes" |
| $\delta$ | Delta function (1 if match, 0 if not) | "Does it match?" |
| $t^{(r)}$ | Class of the r-th neighbor | "What neighbor r voted for" |

---

### **What is Î´ (Delta)?**

Delta is a simple checker:

```python
Î´(A, B) = 1 if A equals B
Î´(A, B) = 0 if A doesn't equal B

Examples:
Î´("apple", "apple")  = 1  âœ“
Î´("apple", "orange") = 0  âœ—
Î´("red", "red")      = 1  âœ“
```

---

### **Complete Example:**

```
K = 5 (we look at 5 neighbors)

Neighbor 1: Apple  
Neighbor 2: Orange
Neighbor 3: Apple
Neighbor 4: Apple  
Neighbor 5: Orange

Question: How many votes for "Apple"?

For class "Apple":
r=1: Î´(Apple, Apple)  = 1 âœ“
r=2: Î´(Apple, Orange) = 0 âœ—
r=3: Î´(Apple, Apple)  = 1 âœ“
r=4: Î´(Apple, Apple)  = 1 âœ“
r=5: Î´(Apple, Orange) = 0 âœ—

Sum = 1 + 0 + 1 + 1 + 0 = 3 votes for Apple

For class "Orange":
r=1: Î´(Orange, Apple)  = 0 âœ—
r=2: Î´(Orange, Orange) = 1 âœ“
r=3: Î´(Orange, Apple)  = 0 âœ—
r=4: Î´(Orange, Apple)  = 0 âœ—
r=5: Î´(Orange, Orange) = 1 âœ“

Sum = 0 + 1 + 0 + 0 + 1 = 2 votes for Orange

argmax: Apple has 3, Orange has 2
Winner: APPLE! ğŸ
```

[Back to Top](#table-of-contents)

---

## ğŸ“ The Complete K-NN Algorithm - Super Simple

```
INPUT: 
- Training data (fruits you've seen)
- New fruit to classify
- K (number of neighbors)

STEP 1: Measure distance from new fruit to EVERY fruit in memory
        Use: âˆš[(differenceâ‚)Â² + (differenceâ‚‚)Â² + ...]

STEP 2: Sort all fruits by distance (closest first)

STEP 3: Pick the top K closest fruits

STEP 4: Count how many of each type
        Apples: 3
        Oranges: 1

STEP 5: Predict the class with most votes
        Answer: APPLE!
```

[Back to Top](#table-of-contents)

---

## ğŸ“Š Visualizing K=1 vs K=3 vs K=5

```
Training Data:
      ğŸ ğŸ
    ğŸ  ?  ğŸŠ
      ğŸ ğŸŠ ğŸŠ

K=1 (look at 1 nearest):
Nearest to ? is ğŸ (distance = 0.5)
Prediction: APPLE

K=3 (look at 3 nearest):
3 nearest: ğŸ, ğŸ, ğŸŠ
Votes: Apple=2, Orange=1
Prediction: APPLE

K=5 (look at 5 nearest):
5 nearest: ğŸ, ğŸ, ğŸ, ğŸŠ, ğŸŠ
Votes: Apple=3, Orange=2
Prediction: APPLE
```

[Back to Top](#table-of-contents)

---

## ğŸ”§ Feature Scaling - Why It Matters

### **The Problem:**

```
Fruit Features:
Weight: 50-200 grams
Color: 0-10 (red scale)

Fruit A: Weight=100g, Color=5
Fruit B: Weight=150g, Color=6
New:     Weight=110g, Color=5

Distance without scaling:
âˆš[(110-100)Â² + (5-5)Â²] = âˆš[100 + 0] = 10
âˆš[(110-150)Â² + (5-6)Â²] = âˆš[1600 + 1] = 40

Weight DOMINATES! Color barely matters!
```

### **The Solution: Make them equal**

```
Scale everything to 0-1:

Weight scaled = (weight - min) / (max - min)
              = (weight - 50) / (200 - 50)

Color scaled = (color - 0) / (10 - 0)

Now both features are 0-1 range!
```

[Back to Top](#table-of-contents)

---

## ğŸ§® Scaling Formula Explained

### **Min-Max Normalization:**

$$x_{scaled} = \frac{x - x_{min}}{x_{max} - x_{min}}$$

**In words:** "Squeeze everything between 0 and 1"

**Example:**
```
Weight values: [50, 100, 150, 200]
Min = 50, Max = 200

For weight = 100:
x_scaled = (100 - 50) / (200 - 50)
         = 50 / 150
         = 0.33

For weight = 200:
x_scaled = (200 - 50) / (200 - 50)
         = 150 / 150
         = 1.0

Result: [0, 0.33, 0.67, 1.0]
```

[Back to Top](#table-of-contents)

---

### **Standardization (Z-score):**

$$x_{scaled} = \frac{x - \mu}{\sigma}$$

Where:
- Î¼ (mu) = average (mean)
- Ïƒ (sigma) = spread (standard deviation)

**In words:** "Center around 0, with most values between -1 and 1"

**Example:**
```
Heights: [150, 160, 170, 180]

Î¼ (average) = (150+160+170+180)/4 = 165
Ïƒ (spread) = 11.2 (calculated using formula)

For height = 170:
x_scaled = (170 - 165) / 11.2
         = 5 / 11.2
         = 0.45

Result: [-1.34, -0.45, 0.45, 1.34]
```

[Back to Top](#table-of-contents)

---

## ğŸ¯ Choosing K - The Simple Rules

### **Visual Guide:**

```
K=1:  Very sensitive          K=too large: Too smooth
      â”Œâ”€â”                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”
    â—â”‚ â”‚â—                            â— â”‚       â”‚ â—
    â—â””â”€â”˜â—  â† Fits every noise          â”‚       â”‚
    â— â— â—                              â”‚   ?   â”‚
                                       â”‚       â”‚
                                     â— â””â”€â”€â”€â”€â”€â”€â”€â”˜ â—

K=3 to 7: JUST RIGHT! âœ“
      â”Œâ”€â”€â”€â”
    â—â”‚   â”‚â—
    â—â”‚ ? â”‚â—
    â—â””â”€â”€â”€â”˜â—
```

### **Simple Rules:**

```
âœ“ If dataset < 100 samples    â†’ Use K = 3-5
âœ“ If dataset 100-1000 samples â†’ Use K = 5-10  
âœ“ If dataset > 1000 samples   â†’ Use K = 10-20

âœ“ Always use ODD numbers (3, 5, 7...) to avoid ties
âœ“ Rule of thumb: K â‰ˆ âˆš(number of samples)

Example: 100 samples â†’ K â‰ˆ âˆš100 = 10
```

[Back to Top](#table-of-contents)

---

## ğŸ§ª Practice Problem - Step by Step

### **Problem:**

```
Training Data (2D points with classes):
Point 1: (1, 2) â†’ Class A
Point 2: (2, 3) â†’ Class A  
Point 3: (6, 5) â†’ Class B
Point 4: (7, 7) â†’ Class B
Point 5: (2, 2) â†’ Class A

New Point: (3, 4) â†’ ???
K = 3
```

---

### **Solution:**

**Step 1: Calculate distances**

```
Distance to Point 1:
âˆš[(3-1)Â² + (4-2)Â²] = âˆš[4 + 4] = âˆš8 = 2.83

Distance to Point 2:
âˆš[(3-2)Â² + (4-3)Â²] = âˆš[1 + 1] = âˆš2 = 1.41 âœ“

Distance to Point 3:
âˆš[(3-6)Â² + (4-5)Â²] = âˆš[9 + 1] = âˆš10 = 3.16

Distance to Point 4:
âˆš[(3-7)Â² + (4-7)Â²] = âˆš[16 + 9] = âˆš25 = 5.00

Distance to Point 5:
âˆš[(3-2)Â² + (4-2)Â²] = âˆš[1 + 4] = âˆš5 = 2.24 âœ“
```

**Step 2: Sort by distance (smallest first)**

```
1. Point 2: 1.41 (Class A) âœ“
2. Point 5: 2.24 (Class A) âœ“
3. Point 1: 2.83 (Class A) âœ“
4. Point 3: 3.16 (Class B)
5. Point 4: 5.00 (Class B)
```

**Step 3: Pick K=3 nearest**

```
Nearest 3:
Point 2 â†’ Class A
Point 5 â†’ Class A
Point 1 â†’ Class A
```

**Step 4: Count votes**

```
Class A: 3 votes âœ“
Class B: 0 votes

Winner: Class A!
```

**Answer: The new point (3, 4) is predicted to be Class A**

[Back to Top](#table-of-contents)

---

## âœ… Summary - The Big Picture

### **K-NN in 4 Steps:**

1. **Store** all training data (no learning!)
2. **Measure** distance to all stored points
3. **Pick** K closest points
4. **Vote** majority class wins

### **Key Formulas:**

```
Distance = âˆš[sum of (difference)Â²]

Prediction = class with most votes among K neighbors

Scaling = (value - min) / (max - min)
```

### **Remember:**

- K-NN is **lazy** (doesn't learn, just memorizes)
- **Distance** = how similar two things are
- **K** = how many neighbors vote
- **Scaling** = make features equal importance

[Back to Top](#table-of-contents)


---
**Created for easy learning and quick reference!** â­
---
