# K-Nearest Neighbors (K-NN) Algorithm - Complete Guide

## ğŸ“š Table of Contents

1. [Introduction](#introduction)
2. [Core Concepts](#core-concepts)
3. [Mathematical Foundation](#mathematical-foundation)
4. [Data Representation](#data-representation)
5. [Distance Metrics](#distance-metrics)
6. [Algorithm Variants](#algorithm-variants)
7. [Implementation Guide](#implementation-guide)
8. [Visualizations & Diagrams](#visualizations--diagrams)
9. [Cheat Sheet](#cheat-sheet)
10. [Mnemonics & Memory Aids](#mnemonics--memory-aids)
11. [Practical Examples](#practical-examples)
12. [Best Practices](#best-practices)
13. [Common Pitfalls](#common-pitfalls)

---

## ğŸ¯ Introduction

**K-Nearest Neighbors (K-NN)** is a simple, intuitive, and powerful supervised machine learning algorithm used for both **classification** and **regression** tasks.

### Key Characteristics:
- **Instance-based learning**: No explicit training phase
- **Non-parametric**: Makes no assumptions about data distribution
- **Lazy learning**: Computation happens during prediction
- **Memory-based**: Stores all training data

---

## ğŸ§  Core Concepts

### 1. **Distance-Based Learning**

K-NN operates on the principle: **"Similar things are close to each other"**

```
If it walks like a duck, quacks like a duck, and looks like a duck,
it's probably a duck!
```

### 2. **Feature Vector Representation**

Each data point is represented as a vector in D-dimensional space:

**Mathematical Notation:**
- **x_n** âˆˆ â„^(DÃ—1) = feature vector (point in D-dimensional space)
- **D** = number of features/dimensions
- **N** = number of training examples

**Example:**
```
A 7Ã—7 image â†’ 49Ã—1 vector of pixel intensities
```

### 3. **Training Data Structure**

**Feature Matrix (X):**
```
X = {xâ‚, xâ‚‚, ..., x_N}  (N Ã— D matrix)
```
- **Rows**: Individual examples (n = 1, 2, ..., N)
- **Columns**: Features (d = 1, 2, ..., D)

**Label Vector (y):**
```
y = {yâ‚, yâ‚‚, ..., y_N}  (N Ã— 1 vector)
```
- **y_n**: Label/response for the n-th example
- Can be categorical (classification) or continuous (regression)

---

## ğŸ“ Mathematical Foundation

### Distance Metrics

Distance metrics measure similarity between data points.

#### 1. **Euclidean Distance (Lâ‚‚ norm)**

**Formula:**
```
d(x_n, x_m) = ||x_n - x_m|| = âˆš[(x_n - x_m)áµ€(x_n - x_m)]
            = âˆš[Î£(x_nd - x_md)Â²] for d=1 to D
```

**Components:**
- `||Â·||`: Lâ‚‚ norm (Euclidean norm)
- `x_n, x_m`: Two feature vectors
- `x_nd`: d-th feature of n-th example
- `âˆš`: Square root
- `Î£`: Summation over all D features

**Visual Representation:**
```
Point A (xâ‚, yâ‚) â”€â”€â”
                   â”‚  Distance = âˆš[(xâ‚‚-xâ‚)Â² + (yâ‚‚-yâ‚)Â²]
Point B (xâ‚‚, yâ‚‚) â”€â”€â”˜
```

#### 2. **Manhattan Distance (Lâ‚ norm)**

**Formula:**
```
dâ‚(x_n, x_m) = ||x_n - x_m||â‚ = Î£|x_nd - x_md| for d=1 to D
```

**When to use:** Grid-based movements, high-dimensional data

#### 3. **Inner Product Similarity**

**Formula:**
```
s(x_n, x_m) = x_náµ€ x_m = Î£(x_nd Ã— x_md) for d=1 to D
```

**For unit vectors:**
```
s(x_n, x_m) = cos(Î¸)  where Î¸ is angle between vectors
```

**Components:**
- `áµ€`: Transpose operator
- `Â·`: Dot product
- Higher value â†’ More similar

#### 4. **Mahalanobis Distance**

**Formula:**
```
d_M(x_n, x_m) = âˆš[(x_n - x_m)áµ€ M (x_n - x_m)]
```

**Where:**
- `M`: Positive semi-definite matrix (learned from data)
- Accounts for correlations between features
- Generalizes Euclidean distance

---

## ğŸ“Š Data Representation

### Vector Space Model

Each data point exists as a point in D-dimensional space:

```
x_n âˆˆ â„^(DÃ—1)  â†’  [x_n1]
                    [x_n2]
                    [ : ]
                    [x_nD]
```

### Feature Matrix Layout

```
        Feature 1  Feature 2  ...  Feature D
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
Ex 1   â”‚  xâ‚â‚    â”‚   xâ‚â‚‚    â”‚...â”‚   xâ‚D    â”‚
Ex 2   â”‚  xâ‚‚â‚    â”‚   xâ‚‚â‚‚    â”‚...â”‚   xâ‚‚D    â”‚
  :    â”‚   :     â”‚    :     â”‚...â”‚    :     â”‚
Ex N   â”‚  x_N1   â”‚   x_N2   â”‚...â”‚   x_ND   â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Supervised Learning Setup

**Input-Output Pairs:**
```
{(xâ‚, yâ‚), (xâ‚‚, yâ‚‚), ..., (x_N, y_N)}
```

**Types of Outputs:**
- **Classification**: y_n âˆˆ {classâ‚, classâ‚‚, ...}
- **Regression**: y_n âˆˆ â„ (continuous values)

---

## ğŸ¯ Algorithm Variants

### 1. Distance from Means Classifier

**Concept:** Classify based on distance to class centroids

**Algorithm:**
```
1. Compute mean for each class:
   Î¼â‚Š = (1/Nâ‚Š) Î£ x_n  (for positive class)
   Î¼â‚‹ = (1/Nâ‚‹) Î£ x_n  (for negative class)

2. For new point x:
   Distance to Î¼â‚Š: ||Î¼â‚Š - x||Â²
   Distance to Î¼â‚‹: ||Î¼â‚‹ - x||Â²

3. Decision rule:
   f(x) = ||Î¼â‚‹ - x||Â² - ||Î¼â‚Š - x||Â²
   
   If f(x) > 0 â†’ Positive class
   If f(x) < 0 â†’ Negative class
```

**Expanded Form:**
```
f(x) = 2âŸ¨Î¼â‚Š - Î¼â‚‹, xâŸ© + ||Î¼â‚‹||Â² - ||Î¼â‚Š||Â²
       â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
          w (weight)      b (bias)
```

**Hyperplane Equation:**
```
w = Î¼â‚Š - Î¼â‚‹  (direction normal to hyperplane)
b = ||Î¼â‚‹||Â² - ||Î¼â‚Š||Â²  (bias term)
```

**Key Properties:**
- Simple and efficient
- Requires sufficient training data per class
- Only learns linear decision boundaries
- Can be extended with nonlinear distance functions

### 2. 1-Nearest Neighbor (1-NN)

**Algorithm:**
```
For test point x:
1. Compute distance to all training points
2. Find nearest neighbor: x_nearest
3. Assign label of x_nearest to x
```

**Prediction Rule:**
```
Å· = y_nearest  where  x_nearest = argmin d(x, x_n)
                                   nâˆˆ{1,...,N}
```

**Properties:**
- Most sensitive to noise/outliers
- Creates Voronoi tessellation
- Decision boundary: perpendicular bisectors between points

### 3. K-Nearest Neighbors (K-NN)

**Algorithm:**
```
For test point x:
1. Compute distances to all training points
2. Select K nearest neighbors
3. For classification: Majority vote
   For regression: Average of labels
```

**Classification Rule:**
```
Å· = mode{y_n : x_n âˆˆ N_K(x)}

where N_K(x) = set of K nearest neighbors
```

**Regression Rule:**
```
Å· = (1/K) Î£ y_n  for x_n âˆˆ N_K(x)
```

**Choosing K:**
- **K = 1**: High variance, low bias
- **Large K**: Low variance, high bias
- **Odd K**: Avoids ties in binary classification
- **Selection method**: Cross-validation

---

## ğŸ’» Implementation Guide

### Step-by-Step K-NN Algorithm

```python
# Pseudocode for K-NN Classification

function knn_predict(X_train, y_train, x_test, K):
    """
    Parameters:
    -----------
    X_train : array of shape (N, D) - training features
    y_train : array of shape (N,) - training labels
    x_test : array of shape (D,) - test point
    K : int - number of neighbors
    
    Returns:
    --------
    prediction : predicted class label
    """
    
    # Step 1: Compute distances
    distances = []
    for i in range(N):
        dist = euclidean_distance(x_test, X_train[i])
        distances.append((dist, y_train[i]))
    
    # Step 2: Sort by distance
    distances.sort(key=lambda x: x[0])
    
    # Step 3: Select K nearest neighbors
    k_nearest = distances[:K]
    
    # Step 4: Majority vote (classification)
    labels = [label for (_, label) in k_nearest]
    prediction = most_common(labels)
    
    return prediction
```

### Distance Calculation Functions

```python
import numpy as np

def euclidean_distance(x1, x2):
    """
    Compute Euclidean distance between two vectors
    
    Formula: d = âˆš[Î£(x1_i - x2_i)Â²]
    """
    return np.sqrt(np.sum((x1 - x2) ** 2))

def manhattan_distance(x1, x2):
    """
    Compute Manhattan distance between two vectors
    
    Formula: d = Î£|x1_i - x2_i|
    """
    return np.sum(np.abs(x1 - x2))

def mahalanobis_distance(x1, x2, M):
    """
    Compute Mahalanobis distance
    
    Formula: d = âˆš[(x1-x2)áµ€ M (x1-x2)]
    """
    diff = x1 - x2
    return np.sqrt(diff.T @ M @ diff)
```

---

## ğŸ¨ Visualizations & Diagrams

### 1. Voronoi Tessellation (1-NN)

```
         Red Region  |  Blue Region
                    |
        â—           |        â—
    (red point)     |    (blue point)
                    |
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                    |
        â—           |        â—
                    |
```

### 2. Decision Boundaries for Different K

```
K=1: Jagged, complex boundary
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â—â—  â–²â–²â–²    â—â—   â–²â–²   â”‚
â”‚  â—â–²â–²â–²  â–²â–² â—  â—â–²â–²  â–²  â”‚
â”‚ â—â— â–²â–²â–²â–²  â— â—â—  â–²â–²â–²   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

K=5: Smoother boundary
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â—â—â—â—     â–²â–²â–²â–²â–²â–²â–²â–²â–²   â”‚
â”‚ â—â—â—â—     â–²â–²â–²â–²â–²â–²â–²â–²â–²   â”‚
â”‚ â—â—â—â—     â–²â–²â–²â–²â–²â–²â–²â–²â–²   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3. Effect of K on Prediction

```
Test Point: â˜…

K=1:  Find 1 nearest â†’ Class A
      â˜…â”€â”€â”€â†’ â— (A)

K=3:  Find 3 nearest â†’ Majority vote
      â˜…â”€â”€â”€â†’ â— (A)
       â”œâ”€â”€â”€â†’ â— (A)
       â””â”€â”€â”€â†’ â–² (B)
      Result: Class A (2 votes)

K=5:  Find 5 nearest â†’ Majority vote
      â˜…â”€â”€â”€â†’ â— (A)
       â”œâ”€â”€â”€â†’ â— (A)
       â”œâ”€â”€â”€â†’ â–² (B)
       â”œâ”€â”€â”€â†’ â–² (B)
       â””â”€â”€â”€â†’ â–² (B)
      Result: Class B (3 votes)
```

---

## ğŸ“ Cheat Sheet

### Quick Reference

| Aspect | Details |
|--------|---------|
| **Type** | Supervised Learning |
| **Tasks** | Classification, Regression |
| **Training** | None (lazy learning) |
| **Prediction Time** | O(ND) per prediction |
| **Space Complexity** | O(ND) - stores all data |
| **Pros** | Simple, no assumptions, versatile |
| **Cons** | Slow prediction, memory-intensive |

### Common Parameters

| Parameter | Symbol | Typical Values | Notes |
|-----------|--------|----------------|-------|
| Number of neighbors | K | 3, 5, 7, 9 | Use odd for binary class |
| Distance metric | d(Â·,Â·) | Euclidean, Manhattan | Match to data type |
| Weights | uniform/distance | uniform | Distance-weighted for better results |

### Distance Metrics Quick Guide

| Metric | Formula | Use Case |
|--------|---------|----------|
| Euclidean | âˆš[Î£(xáµ¢-yáµ¢)Â²] | Default, continuous features   |
| Manhattan | Î£\|xáµ¢-yáµ¢\| | High dimensions, grid paths      |
| Mahalanobis | âˆš[(x-y)áµ€M(x-y)] | Correlated features       |
| Cosine | 1 - (xÂ·y)/(||x|| ||y||) | Text, high-dimensional |

### Notation Reference

| Symbol | Meaning |
|--------|---------|
| x_n | n-th training example (feature vector) |
| y_n | Label for n-th example |
| N | Total number of training examples |
| D | Number of features/dimensions |
| K | Number of neighbors |
| d(Â·,Â·) | Distance function |
| ||Â·|| | Euclidean norm (Lâ‚‚) |
| ||Â·||â‚ | Manhattan norm (Lâ‚) |
| âŸ¨Â·,Â·âŸ© | Inner product |
| áµ€ | Transpose |
| âˆˆ | Element of / belongs to |
| â„ | Real numbers |
| Î£ | Summation |
| Î¼ | Mean/centroid |

---

## ğŸ§  Mnemonics & Memory Aids

### K-NN Core Principle
```
"K-NN = K Nearest Neighbors"
K - Keep
N - Neighbors
N - Near
```

### Algorithm Steps (D-S-S-V)
```
D - Distance: Calculate distances to all points
S - Sort: Order by distance (ascending)
S - Select: Pick K nearest neighbors
V - Vote: Majority vote (classification) or Average (regression)
```

### Distance Metrics (E-M-C-M)
```
E - Euclidean: "As the crow flies" (straight line)
M - Manhattan: "Taxi cab" distance (grid)
C - Cosine: Angle between vectors
M - Mahalanobis: Scaled Euclidean (accounts for correlations)
```

### Choosing K
```
"K Should Be:
O - Odd (for binary classification)
D - Determined by cross-validation
D - âˆšN is a good starting point"
```

### Bias-Variance Trade-off
```
Small K â†’ High Variance (overfitting)
  "Small = Sensitive"

Large K â†’ High Bias (underfitting)
  "Large = Smooth"
```

---

## ğŸ” Practical Examples

### Example 1: Binary Classification

**Problem:** Classify a new point based on 2 features

**Training Data:**
```
Class A (â—): (1, 2), (2, 3), (3, 3)
Class B (â–²): (6, 5), (7, 7), (8, 6)
Test Point (â˜…): (4, 4)
```

**Solution with K=3:**

1. **Calculate distances (Euclidean):**
   ```
   d(â˜…, â—â‚) = âˆš[(4-1)Â² + (4-2)Â²] = âˆš13 â‰ˆ 3.61
   d(â˜…, â—â‚‚) = âˆš[(4-2)Â² + (4-3)Â²] = âˆš5 â‰ˆ 2.24
   d(â˜…, â—â‚ƒ) = âˆš[(4-3)Â² + (4-3)Â²] = âˆš2 â‰ˆ 1.41
   d(â˜…, â–²â‚) = âˆš[(4-6)Â² + (4-5)Â²] = âˆš5 â‰ˆ 2.24
   d(â˜…, â–²â‚‚) = âˆš[(4-7)Â² + (4-7)Â²] = âˆš18 â‰ˆ 4.24
   d(â˜…, â–²â‚ƒ) = âˆš[(4-8)Â² + (4-6)Â²] = âˆš20 â‰ˆ 4.47
   ```

2. **Sort by distance:**
   ```
   1. â—â‚ƒ â†’ 1.41
   2. â—â‚‚ â†’ 2.24
   3. â–²â‚ â†’ 2.24
   4. â—â‚ â†’ 3.61
   5. â–²â‚‚ â†’ 4.24
   6. â–²â‚ƒ â†’ 4.47
   ```

3. **Select K=3 nearest:**
   ```
   â—â‚ƒ, â—â‚‚, â–²â‚
   ```

4. **Majority vote:**
   ```
   Class A: 2 votes
   Class B: 1 vote
   
   Prediction: Class A (â—)
   ```

### Example 2: Regression

**Problem:** Predict house price based on area

**Training Data:**
```
Area (sqft) | Price ($k)
------------|----------
1000        | 200
1200        | 240
1500        | 300
1800        | 350
2000        | 400

Test: Area = 1400 sqft, K = 3
```

**Solution:**

1. **Calculate distances:**
   ```
   |1400 - 1000| = 400
   |1400 - 1200| = 200  â† 2nd nearest
   |1400 - 1500| = 100  â† 1st nearest
   |1400 - 1800| = 400
   |1400 - 2000| = 600  â† 3rd nearest
   ```

2. **K=3 nearest neighbors:**
   ```
   1500 sqft â†’ $300k
   1200 sqft â†’ $240k
   2000 sqft â†’ $400k
   ```

3. **Average:**
   ```
   Prediction = (300 + 240 + 400) / 3 = $313.33k
   ```

---

## âœ… Best Practices

### 1. Feature Scaling
```python
# Always normalize features before using K-NN
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
```

**Why?** Features on different scales dominate distance calculations

### 2. Choosing K

```python
# Use cross-validation
from sklearn.model_selection import cross_val_score

k_values = range(1, 31)
cv_scores = []

for k in k_values:
    knn = KNeighborsClassifier(n_neighbors=k)
    scores = cross_val_score(knn, X_train, y_train, cv=5)
    cv_scores.append(scores.mean())

optimal_k = k_values[np.argmax(cv_scores)]
```

### 3. Distance Metric Selection

| Data Type | Recommended Metric |
|-----------|-------------------|
| Continuous, similar scales | Euclidean |
| Mixed scales | Mahalanobis |
| Binary/categorical | Hamming |
| Text data | Cosine |
| High dimensional | Manhattan |

### 4. Handling Imbalanced Data

```python
# Use weighted K-NN
knn = KNeighborsClassifier(n_neighbors=5, weights='distance')
```

### 5. Optimization Techniques

**For large datasets:**
- Use KD-Trees or Ball Trees
- Apply dimensionality reduction (PCA)
- Use approximate nearest neighbor algorithms

```python
# Using KD-Tree
knn = KNeighborsClassifier(n_neighbors=5, algorithm='kd_tree')
```

---

## âš ï¸ Common Pitfalls

### 1. Curse of Dimensionality

**Problem:** K-NN performance degrades in high dimensions

**Why?**
```
In high dimensions:
- All points become equidistant
- Nearest neighbors aren't actually "near"
```

**Solution:**
- Dimensionality reduction (PCA, t-SNE)
- Feature selection
- Use alternative algorithms for D > 20

### 2. No Feature Scaling

**Problem:**
```
Feature 1: [0, 1]      (normalized)
Feature 2: [0, 10000]  (not normalized)

Distance dominated by Feature 2!
```

**Solution:** Always scale features

### 3. Even K in Binary Classification

**Problem:** Ties in voting

```
K=4, Classes: A, A, B, B â†’ Tie!
```

**Solution:** Use odd K

### 4. Outliers

**Impact:** 1-NN highly sensitive to noise

```
True pattern:  â—â—â—â—â—   â–²â–²â–²â–²â–²
Outlier:       â—â—â—â—‹â—   â–²â–²â–²â–²â–²
                    â†‘
               Misclassifies nearby points
```

**Solutions:**
- Use larger K
- Remove outliers during preprocessing
- Use robust distance metrics

### 5. Not Considering Computational Cost

**Problem:**
```
Prediction time: O(ND) per query
Large N, D â†’ Very slow!
```

**Solutions:**
- Use indexing structures (KD-Tree)
- Consider approximate methods
- For real-time systems, use alternative algorithms

---

## ğŸ“Š Performance Metrics

### Classification Metrics

```python
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Accuracy
accuracy = accuracy_score(y_true, y_pred)

# Precision
precision = precision_score(y_true, y_pred, average='weighted')

# Recall
recall = recall_score(y_true, y_pred, average='weighted')

# F1-Score
f1 = f1_score(y_true, y_pred, average='weighted')
```

### Regression Metrics

```python
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# MSE
mse = mean_squared_error(y_true, y_pred)

# MAE
mae = mean_absolute_error(y_true, y_pred)

# RÂ² Score
r2 = r2_score(y_true, y_pred)
```

---

## ğŸ“ Advanced Topics

### 1. Weighted K-NN

```
Instead of equal votes, weight by inverse distance:

w_i = 1 / d(x_test, x_i)

Closer neighbors have more influence
```

### 2. Locally Weighted Learning

```
Fit local model at prediction time using nearby points
```

### 3. Adaptive K

```
Use different K values for different regions of space
Dense regions â†’ larger K
Sparse regions â†’ smaller K
```

---

## ğŸ“š Additional Resources

### Key Papers
1. Cover & Hart (1967) - Original K-NN paper
2. Fix & Hodges (1951) - Nearest neighbor classification

### Libraries
```python
# scikit-learn
from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor

# For large scale
import faiss  # Facebook AI Similarity Search
```

### Complexity Analysis

| Operation | Time Complexity | Space Complexity |
|-----------|----------------|------------------|
| Training | O(1) | O(ND) |
| Prediction (brute force) | O(ND) | O(1) |
| Prediction (KD-Tree) | O(D log N) | O(ND) |
| Prediction (Ball Tree) | O(D log N) | O(ND) |

---

## ğŸ¯ Summary

K-NN is a powerful, intuitive algorithm that:
- âœ… Requires no training phase
- âœ… Works for classification and regression
- âœ… Handles multi-class problems naturally
- âœ… Can capture complex decision boundaries
- âŒ Slow at prediction time
- âŒ Requires lots of memory
- âŒ Sensitive to irrelevant features
- âŒ Needs careful feature scaling

**Best Use Cases:**
- Small to medium datasets
- Low to moderate dimensions
- When interpretability matters
- As a baseline model

---
