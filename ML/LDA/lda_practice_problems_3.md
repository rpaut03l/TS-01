# ğŸ“ LDA Practice Problems with Detailed Solutions

> **10 Carefully Crafted Problems** covering all aspects of Linear Discriminant Analysis
> 
> Solutions are hidden in collapsible sections â€” try solving first!

---

## ğŸ“‹ Table of Contents

| # | Problem | Difficulty | Topics Covered |
|---|---------|------------|----------------|
| 1 | [Basic 2D LDA Projection](#problem-1-basic-2d-lda-projection) | â­ Easy | Class means, projection, classification |
| 2 | [Within-Class Scatter Matrix](#problem-2-within-class-scatter-matrix) | â­â­ Medium | Scatter matrix computation, matrix operations |
| 3 | [Complete LDA from Scratch](#problem-3-complete-lda-from-scratch) | â­â­â­ Hard | All 6 steps, matrix inverse, classification |
| 4 | [LDA vs PCA Conceptual](#problem-4-lda-vs-pca-conceptual) | â­ Easy | Understanding differences, when to use which |
| 5 | [3-Class LDA Axes](#problem-5-3-class-lda-axes) | â­â­ Medium | Number of axes, eigenvalues, multi-class |
| 6 | [Fisher's Criterion Calculation](#problem-6-fishers-criterion-calculation) | â­â­ Medium | Ratio computation, interpretation |
| 7 | [2Ã—2 Matrix Inverse](#problem-7-2x2-matrix-inverse-shortcut) | â­ Easy | Matrix algebra, determinant |
| 8 | [Between-Class Scatter](#problem-8-between-class-scatter-matrix) | â­â­ Medium | SÊ™ computation, outer product |
| 9 | [Projection and Classification](#problem-9-projection-and-classification) | â­â­â­ Hard | Complete workflow, decision boundary |
| 10 | [Assumption Violation](#problem-10-assumption-violation-qda) | â­â­ Medium | When LDA fails, QDA alternative |

---

## Problem 1: Basic 2D LDA Projection

### Question

You have already computed the optimal LDA direction vector:

```
w* = [0.6, 0.8]
```

And the projected class means are:
```
mÌƒâ‚ = 2.0   (Class 1)
mÌƒâ‚‚ = 8.0   (Class 2)
```

**Tasks:**
1. Project the test point `x = [3, 4]` onto the LDA axis
2. Compute the decision threshold
3. Classify the test point

<details>
<summary><b>ğŸ‘‰ Click to reveal solution</b></summary>

---

### Solution

#### **Step 1: Project the test point**

**Formula Used:**
```
y = wáµ€ x    (dot product / projection formula)
```

**Calculation:**
```
x = [3, 4]
w* = [0.6, 0.8]

y = wáµ€ x
  = (0.6 Ã— 3) + (0.8 Ã— 4)
  = 1.8 + 3.2
  = 5.0
```

**Result:** The projected value of the test point is **y = 5.0**

---

#### **Step 2: Compute decision threshold**

**Formula Used:**
```
threshold t = (mÌƒâ‚ + mÌƒâ‚‚) / 2
```

**Why this formula?**
> The decision boundary is placed at the midpoint between the two projected class means.

**Calculation:**
```
mÌƒâ‚ = 2.0
mÌƒâ‚‚ = 8.0

t = (2.0 + 8.0) / 2
  = 10.0 / 2
  = 5.0
```

**Result:** Threshold **t = 5.0**

---

#### **Step 3: Classify the test point**

**Decision Rule:**
```
If y â‰¥ t  â†’  Class 2
If y < t  â†’  Class 1
```

**Comparison:**
```
y = 5.0
t = 5.0

y â‰¥ t  â†’  5.0 â‰¥ 5.0  âœ“ TRUE
```

**Result:** Classified as **Class 2**

---

**Visual Representation:**
```
1D Projected Line:

  2.0         5.0         5.0         8.0
   |           |           |           |
   â†“           â†“           â†“           â†“
  mÌƒâ‚      threshold    test pt       mÌƒâ‚‚
(Class 1)     (t)         (y)      (Class 2)

Since y is at the threshold (boundary case), 
and our rule uses â‰¥, it goes to Class 2.
```

---

**Key Concepts:**
- **Projection:** Reducing multi-dimensional point to 1D value
- **Dot Product:** wáµ€x = wâ‚xâ‚ + wâ‚‚xâ‚‚
- **Threshold:** Midpoint decision boundary
- **Classification:** Compare projected value to threshold

---

</details>

---

## Problem 2: Within-Class Scatter Matrix

### Question

Given the following data:

**Class 1 (Green):**
```
xâ‚ = [2, 1]
xâ‚‚ = [3, 2]
```

**Class mean already computed:**
```
Î¼â‚ = [2.5, 1.5]
```

**Task:** Compute the within-class scatter matrix Sâ‚ for Class 1.

<details>
<summary><b>ğŸ‘‰ Click to reveal solution</b></summary>

---

### Solution

#### **Formula Used:**
```
Sâ‚– = Î£ (xâ‚™ âˆ’ Î¼â‚–)(xâ‚™ âˆ’ Î¼â‚–)áµ€
    nâˆˆCâ‚–

For each point in class k:
  1. Compute deviation: xâ‚™ âˆ’ Î¼â‚–
  2. Compute outer product: (xâ‚™ âˆ’ Î¼â‚–)(xâ‚™ âˆ’ Î¼â‚–)áµ€
  3. Sum all outer products
```

---

#### **Step 1: Compute deviations**

**For point xâ‚:**
```
xâ‚ = [2, 1]
Î¼â‚ = [2.5, 1.5]

diffâ‚ = xâ‚ âˆ’ Î¼â‚
      = [2 âˆ’ 2.5, 1 âˆ’ 1.5]
      = [âˆ’0.5, âˆ’0.5]
```

**For point xâ‚‚:**
```
xâ‚‚ = [3, 2]
Î¼â‚ = [2.5, 1.5]

diffâ‚‚ = xâ‚‚ âˆ’ Î¼â‚
      = [3 âˆ’ 2.5, 2 âˆ’ 1.5]
      = [0.5, 0.5]
```

---

#### **Step 2: Compute outer products**

**What is an outer product?**
> For a column vector v, the outer product v Ã— váµ€ produces a matrix.
> 
> Example: [a] Ã— [a, b] = [aÃ—a  aÃ—b]
>          [b]             [bÃ—a  bÃ—b]

**For diffâ‚:**
```
diffâ‚ = [âˆ’0.5, âˆ’0.5]

As column vector: [âˆ’0.5]
                  [âˆ’0.5]

diffâ‚ Ã— diffâ‚áµ€ = [âˆ’0.5] Ã— [âˆ’0.5, âˆ’0.5]
                  [âˆ’0.5]

                = [âˆ’0.5Ã—(âˆ’0.5)   âˆ’0.5Ã—(âˆ’0.5)]
                  [âˆ’0.5Ã—(âˆ’0.5)   âˆ’0.5Ã—(âˆ’0.5)]

                = [0.25  0.25]
                  [0.25  0.25]
```

**For diffâ‚‚:**
```
diffâ‚‚ = [0.5, 0.5]

diffâ‚‚ Ã— diffâ‚‚áµ€ = [0.5] Ã— [0.5, 0.5]
                  [0.5]

                = [0.5Ã—0.5   0.5Ã—0.5]
                  [0.5Ã—0.5   0.5Ã—0.5]

                = [0.25  0.25]
                  [0.25  0.25]
```

---

#### **Step 3: Sum the outer products**

```
Sâ‚ = diffâ‚Ã—diffâ‚áµ€ + diffâ‚‚Ã—diffâ‚‚áµ€

   = [0.25  0.25] + [0.25  0.25]
     [0.25  0.25]   [0.25  0.25]

   = [0.25+0.25    0.25+0.25]
     [0.25+0.25    0.25+0.25]

   = [0.5  0.5]
     [0.5  0.5]
```

---

#### **Final Answer:**
```
Sâ‚ = [0.5  0.5]
     [0.5  0.5]
```

---

**Interpretation:**
- **Diagonal elements (0.5, 0.5):** Variance in each dimension
- **Off-diagonal elements (0.5):** Covariance between dimensions
- **Symmetry:** Scatter matrices are always symmetric (Sáµ€ = S)

---

**Key Formulas Recap:**
1. **Deviation:** diff = xâ‚™ âˆ’ Î¼â‚–
2. **Outer product:** diff Ã— diffáµ€ (column Ã— row = matrix)
3. **Scatter matrix:** Sum of all outer products
4. **Properties:** Always square, always symmetric

---

</details>

---

## Problem 3: Complete LDA from Scratch

### Question

**Training Data:**

Class 1 (Setosa):
```
xâ‚ = [1, 2]
xâ‚‚ = [2, 3]
```

Class 2 (Versicolor):
```
xâ‚ƒ = [6, 5]
xâ‚„ = [7, 6]
```

**Test point:**
```
x_test = [4, 3]
```

**Tasks:**
1. Compute class means Î¼â‚ and Î¼â‚‚
2. Compute within-class scatter matrix Sáµ‚
3. Compute between-class scatter matrix SÊ™
4. Find optimal direction w* = Sáµ‚â»Â¹(Î¼â‚‚ âˆ’ Î¼â‚)
5. Project test point and classify it

<details>
<summary><b>ğŸ‘‰ Click to reveal solution</b></summary>

---

### Solution

---

#### **STEP 1: Compute Class Means**

**Formula:**
```
Î¼â‚– = (1/Nâ‚–) Î£ xâ‚™
```

**For Class 1 (Setosa):**
```
xâ‚ = [1, 2]
xâ‚‚ = [2, 3]
Nâ‚ = 2

Î¼â‚ = (xâ‚ + xâ‚‚) / 2
   = ([1,2] + [2,3]) / 2
   = [3, 5] / 2
   = [1.5, 2.5]
```

**For Class 2 (Versicolor):**
```
xâ‚ƒ = [6, 5]
xâ‚„ = [7, 6]
Nâ‚‚ = 2

Î¼â‚‚ = (xâ‚ƒ + xâ‚„) / 2
   = ([6,5] + [7,6]) / 2
   = [13, 11] / 2
   = [6.5, 5.5]
```

**Result:** 
```
Î¼â‚ = [1.5, 2.5]   â† Setosa center
Î¼â‚‚ = [6.5, 5.5]   â† Versicolor center
```

---

#### **STEP 2: Compute Within-Class Scatter Sáµ‚**

**Formula:**
```
Sáµ‚ = Sâ‚ + Sâ‚‚
Sâ‚– = Î£ (xâ‚™ âˆ’ Î¼â‚–)(xâ‚™ âˆ’ Î¼â‚–)áµ€
```

**For Class 1:**

Point xâ‚:
```
diffâ‚ = xâ‚ âˆ’ Î¼â‚ = [1,2] âˆ’ [1.5,2.5] = [âˆ’0.5, âˆ’0.5]

diffâ‚ Ã— diffâ‚áµ€ = [âˆ’0.5] Ã— [âˆ’0.5, âˆ’0.5]
                  [âˆ’0.5]

                = [0.25  0.25]
                  [0.25  0.25]
```

Point xâ‚‚:
```
diffâ‚‚ = xâ‚‚ âˆ’ Î¼â‚ = [2,3] âˆ’ [1.5,2.5] = [0.5, 0.5]

diffâ‚‚ Ã— diffâ‚‚áµ€ = [0.5] Ã— [0.5, 0.5]
                  [0.5]

                = [0.25  0.25]
                  [0.25  0.25]
```

Sum for Sâ‚:
```
Sâ‚ = [0.25  0.25] + [0.25  0.25]
     [0.25  0.25]   [0.25  0.25]

   = [0.5  0.5]
     [0.5  0.5]
```

**For Class 2:**

Point xâ‚ƒ:
```
diffâ‚ƒ = xâ‚ƒ âˆ’ Î¼â‚‚ = [6,5] âˆ’ [6.5,5.5] = [âˆ’0.5, âˆ’0.5]

diffâ‚ƒ Ã— diffâ‚ƒáµ€ = [0.25  0.25]
                  [0.25  0.25]
```

Point xâ‚„:
```
diffâ‚„ = xâ‚„ âˆ’ Î¼â‚‚ = [7,6] âˆ’ [6.5,5.5] = [0.5, 0.5]

diffâ‚„ Ã— diffâ‚„áµ€ = [0.25  0.25]
                  [0.25  0.25]
```

Sum for Sâ‚‚:
```
Sâ‚‚ = [0.25  0.25] + [0.25  0.25]
     [0.25  0.25]   [0.25  0.25]

   = [0.5  0.5]
     [0.5  0.5]
```

**Total Within-Class Scatter:**
```
Sáµ‚ = Sâ‚ + Sâ‚‚

   = [0.5  0.5] + [0.5  0.5]
     [0.5  0.5]   [0.5  0.5]

   = [1.0  1.0]
     [1.0  1.0]
```

---

#### **STEP 3: Compute Between-Class Scatter SÊ™**

**Formula:**
```
SÊ™ = (Î¼â‚‚ âˆ’ Î¼â‚)(Î¼â‚‚ âˆ’ Î¼â‚)áµ€
```

**Calculate mean difference:**
```
Î¼â‚‚ âˆ’ Î¼â‚ = [6.5, 5.5] âˆ’ [1.5, 2.5]
        = [5.0, 3.0]
```

**Compute outer product:**
```
SÊ™ = [5.0] Ã— [5.0, 3.0]
     [3.0]

   = [5.0Ã—5.0   5.0Ã—3.0]
     [3.0Ã—5.0   3.0Ã—3.0]

   = [25.0  15.0]
     [15.0   9.0]
```

---

#### **STEP 4: Find Optimal Direction w***

**Formula:**
```
w* = Sáµ‚â»Â¹ (Î¼â‚‚ âˆ’ Î¼â‚)
```

**Need to compute Sáµ‚â»Â¹:**

For 2Ã—2 matrix:
```
A = [a  b]     Aâ»Â¹ = â”€â”€â”€1â”€â”€â”€  Ã— [ d  âˆ’b]
    [c  d]           ad âˆ’ bc     [âˆ’c   a]
```

Given:
```
Sáµ‚ = [1.0  1.0]
     [1.0  1.0]

a = 1.0,  b = 1.0,  c = 1.0,  d = 1.0

det(Sáµ‚) = ad âˆ’ bc
        = (1.0 Ã— 1.0) âˆ’ (1.0 Ã— 1.0)
        = 1.0 âˆ’ 1.0
        = 0
```

**âš ï¸ PROBLEM:** Determinant is 0! Matrix is **singular** (non-invertible).

**Why?**
> The scatter matrix has rank 1 (both rows are identical).
> This happens when data points lie on a line.

**Solution approach:**
> In practice, we'd add regularization: Sáµ‚ + Î»I where Î» is small (e.g., 0.01)

**Let's add regularization:**
```
Î» = 0.01

Sáµ‚_reg = [1.0  1.0] + [0.01  0   ]
         [1.0  1.0]   [0     0.01]

       = [1.01  1.00]
         [1.00  1.01]

det(Sáµ‚_reg) = (1.01 Ã— 1.01) âˆ’ (1.00 Ã— 1.00)
             = 1.0201 âˆ’ 1.0000
             = 0.0201

Sáµ‚_regâ»Â¹ = â”€â”€1â”€â”€â”€â”€ Ã— [ 1.01  âˆ’1.00]
            0.0201    [âˆ’1.00   1.01]

         = 49.75 Ã— [ 1.01  âˆ’1.00]
                    [âˆ’1.00   1.01]

         = [ 50.25  âˆ’49.75]
           [âˆ’49.75   50.25]
```

**Now compute w*:**
```
Î¼â‚‚ âˆ’ Î¼â‚ = [5.0, 3.0]

w* = Sáµ‚_regâ»Â¹ Ã— (Î¼â‚‚ âˆ’ Î¼â‚)

   = [ 50.25  âˆ’49.75] Ã— [5.0]
     [âˆ’49.75   50.25]   [3.0]

w*[1] = (50.25 Ã— 5.0) + (âˆ’49.75 Ã— 3.0)
      = 251.25 âˆ’ 149.25
      = 102.0

w*[2] = (âˆ’49.75 Ã— 5.0) + (50.25 Ã— 3.0)
      = âˆ’248.75 + 150.75
      = âˆ’98.0

w* = [102.0, âˆ’98.0]
```

**Normalize (optional, for cleaner numbers):**
```
â€–w*â€– = âˆš(102Â² + 98Â²) = âˆš(10404 + 9604) = âˆš20008 â‰ˆ 141.45

Åµ = w* / â€–w*â€– = [102/141.45, âˆ’98/141.45]
              â‰ˆ [0.72, âˆ’0.69]
```

For this problem, we'll use **w* = [102.0, âˆ’98.0]** (unnormalized).

---

#### **STEP 5: Project and Classify Test Point**

**Project class means:**
```
mÌƒâ‚ = (w*)áµ€ Î¼â‚
    = (102.0 Ã— 1.5) + (âˆ’98.0 Ã— 2.5)
    = 153.0 âˆ’ 245.0
    = âˆ’92.0

mÌƒâ‚‚ = (w*)áµ€ Î¼â‚‚
    = (102.0 Ã— 6.5) + (âˆ’98.0 Ã— 5.5)
    = 663.0 âˆ’ 539.0
    = 124.0
```

**Project test point:**
```
x_test = [4, 3]

y_test = (w*)áµ€ x_test
       = (102.0 Ã— 4) + (âˆ’98.0 Ã— 3)
       = 408.0 âˆ’ 294.0
       = 114.0
```

**Compute threshold:**
```
t = (mÌƒâ‚ + mÌƒâ‚‚) / 2
  = (âˆ’92.0 + 124.0) / 2
  = 32.0 / 2
  = 16.0
```

**Classify:**
```
y_test = 114.0
t = 16.0

y_test > t  â†’  114.0 > 16.0  âœ“

â†’ Classified as CLASS 2 (Versicolor)
```

---

**Visual on 1D line:**
```
  âˆ’92.0        16.0       114.0      124.0
    |            |           |          |
    â†“            â†“           â†“          â†“
   mÌƒâ‚       threshold    y_test       mÌƒâ‚‚
(Setosa)        (t)      (TEST)   (Versicolor)

Test point is much closer to Versicolor! âœ“
```

---

**Key Takeaways:**
1. **Singular matrices** require regularization in practice
2. **All 6 steps** executed: means â†’ scatter â†’ direction â†’ project â†’ classify
3. **Matrix inverse** for 2Ã—2 case uses the shortcut formula
4. **Outer products** build scatter matrices
5. **Threshold** is midpoint of projected means

---

</details>

---

## Problem 4: LDA vs PCA Conceptual

### Question

**Scenario:** You have a dataset of patient health records with 50 features (blood pressure, cholesterol, age, etc.) and 3 disease categories (Healthy, Type-A, Type-B). You want to visualize the data in 2D.

**Tasks:**
1. Should you use PCA or LDA? Why?
2. What if the categories were unknown (unsupervised clustering task)?
3. If you use LDA, how many axes will you get? Why?
4. Give one advantage of PCA over LDA in this scenario.

<details>
<summary><b>ğŸ‘‰ Click to reveal solution</b></summary>

---

### Solution

---

#### **Question 1: Should you use PCA or LDA?**

**Answer: Use LDA**

**Reasoning:**

```
Goal: Visualize 3 disease categories separately

âœ“ LDA is the right choice because:
  1. You HAVE class labels (Healthy, Type-A, Type-B)
  2. You want to SEE separation between disease types
  3. LDA maximizes between-class separation
  4. Resulting plot will show distinct clusters

âœ— PCA would be WRONG because:
  1. PCA doesn't use labels â†’ ignores disease categories
  2. PCA finds directions of VARIANCE, not SEPARATION
  3. High-variance features (e.g., age range) might dominate
  4. Categories might overlap in PCA plot even if separable
```

**Visualization comparison:**
```
PCA Result (PC1 vs PC2):        LDA Result (LD1 vs LD2):
   Healthy patients               Healthy cluster
   Type-A patients                  â—â—â—â—â—
   Type-B patients                â—â—â—â—â—â—â—

PC2 â†‘                           LD2 â†‘
    â”‚ â—â– â–²â—â– â–²                        â”‚     â—â—â—â—
    â”‚ â–²â—â– â–²â—â–                         â”‚     â—â—â—â—â—
    â”‚ â– â—â–²â– â—â–²                        â”‚  â– â– â– â–      â–²â–²â–²â–²
    â”‚ â—â– â–²â—â– â–²                        â”‚  â– â– â– â–      â–²â–²â–²â–²
    â””â”€â”€â”€â”€â”€â”€â”€â†’ PC1                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ LD1

  Mixed! Can't tell               Clear clusters! âœ“
  categories apart
```

---

#### **Question 2: What if categories were unknown?**

**Answer: Use PCA**

**Reasoning:**

```
If NO labels available (unsupervised):

âœ“ PCA is the right choice:
  1. Works without labels
  2. Reduces dimensions to visualize structure
  3. Can be followed by clustering (K-means, etc.)

âœ— LDA CANNOT be used:
  1. LDA is SUPERVISED â†’ requires labels
  2. No way to compute class means without labels
  3. Cannot compute between-class scatter
```

**Workflow for unknown categories:**
```
1. Apply PCA (unsupervised dimensionality reduction)
2. Visualize in 2D (PC1 vs PC2)
3. Apply K-means or DBSCAN clustering
4. Discover natural groupings
```

---

#### **Question 3: How many LDA axes? Why?**

**Answer: 2 axes (LD1 and LD2)**

**Formula:**
```
# LDA axes = min(C âˆ’ 1,  p)
                 â†‘       â†‘
              classes  features

C = 3 categories (Healthy, Type-A, Type-B)
p = 50 features

# axes = min(3 âˆ’ 1, 50)
      = min(2, 50)
      = 2
```

**Geometric explanation:**
```
2 points â†’ define 1 line   â†’ 1 axis needed
3 points â†’ define 1 plane  â†’ 2 axes needed
4 points â†’ define 3D space â†’ 3 axes needed

3 class centroids lie in a 2D plane â†’ need 2 axes to span it
```

**Result:**
```
LDA reduces 50D â†’ 2D
  Feature 1:  LD1 (best separating axis)
  Feature 2:  LD2 (second-best separating axis)

Perfect for visualization! âœ“
```

---

#### **Question 4: One advantage of PCA over LDA?**

**Answer: PCA can use more dimensions for better representation**

**Explanation:**

```
PCA:  Can use up to p axes (all 50 in this case)
      PC1, PC2, ..., PC50

      You can capture 95% of variance with maybe PC1-PC10

LDA:  Limited to Câˆ’1 = 2 axes only
      LD1, LD2 (that's it!)

      Might lose important variance not related to class separation
```

**Example scenario where PCA wins:**

```
Suppose within each disease category, there are important
sub-patterns (e.g., age groups, genetic variants).

PCA can capture these sub-patterns using PC3, PC4, PC5...

LDA is STUCK with 2 axes â†’ can only show class separation,
loses all within-class structure.
```

**Other PCA advantages:**
- Works with ANY number of samples (LDA needs enough for stable covariance)
- No assumption of equal covariance across classes
- Can detect outliers in unsupervised manner
- Useful for data compression (not just visualization)

---

**Summary Table:**

| Aspect | PCA | LDA |
|--------|-----|-----|
| **For this scenario** | âŒ No (we have labels!) | âœ… Yes (use labels!) |
| **If no labels** | âœ… Yes | âŒ No (impossible) |
| **# axes possible** | Up to 50 | Only 2 (Câˆ’1) |
| **Best for** | Variance preservation | Class separation |
| **Advantage** | More flexibility | Better classification |

---

</details>

---

## Problem 5: 3-Class LDA Axes

### Question

You are working with gene expression data:
- **Features:** 10,000 genes per patient
- **Classes:** 3 cancer subtypes (A, B, C)
- **Samples:** 100 patients total

**Tasks:**
1. How many LDA axes will be created? Show the formula.
2. What are these axes called?
3. Which axis is most important and why?
4. If you had 2 classes instead, how many axes?
5. If you had 100 classes, how many axes?

<details>
<summary><b>ğŸ‘‰ Click to reveal solution</b></summary>

---

### Solution

---

#### **Question 1: How many LDA axes?**

**Formula:**
```
# LDA axes = min(C âˆ’ 1,  p)
```

**Where:**
- **C** = number of classes
- **p** = number of features

**Given:**
- C = 3 (cancer subtypes A, B, C)
- p = 10,000 (genes)

**Calculation:**
```
# axes = min(3 âˆ’ 1, 10,000)
      = min(2, 10,000)
      = 2
```

**Answer: 2 LDA axes** (LD1 and LD2)

---

#### **Why this formula?**

**Geometric reasoning:**
```
3 class means (Î¼_A, Î¼_B, Î¼_C) in 10,000D space

These 3 points define a PLANE in high-dimensional space.

To span a plane, you need 2 axes.

General rule:
  N points define an (Nâˆ’1)-dimensional space
  â†’ need Nâˆ’1 axes to represent it
```

**Visual analogy (in 3D):**
```
Imagine 3 dots in a 3D room:

        â— Î¼_C
       /â”‚\
      / â”‚ \
     /  â”‚  \
    â—â”€â”€â”€â”¼â”€â”€â”€â— 
   Î¼_A  â”‚  Î¼_B
        â”‚
    (They define a flat plane)

To describe positions ON this plane,
you only need 2 coordinates (x, y on the plane),
NOT all 3 dimensions of the room.

Same idea: 3 classes â†’ 2 LDA axes
```

---

#### **Question 2: What are these axes called?**

**Answer:**
```
LD1 and LD2

LD = Linear Discriminant

Full names:
  LD1 = First Linear Discriminant (best separating axis)
  LD2 = Second Linear Discriminant (second-best axis)
```

**Alternative names you might see:**
- Discriminant Function 1 and 2
- Canonical Variables 1 and 2
- LDA Component 1 and 2

---

#### **Question 3: Which axis is most important?**

**Answer: LD1 is most important**

**Reasoning:**

```
LDA axes are ranked by their EIGENVALUES (Î»)

When solving:  SÊ™ w = Î» Sáµ‚ w

We get multiple eigenvectors (wâ‚, wâ‚‚) with eigenvalues (Î»â‚, Î»â‚‚)

Î»â‚ > Î»â‚‚  â†’  LD1 separates classes better than LD2
```

**What the eigenvalue means:**
```
         wáµ€ SÊ™ w         Between-class variance along w
J(w) = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
         wáµ€ Sáµ‚ w         Within-class variance along w

Î» = this ratio for the axis w

Higher Î» â†’ better separation!

Typical values:
  Î»â‚ = 15.3  â† LD1 (best!)
  Î»â‚‚ = 4.2   â† LD2 (good, but not as good as LD1)
```

**Practical implication:**
```
For visualization:
  - LD1 vs LD2 plot: Best 2D view
  - LD1 alone: Often sufficient for classification
  - LD2 alone: Less useful than LD1

For classification:
  - Can use LD1 only for simpler model
  - Use both LD1 + LD2 for better accuracy
```

---

#### **Question 4: If 2 classes, how many axes?**

**Answer: 1 axis**

**Calculation:**
```
C = 2
p = 10,000

# axes = min(C âˆ’ 1, p)
      = min(2 âˆ’ 1, 10,000)
      = min(1, 10,000)
      = 1
```

**Intuition:**
```
2 class means define a LINE (not a plane)

   Î¼_A â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â— Î¼_B

A line is 1-dimensional â†’ need only 1 axis (LD1)

This is the standard binary LDA case!
```

---

#### **Question 5: If 100 classes, how many axes?**

**Answer: 99 axes**

**Calculation:**
```
C = 100
p = 10,000

# axes = min(C âˆ’ 1, p)
      = min(100 âˆ’ 1, 10,000)
      = min(99, 10,000)
      = 99
```

**BUT there's a practical limit:**

```
âš ï¸ Even though mathematically we get 99 axes,
   we can only compute them if we have ENOUGH samples!

Rule of thumb:
  Need at least C samples (ideally much more)
  to reliably estimate C class means and covariances

With only 100 patients total:
  100 samples / 100 classes = 1 sample per class!
  â†’ Not enough to compute reliable statistics
  â†’ LDA would fail

Better approach with 100 classes:
  1. Use hierarchical classification
  2. Group similar classes
  3. Use deep learning instead
```

---

**Summary Table:**

| Classes (C) | Features (p) | LDA Axes | Limiting Factor |
|-------------|--------------|----------|-----------------|
| 2 | 10,000 | 1 | Câˆ’1 = 1 |
| 3 | 10,000 | 2 | Câˆ’1 = 2 |
| 10 | 10,000 | 9 | Câˆ’1 = 9 |
| 100 | 10,000 | 99 | Câˆ’1 = 99 |
| 3 | 2 | 2 | p = 2 (both limit equally) |
| 10 | 5 | 5 | **p = 5** (features limit!) |

---

**Key Formula to Remember:**
```
# LDA axes = min(C âˆ’ 1, p)
            â†‘        â†‘
        class limit  feature limit

Whichever is SMALLER is the bottleneck!
```

---

</details>

---

## Problem 6: Fisher's Criterion Calculation

### Question

Given the following 1D projected data:

**Class 1 (after projection onto w):**
```
Projected points: yâ‚ = 2, yâ‚‚ = 3, yâ‚ƒ = 4
```

**Class 2 (after projection onto w):**
```
Projected points: yâ‚„ = 8, yâ‚… = 9, yâ‚† = 10
```

**Tasks:**
1. Compute projected class means mÌƒâ‚ and mÌƒâ‚‚
2. Compute within-class scatter sâ‚Â² and sâ‚‚Â²
3. Compute Fisher's criterion J
4. Interpret: Is this a good separation?

<details>
<summary><b>ğŸ‘‰ Click to reveal solution</b></summary>

---

### Solution

---

#### **STEP 1: Compute Projected Class Means**

**Formula:**
```
mÌƒâ‚– = (1/Nâ‚–) Î£ yâ‚™
```

**For Class 1:**
```
yâ‚ = 2,  yâ‚‚ = 3,  yâ‚ƒ = 4
Nâ‚ = 3

mÌƒâ‚ = (yâ‚ + yâ‚‚ + yâ‚ƒ) / 3
   = (2 + 3 + 4) / 3
   = 9 / 3
   = 3.0
```

**For Class 2:**
```
yâ‚„ = 8,  yâ‚… = 9,  yâ‚† = 10
Nâ‚‚ = 3

mÌƒâ‚‚ = (yâ‚„ + yâ‚… + yâ‚†) / 3
   = (8 + 9 + 10) / 3
   = 27 / 3
   = 9.0
```

**Result:**
```
mÌƒâ‚ = 3.0   (Class 1 center on projected axis)
mÌƒâ‚‚ = 9.0   (Class 2 center on projected axis)
```

---

#### **STEP 2: Compute Within-Class Scatter**

**Formula:**
```
sâ‚–Â² = Î£ (yâ‚™ âˆ’ mÌƒâ‚–)Â²
     nâˆˆCâ‚–

This is the variance of projected points around their class mean.
```

**For Class 1:**
```
mÌƒâ‚ = 3.0

Point yâ‚ = 2:   (yâ‚ âˆ’ mÌƒâ‚)Â² = (2 âˆ’ 3.0)Â² = (âˆ’1.0)Â² = 1.0
Point yâ‚‚ = 3:   (yâ‚‚ âˆ’ mÌƒâ‚)Â² = (3 âˆ’ 3.0)Â² = (0.0)Â²  = 0.0
Point yâ‚ƒ = 4:   (yâ‚ƒ âˆ’ mÌƒâ‚)Â² = (4 âˆ’ 3.0)Â² = (1.0)Â²  = 1.0

sâ‚Â² = 1.0 + 0.0 + 1.0 = 2.0
```

**For Class 2:**
```
mÌƒâ‚‚ = 9.0

Point yâ‚„ = 8:   (yâ‚„ âˆ’ mÌƒâ‚‚)Â² = (8 âˆ’ 9.0)Â²  = (âˆ’1.0)Â² = 1.0
Point yâ‚… = 9:   (yâ‚… âˆ’ mÌƒâ‚‚)Â² = (9 âˆ’ 9.0)Â²  = (0.0)Â²  = 0.0
Point yâ‚† = 10:  (yâ‚† âˆ’ mÌƒâ‚‚)Â² = (10 âˆ’ 9.0)Â² = (1.0)Â²  = 1.0

sâ‚‚Â² = 1.0 + 0.0 + 1.0 = 2.0
```

**Result:**
```
sâ‚Â² = 2.0   (Class 1 spread)
sâ‚‚Â² = 2.0   (Class 2 spread)
```

---

#### **STEP 3: Compute Fisher's Criterion**

**Formula:**
```
         (mÌƒâ‚ âˆ’ mÌƒâ‚‚)Â²
J(w) = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
         sâ‚Â² + sâ‚‚Â²
```

**Numerator (between-class separation):**
```
(mÌƒâ‚ âˆ’ mÌƒâ‚‚)Â² = (3.0 âˆ’ 9.0)Â²
            = (âˆ’6.0)Â²
            = 36.0
```

**Denominator (within-class scatter):**
```
sâ‚Â² + sâ‚‚Â² = 2.0 + 2.0
          = 4.0
```

**Fisher's Criterion:**
```
J = 36.0 / 4.0
  = 9.0
```

**Result: J = 9.0**

---

#### **STEP 4: Interpretation**

**Is this a good separation?**

**Answer: YES, excellent separation! âœ“**

**Reasoning:**

```
J = 9.0

What does this mean?

J = (between-class distance)Â² / (within-class scatter)
  = 36.0 / 4.0
  = 9.0

The between-class distance is 9Ã— larger than within-class spread!
```

**Visual representation:**
```
1D Number Line:

  2  3  4        8  9  10
  â—  â—  â—        â—  â—  â—
  Class 1        Class 2
  â†‘              â†‘
  mÌƒâ‚=3          mÌƒâ‚‚=9

Gap = |9 âˆ’ 3| = 6 units
Spread (Ïƒ) â‰ˆ âˆš2 â‰ˆ 1.4 units per class

Gap / Spread â‰ˆ 6 / 1.4 â‰ˆ 4.3

The gap is 4.3Ã— larger than the typical spread â†’ CLEAN SEPARATION âœ“
```

**Guidelines for interpreting J:**

| J Value | Quality | Interpretation |
|---------|---------|----------------|
| J < 1 | Poor | Classes overlap heavily |
| 1 â‰¤ J < 3 | Moderate | Some overlap, classification challenging |
| 3 â‰¤ J < 10 | Good | Clear separation, good for classification |
| J â‰¥ 10 | Excellent | Very clean separation |

**In this case:** J = 9.0 â†’ **Good to Excellent** range

---

**Alternative metric â€” Separation Ratio:**

```
How many "scatter widths" fit in the gap?

Scatter width â‰ˆ âˆš(sâ‚Â² + sâ‚‚Â²) = âˆš4 = 2.0
Gap = |mÌƒâ‚‚ âˆ’ mÌƒâ‚| = 6.0

Ratio = 6.0 / 2.0 = 3.0

The gap is 3 scatter-widths wide â†’ very clean!
```

---

**What if J was different?**

**Example 1: Poor separation (J = 0.5)**
```
Suppose mÌƒâ‚ = 3.0, mÌƒâ‚‚ = 5.0, sâ‚Â² = sâ‚‚Â² = 4.0

J = (3.0 âˆ’ 5.0)Â² / (4.0 + 4.0)
  = 4.0 / 8.0
  = 0.5

Visual:
  1  2  3  4  5  6  7  8
  â—  â—  â—  â—  â—  â—  â—  â—
     Class 1  |  Class 2
     (spread)    (spread)

Heavily overlapping! Classification would be poor.
```

**Example 2: Excellent separation (J = 25)**
```
Suppose mÌƒâ‚ = 0.0, mÌƒâ‚‚ = 10.0, sâ‚Â² = sâ‚‚Â² = 1.0

J = (0.0 âˆ’ 10.0)Â² / (1.0 + 1.0)
  = 100.0 / 2.0
  = 50.0

Visual:
  -1  0  1         8  9  10  11
   â—  â—  â—         â—  â—  â—
   Class 1         Class 2

Perfectly separated! Near-perfect classification.
```

---

**Key Formulas Recap:**

1. **Projected mean:** mÌƒâ‚– = (1/Nâ‚–) Î£ yâ‚™
2. **Scatter:** sâ‚–Â² = Î£ (yâ‚™ âˆ’ mÌƒâ‚–)Â²
3. **Fisher's criterion:** J = (mÌƒâ‚ âˆ’ mÌƒâ‚‚)Â² / (sâ‚Â² + sâ‚‚Â²)
4. **Interpretation:** Higher J = better separation

---

</details>

---

## Problem 7: 2Ã—2 Matrix Inverse Shortcut

### Question

**Given the within-class scatter matrix:**
```
Sáµ‚ = [3.0  1.0]
     [1.0  2.0]
```

**Tasks:**
1. Compute the determinant
2. Find the inverse Sáµ‚â»Â¹ using the 2Ã—2 shortcut formula
3. Verify your answer by checking Sáµ‚ Ã— Sáµ‚â»Â¹ = I

<details>
<summary><b>ğŸ‘‰ Click to reveal solution</b></summary>

---

### Solution

---

#### **STEP 1: Compute the Determinant**

**Formula for 2Ã—2 determinant:**
```
For matrix A = [a  b]
               [c  d]

det(A) = ad âˆ’ bc
```

**Given:**
```
Sáµ‚ = [3.0  1.0]    â†’  a = 3.0,  b = 1.0
     [1.0  2.0]        c = 1.0,  d = 2.0
```

**Calculation:**
```
det(Sáµ‚) = ad âˆ’ bc
        = (3.0 Ã— 2.0) âˆ’ (1.0 Ã— 1.0)
        = 6.0 âˆ’ 1.0
        = 5.0
```

**Result: det(Sáµ‚) = 5.0**

**âœ“ Matrix is invertible** (det â‰  0)

---

#### **STEP 2: Compute the Inverse**

**Formula for 2Ã—2 inverse:**
```
For matrix A = [a  b]
               [c  d]

         1
Aâ»Â¹ = â”€â”€â”€â”€â”€â”€ Ã— [ d  âˆ’b]
      adâˆ’bc    [âˆ’c   a]

Memory trick: "Swap a and d, negate b and c, divide by det"
```

**Apply to Sáµ‚:**
```
Sáµ‚ = [3.0  1.0]
     [1.0  2.0]

det(Sáµ‚) = 5.0

Step 1: Swap diagonal elements (a â†” d)
        [ d  ?] = [2.0  ?]
        [ ?  a]   [?   3.0]

Step 2: Negate off-diagonal elements (âˆ’b, âˆ’c)
        [2.0  âˆ’1.0]
        [âˆ’1.0  3.0]

Step 3: Divide by determinant
        1
Sáµ‚â»Â¹ = â”€â”€â”€ Ã— [2.0  âˆ’1.0]
        5.0    [âˆ’1.0  3.0]
```

**Final computation:**
```
Sáµ‚â»Â¹ = [2.0/5.0   âˆ’1.0/5.0]
       [âˆ’1.0/5.0   3.0/5.0]

     = [0.4   âˆ’0.2]
       [âˆ’0.2   0.6]
```

**Result:**
```
Sáµ‚â»Â¹ = [0.4   âˆ’0.2]
       [âˆ’0.2   0.6]
```

---

#### **STEP 3: Verify the Inverse**

**Property to check:**
```
A Ã— Aâ»Â¹ = I   (Identity matrix)

I = [1  0]
    [0  1]
```

**Compute Sáµ‚ Ã— Sáµ‚â»Â¹:**
```
Sáµ‚ Ã— Sáµ‚â»Â¹ = [3.0  1.0] Ã— [0.4   âˆ’0.2]
            [1.0  2.0]   [âˆ’0.2   0.6]
```

**Matrix multiplication (row Ã— column):**

**Element [1,1]:**
```
Row 1 Ã— Column 1
= (3.0 Ã— 0.4) + (1.0 Ã— âˆ’0.2)
= 1.2 + (âˆ’0.2)
= 1.0 âœ“
```

**Element [1,2]:**
```
Row 1 Ã— Column 2
= (3.0 Ã— âˆ’0.2) + (1.0 Ã— 0.6)
= âˆ’0.6 + 0.6
= 0.0 âœ“
```

**Element [2,1]:**
```
Row 2 Ã— Column 1
= (1.0 Ã— 0.4) + (2.0 Ã— âˆ’0.2)
= 0.4 + (âˆ’0.4)
= 0.0 âœ“
```

**Element [2,2]:**
```
Row 2 Ã— Column 2
= (1.0 Ã— âˆ’0.2) + (2.0 Ã— 0.6)
= âˆ’0.2 + 1.2
= 1.0 âœ“
```

**Result:**
```
Sáµ‚ Ã— Sáµ‚â»Â¹ = [1.0  0.0] = I âœ“âœ“âœ“
            [0.0  1.0]

VERIFIED! The inverse is correct.
```

---

**Summary of 2Ã—2 Inverse Steps:**

```
Given: A = [a  b]
           [c  d]

Step 1: Compute det(A) = ad âˆ’ bc

Step 2: If det â‰  0, compute:

         1      [ d  âˆ’b]
Aâ»Â¹ = â”€â”€â”€â”€â”€â”€ Ã— [      ]
      adâˆ’bc    [âˆ’c   a]

Step 3: Verify: A Ã— Aâ»Â¹ = I

Shortcut memory:
"Flip main diagonal, flip signs of anti-diagonal, divide by det"
```

---

**Common mistakes to avoid:**

âŒ **Mistake 1:** Forgetting to negate off-diagonal
```
WRONG: Aâ»Â¹ = (1/det) Ã— [d  b]   â† b should be âˆ’b
                        [c  a]   â† c should be âˆ’c
```

âŒ **Mistake 2:** Not dividing by determinant
```
WRONG: Aâ»Â¹ = [d  âˆ’b]   â† Missing the 1/det factor!
             [âˆ’c  a]
```

âŒ **Mistake 3:** Swapping wrong elements
```
WRONG: Aâ»Â¹ = (1/det) Ã— [a  âˆ’b]   â† a,d should swap!
                        [âˆ’c  d]
```

âœ“ **Correct:**
```
Aâ»Â¹ = (1/det) Ã— [ d  âˆ’b]   â† d in top-left
                [âˆ’c   a]   â† a in bottom-right
```

---

</details>

---

## Problem 8: Between-Class Scatter Matrix

### Question

**Given class means:**
```
Î¼â‚ = [2, 1]   (Class 1 center)
Î¼â‚‚ = [5, 4]   (Class 2 center)
```

**Tasks:**
1. Compute the mean difference vector
2. Compute the between-class scatter matrix SÊ™
3. Interpret the result: What do the matrix elements represent?

<details>
<summary><b>ğŸ‘‰ Click to reveal solution</b></summary>

---

### Solution

---

#### **STEP 1: Compute Mean Difference Vector**

**Formula:**
```
Î”Î¼ = Î¼â‚‚ âˆ’ Î¼â‚
```

**Calculation:**
```
Î¼â‚ = [2, 1]
Î¼â‚‚ = [5, 4]

Î”Î¼ = Î¼â‚‚ âˆ’ Î¼â‚
   = [5, 4] âˆ’ [2, 1]
   = [5âˆ’2, 4âˆ’1]
   = [3, 3]
```

**Result: Î”Î¼ = [3, 3]**

**Interpretation:**
> This vector points FROM Class 1 center TO Class 2 center.
> It shows the direction and magnitude of class separation.

---

#### **STEP 2: Compute Between-Class Scatter SÊ™**

**Formula:**
```
SÊ™ = (Î¼â‚‚ âˆ’ Î¼â‚)(Î¼â‚‚ âˆ’ Î¼â‚)áµ€
   = Î”Î¼ Ã— (Î”Î¼)áµ€
   = (column vector) Ã— (row vector)
   = outer product
```

**Setup:**
```
Î”Î¼ as column vector:  [3]
                      [3]

Î”Î¼ as row vector:  [3, 3]
```

**Outer product computation:**
```
SÊ™ = [3] Ã— [3, 3]
     [3]

   = [3Ã—3  3Ã—3]
     [3Ã—3  3Ã—3]

   = [9  9]
     [9  9]
```

**Result:**
```
SÊ™ = [9  9]
     [9  9]
```

---

#### **STEP 3: Interpretation**

**What do these matrix elements mean?**

```
SÊ™ = [SÊ™â‚â‚  SÊ™â‚â‚‚]
     [SÊ™â‚‚â‚  SÊ™â‚‚â‚‚]

SÊ™â‚â‚ = 9   â†’  Variance of class separation in dimension 1
SÊ™â‚‚â‚‚ = 9   â†’  Variance of class separation in dimension 2
SÊ™â‚â‚‚ = 9   â†’  Covariance between dimensions
SÊ™â‚‚â‚ = 9   â†’  Same as SÊ™â‚â‚‚ (matrix is symmetric)
```

**Breaking it down:**

**Diagonal elements (SÊ™â‚â‚, SÊ™â‚‚â‚‚):**
```
SÊ™â‚â‚ = (Î¼â‚‚[1] âˆ’ Î¼â‚[1])Â²
     = (5 âˆ’ 2)Â²
     = 3Â²
     = 9

"How far apart are the class means in dimension 1?"
â†’ 3 units apart â†’ 9 when squared

SÊ™â‚‚â‚‚ = (Î¼â‚‚[2] âˆ’ Î¼â‚[2])Â²
     = (4 âˆ’ 1)Â²
     = 3Â²
     = 9

"How far apart are the class means in dimension 2?"
â†’ 3 units apart â†’ 9 when squared
```

**Off-diagonal elements (SÊ™â‚â‚‚, SÊ™â‚‚â‚):**
```
SÊ™â‚â‚‚ = (Î¼â‚‚[1] âˆ’ Î¼â‚[1]) Ã— (Î¼â‚‚[2] âˆ’ Î¼â‚[2])
     = (5 âˆ’ 2) Ã— (4 âˆ’ 1)
     = 3 Ã— 3
     = 9

"How correlated is the separation across dimensions?"
â†’ Perfect correlation (both dimensions separate equally)
```

---

**Geometric interpretation:**

```
2D plot:

     Dim 2 â†‘
         4 â”‚           â— Î¼â‚‚ [5,4]
         3 â”‚          /
         2 â”‚         /  â† Î”Î¼ = [3,3]
         1 â”‚        /     separation vector
           â”‚â— Î¼â‚ [2,1]
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Dim 1
             1  2  3  4  5

The classes are separated:
  - 3 units in dimension 1
  - 3 units in dimension 2
  - Diagonal direction (45Â° angle)
```

**Why is SÊ™ singular (rank 1)?**
```
det(SÊ™) = (9 Ã— 9) âˆ’ (9 Ã— 9) = 81 âˆ’ 81 = 0

Reason: All columns/rows are multiples of each other
        Row 2 = 1 Ã— Row 1

This is ALWAYS true for 2-class LDA:
  SÊ™ has rank 1 (only 1 direction of separation)
```

---

**Properties of Between-Class Scatter:**

1. **Always symmetric:** SÊ™ = SÊ™áµ€
2. **Always positive semi-definite:** xáµ€SÊ™x â‰¥ 0 for all x
3. **Rank = min(Câˆ’1, p):** For 2 classes, rank = 1
4. **Larger values = better separation**

---

**Compare to Within-Class Scatter:**

```
Within-class (Sáµ‚):                Between-class (SÊ™):
- Measures spread INSIDE classes  - Measures gap BETWEEN classes
- Want SMALL (tight clusters)     - Want LARGE (far apart means)
- Sum over all points             - Based only on means
- Full rank (usually)             - Rank â‰¤ Câˆ’1
```

---

**Using SÊ™ in LDA:**

```
The LDA solution is:

w* = Sáµ‚â»Â¹ Ã— (Î¼â‚‚ âˆ’ Î¼â‚)

This can also be written as:

w* = Sáµ‚â»Â¹ Ã— SÊ™   (when SÊ™ is treated as outer product)

The direction w* balances:
  - Maximizing SÊ™ (push classes apart)
  - Minimizing Sáµ‚ (keep classes tight)
```

---

**Calculation shortcut:**

For 2-class LDA, you don't actually need to form SÊ™ as a matrix!

```
Direct formula:
w* = Sáµ‚â»Â¹ (Î¼â‚‚ âˆ’ Î¼â‚)

This is simpler than:
1. Computing SÊ™
2. Solving eigenvalue problem
3. Finding eigenvectors

The 2-class case has a closed form! âœ“
```

---

</details>

---

## Problem 9: Projection and Classification

### Question

**Setup:**
You have computed the optimal LDA direction:
```
w* = [4, âˆ’3]
```

**Training data projections (already computed):**
```
Class 1 projected values: [1.0, 1.5, 2.0, 2.5, 3.0]
Class 2 projected values: [7.0, 7.5, 8.0, 8.5, 9.0]
```

**Tasks:**
1. Compute the threshold for classification
2. A new patient has features x_new = [2.0, 1.5]. Project and classify.
3. Another patient has x_new = [1.5, 3.0]. Project and classify.
4. What is the decision boundary equation in the original 2D space?

<details>
<summary><b>ğŸ‘‰ Click to reveal solution</b></summary>

---

### Solution

---

#### **STEP 1: Compute Classification Threshold**

**Method: Use projected class means**

**Class 1 projected mean:**
```
Projected values: [1.0, 1.5, 2.0, 2.5, 3.0]
Nâ‚ = 5

mÌƒâ‚ = (1.0 + 1.5 + 2.0 + 2.5 + 3.0) / 5
   = 10.0 / 5
   = 2.0
```

**Class 2 projected mean:**
```
Projected values: [7.0, 7.5, 8.0, 8.5, 9.0]
Nâ‚‚ = 5

mÌƒâ‚‚ = (7.0 + 7.5 + 8.0 + 8.5 + 9.0) / 5
   = 40.0 / 5
   = 8.0
```

**Threshold (midpoint):**
```
t = (mÌƒâ‚ + mÌƒâ‚‚) / 2
  = (2.0 + 8.0) / 2
  = 10.0 / 2
  = 5.0
```

**Result: Threshold t = 5.0**

**Decision rule:**
```
If y < 5.0  â†’  Class 1
If y â‰¥ 5.0  â†’  Class 2
```

---

#### **STEP 2: Patient 1 â€” x_new = [2.0, 1.5]**

**Project onto w*:**
```
w* = [4, âˆ’3]
x_new = [2.0, 1.5]

y = wáµ€ x
  = (4 Ã— 2.0) + (âˆ’3 Ã— 1.5)
  = 8.0 + (âˆ’4.5)
  = 3.5
```

**Compare to threshold:**
```
y = 3.5
t = 5.0

y < t  â†’  3.5 < 5.0  âœ“

â†’ Classified as CLASS 1
```

**Visual:**
```
1D Line:

  1.0      2.0      3.5      5.0      7.0      8.0      9.0
   |        |        |        |        |        |        |
   â†“        â†“        â†“        â†“        â†“        â†“        â†“
  C1      mÌƒâ‚    Patient1  threshold  C2      mÌƒâ‚‚      C2
points                (t)          points

Patient 1 falls in the Class 1 region âœ“
```

---

#### **STEP 3: Patient 2 â€” x_new = [1.5, 3.0]**

**Project onto w*:**
```
w* = [4, âˆ’3]
x_new = [1.5, 3.0]

y = wáµ€ x
  = (4 Ã— 1.5) + (âˆ’3 Ã— 3.0)
  = 6.0 + (âˆ’9.0)
  = âˆ’3.0
```

**Compare to threshold:**
```
y = âˆ’3.0
t = 5.0

y < t  â†’  âˆ’3.0 < 5.0  âœ“

â†’ Classified as CLASS 1
```

**Note the negative projection:**
```
A negative y value means the point projects to the
"left" side of the origin on the LDA axis.

Since âˆ’3.0 < 5.0, it's still Class 1 (which makes sense,
as Class 1 has lower projected values).
```

**Visual:**
```
1D Line:

  âˆ’3.0     1.0      2.0      5.0      7.0      8.0
    |       |        |        |        |        |
    â†“       â†“        â†“        â†“        â†“        â†“
 Patient2   C1      mÌƒâ‚    threshold  C2      mÌƒâ‚‚
                                    points

Patient 2 is even further into Class 1 region âœ“
```

---

#### **STEP 4: Decision Boundary in Original 2D Space**

**The LDA decision boundary is a hyperplane (in 2D, a line):**

**General form:**
```
wáµ€x = constant

For classification, the boundary is where:
wáµ€x = threshold

Expanded:
wâ‚xâ‚ + wâ‚‚xâ‚‚ = t
```

**Substitute values:**
```
w* = [4, âˆ’3]
t = 5.0

4xâ‚ + (âˆ’3)xâ‚‚ = 5.0

Simplified:
4xâ‚ âˆ’ 3xâ‚‚ = 5
```

**This is the decision boundary equation! âœ“**

---

**Express as y = mx + b (standard line form):**

```
4xâ‚ âˆ’ 3xâ‚‚ = 5

Solve for xâ‚‚:
âˆ’3xâ‚‚ = 5 âˆ’ 4xâ‚
xâ‚‚ = (4xâ‚ âˆ’ 5) / 3
xâ‚‚ = (4/3)xâ‚ âˆ’ 5/3

Slope m = 4/3
Intercept b = âˆ’5/3
```

---

**Visualize in 2D:**

```
Feature xâ‚‚ â†‘
           â”‚
         3 â”‚          â— Patient 2 [1.5, 3.0]
           â”‚         /
         2 â”‚        /  Decision boundary:
           â”‚       /   4xâ‚ âˆ’ 3xâ‚‚ = 5
         1 â”‚â— Patient 1 [2.0, 1.5]
           â”‚     /
           â”‚    /
         0 â”‚â”€â”€â”€/â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Feature xâ‚
           0   1   2   3   4   5

Points BELOW line â†’ Class 1
Points ABOVE line â†’ Class 2

(Both patients are below the line â†’ Class 1 âœ“)
```

---

**Classification regions:**

```
REGION 1 (Class 1):
  4xâ‚ âˆ’ 3xâ‚‚ < 5
  All points below/left of the line

REGION 2 (Class 2):
  4xâ‚ âˆ’ 3xâ‚‚ â‰¥ 5
  All points above/right of the line
```

---

**Alternative formulation:**

The decision boundary can also be written as:
```
f(x) = wáµ€x âˆ’ t = 0

f(x) = 4xâ‚ âˆ’ 3xâ‚‚ âˆ’ 5 = 0

Decision rule:
  f(x) < 0  â†’  Class 1
  f(x) â‰¥ 0  â†’  Class 2
```

**Verify patients:**
```
Patient 1: f([2.0, 1.5])
         = 4(2.0) âˆ’ 3(1.5) âˆ’ 5
         = 8.0 âˆ’ 4.5 âˆ’ 5
         = âˆ’1.5 < 0  â†’  Class 1 âœ“

Patient 2: f([1.5, 3.0])
         = 4(1.5) âˆ’ 3(3.0) âˆ’ 5
         = 6.0 âˆ’ 9.0 âˆ’ 5
         = âˆ’8.0 < 0  â†’  Class 1 âœ“
```

---

**Key Concepts:**

1. **Projection:** wáµ€x reduces 2D point to 1D value
2. **Threshold:** Midpoint between projected class means
3. **Decision boundary:** Hyperplane perpendicular to w*
4. **Equation:** wáµ€x = t separates the space
5. **Classification:** Based on which side of hyperplane

---

</details>

---

## Problem 10: Assumption Violation (QDA)

### Question

You are trying to use LDA on a dataset with 2 classes, but the scatter plots show:

**Class 1 (circles):**
- Very tight cluster
- Small variance in all directions
- Covariance matrix: Î£â‚ = [[1, 0], [0, 1]]

**Class 2 (squares):**
- Very spread out cluster
- Large variance in all directions
- Covariance matrix: Î£â‚‚ = [[100, 0], [0, 100]]

**Tasks:**
1. Which LDA assumption is violated here?
2. Will LDA perform well? Why or why not?
3. What alternative should you use instead?
4. Draw a sketch showing why LDA's linear boundary might fail.

<details>
<summary><b>ğŸ‘‰ Click to reveal solution</b></summary>

---

### Solution

---

#### **Question 1: Which LDA Assumption is Violated?**

**Answer: The equal covariance assumption**

**LDA's Key Assumptions:**
```
1. Classes are normally distributed (Gaussian) âœ“ (might be OK)
2. Classes have EQUAL covariance matrices âœ— (VIOLATED!)
   Î£â‚ = Î£â‚‚ = Î£

In this problem:
Î£â‚ = [[1, 0], [0, 1]]       â† Small, tight
Î£â‚‚ = [[100, 0], [0, 100]]   â† Large, spread out

Î£â‚ â‰  Î£â‚‚  â†’  ASSUMPTION VIOLATED âœ—
```

**What does equal covariance mean?**
```
Equal covariance = same shape and orientation of clusters

âœ“ Same shape:    â—â—â—   â– â– â–     (both circular)
                  â—â—â—   â– â– â– 

âœ— Different:     â—â—â—   â– â– â– â– â– â–   (different sizes)
                  â—â—â—   â– â– â– â– â– â– 
                        â– â– â– â– â– â– 

Our problem is the second case!
```

---

#### **Question 2: Will LDA Perform Well?**

**Answer: NO, LDA will perform poorly âœ—**

**Reasoning:**

```
LDA assumes:  Sáµ‚ = Sâ‚ + Sâ‚‚ â‰ˆ Î£â‚ = Î£â‚‚

Reality:      Sâ‚ is tiny, Sâ‚‚ is huge
              Sâ‚ + Sâ‚‚ â‰ˆ Sâ‚‚  (dominated by Class 2!)

Problem: LDA will create a decision boundary that:
  - Is too influenced by Class 2's large spread
  - Doesn't adapt to Class 1's tight cluster
  - May misclassify Class 1 points that are actually close
    to their own mean but far from Class 2
```

---

**Numerical example:**

Suppose:
```
Class 1: Î¼â‚ = [0, 0],  tight cluster (Ïƒ = 1)
Class 2: Î¼â‚‚ = [10, 0], spread out (Ïƒ = 10)
```

**What LDA does:**
```
Sáµ‚ = Sâ‚ + Sâ‚‚ â‰ˆ [[1,0],[0,1]] + [[100,0],[0,100]]
              â‰ˆ [[101,0],[0,101]]

LDA assumes BOTH classes have this large spread!

Decision boundary is placed at:
  xâ‚ = 5  (midpoint between 0 and 10)

But Class 1 is tightly at 0 â†’ boundary should be closer!
```

**Misclassification scenario:**
```
Point at x = [2, 0]:

Truth: Very close to Class 1 (within 2Ïƒ), far from Class 2
LDA says: "It's past xâ‚ = 5? No. It's Class 1." â† Correct by luck

Point at x = [8, 0]:

Truth: Far from Class 1, but within Class 2's large spread
      â†’ Should be Class 2
LDA says: "It's past xâ‚ = 5? Yes. It's Class 2." â† Correct

Point at x = [3, 6]:

Truth: Far from Class 1's tight cluster (distance = âˆš45 â‰ˆ 6.7)
      Way too far! Should be Class 2.
LDA boundary: Linear, may misclassify based on xâ‚ position only
```

The linear boundary doesn't capture the different cluster shapes!

---

#### **Question 3: What Alternative to Use?**

**Answer: Use Quadratic Discriminant Analysis (QDA)**

**QDA relaxes the equal covariance assumption:**

```
LDA:  Î£â‚ = Î£â‚‚ = Î£  (forced to be equal)
QDA:  Î£â‚ â‰  Î£â‚‚     (each class gets its own)

QDA allows:
  - Class 1 to have small Î£â‚
  - Class 2 to have large Î£â‚‚
  - Curved (quadratic) decision boundary
```

---

**LDA vs QDA Comparison:**

| Feature | LDA | QDA |
|---------|-----|-----|
| **Assumption** | Equal Î£ | Different Î£ allowed |
| **Boundary** | Linear (hyperplane) | Curved (quadratic) |
| **Parameters** | Fewer (pÂ² + p) | More (C Ã— pÂ² + p) |
| **Works best when** | Equal covariances | Unequal covariances |
| **Sample size needed** | Smaller OK | Needs more samples |

Where:
- p = # features
- C = # classes

---

**QDA decision boundary:**

```
Instead of:  wáµ€x + b = 0  (linear)

QDA uses:    xáµ€Ax + báµ€x + c = 0  (quadratic)

A depends on Î£â‚ and Î£â‚‚ (different for each class)
```

---

**When to use each:**

```
Use LDA when:
  âœ“ Classes have similar spread
  âœ“ Small dataset (need parameter efficiency)
  âœ“ Want interpretable linear boundary

Use QDA when:
  âœ“ Classes have different shapes/sizes â† OUR CASE
  âœ“ Enough data for more parameters
  âœ“ Higher accuracy > interpretability
```

---

#### **Question 4: Sketch Why LDA Fails**

**Visual explanation:**

```
ORIGINAL 2D DATA:

  xâ‚‚ â†‘
     â”‚
   8 â”‚               â– 
     â”‚          â–          â– 
   6 â”‚     â–                   â– 
     â”‚          â–          â– 
   4 â”‚     â–          â– 
     â”‚          â– 
   2 â”‚  â—â—â—
     â”‚  â—â—â—        Class 2: spread out â– â– â– 
   0 â”‚  â—â—â—        (large Î£â‚‚)
     â”‚
  -2 â”‚  Class 1: tight â—â—â—
     â”‚  (small Î£â‚)
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ xâ‚
        -2  0  2  4  6  8  10  12

LDA Decision Boundary (linear):
     â”‚
     â”‚        /
     â”‚       /  â† Straight line
     â”‚      /     Doesn't wrap around
     â”‚     /      Class 1
     â”‚    /
     â”‚   /
     â”‚  /
     â”‚ /
     â”‚/
```

**The problem:**
```
LDA's linear boundary:
  - Splits space with a straight line
  - Cannot create a "bubble" around Class 1
  - Misclassifies Class 2 points that are near but
    not inside Class 1's tight cluster

Better boundary (QDA):
  - Circular/elliptical around Class 1
  - Adapts to Class 1's small spread
  - Adapts to Class 2's large spread
```

---

**QDA Decision Boundary (curved):**

```
  xâ‚‚ â†‘
     â”‚
   8 â”‚               â– 
     â”‚          â–          â– 
   6 â”‚     â–         â•±â”€â•²       â– 
     â”‚          â–   â”‚ â—â—â—â”‚ â– 
   4 â”‚     â–        â”‚â—â—â—â”‚
     â”‚          â–   â”‚ â—â—â—â”‚
   2 â”‚             â•²â”€â•±
     â”‚
   0 â”‚        Circular boundary
     â”‚        around Class 1 â† QDA creates this!
  -2 â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ xâ‚

Now Class 1 is properly enclosed! âœ“
Everything outside the circle â†’ Class 2
```

---

**Mathematical intuition:**

```
LDA uses pooled covariance:
  Sáµ‚ = Sâ‚ + Sâ‚‚

When Sâ‚‚ >> Sâ‚:
  Sáµ‚ â‰ˆ Sâ‚‚  (dominated by larger class)

LDA treats BOTH classes as if they have spread â‰ˆ Sâ‚‚
â†’ Ignores Class 1's tightness
â†’ Poor boundary

QDA uses separate covariances:
  Class 1 uses Î£â‚
  Class 2 uses Î£â‚‚

Each class is modeled with its true shape âœ“
â†’ Better boundary
```

---

**Real-world analogy:**

```
Imagine classifying:
  Class 1: Basketballs (all same size, tight cluster)
  Class 2: Beach balls (varying sizes, spread out)

LDA assumes:
  "All balls have the same size distribution"
  â†’ Wrong! Misses that basketballs are uniform.

QDA recognizes:
  "Basketballs: tight size range"
  "Beach balls: wide size range"
  â†’ Correct! Adapts to each class.
```

---

**Conclusion:**

When covariances differ significantly:
  1. LDA assumption is violated
  2. Linear boundary performs poorly
  3. Switch to QDA for curved boundary
  4. Accept increased model complexity for better accuracy

---

</details>

---

## ğŸ¯ Summary & Tips

### How to Approach These Problems:

1. **Read carefully** â€” Note what's given vs what needs to be computed
2. **Write formulas first** â€” Before calculating, write down the relevant formula
3. **Show all steps** â€” Don't skip intermediate calculations
4. **Check units** â€” Make sure dimensions match (2Ã—2 matrices, etc.)
5. **Verify answers** â€” Use properties like Sáµ‚ Ã— Sáµ‚â»Â¹ = I to check

### Common Formula Patterns:

```
Means:           Î¼â‚– = (1/Nâ‚–) Î£ xâ‚™
Projection:      y = wáµ€ x
Scatter:         Sâ‚– = Î£ (xâ‚™âˆ’Î¼â‚–)(xâ‚™âˆ’Î¼â‚–)áµ€
Between:         SÊ™ = (Î¼â‚‚âˆ’Î¼â‚)(Î¼â‚‚âˆ’Î¼â‚)áµ€
LDA solution:    w* = Sáµ‚â»Â¹(Î¼â‚‚âˆ’Î¼â‚)
Fisher's J:      J = (mÌƒâ‚‚âˆ’mÌƒâ‚)Â² / (sâ‚Â²+sâ‚‚Â²)
Threshold:       t = (mÌƒâ‚ + mÌƒâ‚‚) / 2
2Ã—2 inverse:     Aâ»Â¹ = (1/det) Ã— [d âˆ’b; âˆ’c a]
```

### Exam Strategy:

- âœ… **Memorize the 2Ã—2 inverse formula** â€” Saves huge time
- âœ… **Practice outer products** â€” Key to scatter matrices
- âœ… **Know when LDA fails** â€” Equal covariance assumption
- âœ… **Understand QDA alternative** â€” Curved boundaries
- âœ… **Draw diagrams** â€” Visual understanding prevents mistakes

---

*Practice these problems multiple times until the steps become automatic!*
