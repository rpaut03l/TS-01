# ğŸ“Š Linear Discriminant Analysis (LDA) â€” Complete Study Guide
### Parts 1 & 2 Combined | LDA Details | Machine Learning (ML)..

> **"LDA is like PCA, but it has eyes â€” it can SEE the class labels!"**

---

## ğŸ“š Table of Contents

| # | Section |
|---|---------|
| 1 | [The Real-World Problem LDA Solves](#1-the-real-world-problem-lda-solves) |
| 2 | [What is LDA?](#2-what-is-lda) |
| 3 | [LDA vs PCA â€” The Core Difference](#3-lda-vs-pca--the-core-difference) |
| 4 | [Visual Intuition â€” Step by Step](#4-visual-intuition--step-by-step) |
| 5 | [All Notations & Symbols Decoded](#5-all-notations--symbols-decoded) |
| 6 | [Part 1 â€” Fisher's LDA: The Math](#6-part-1--fishers-lda-the-math) |
| 7 | [Part 2 â€” The Full Algorithm (2 Classes)](#7-part-2--the-full-algorithm-2-classes) |
| 8 | [Full Worked Example (Flower Classification)](#8-full-worked-example-flower-classification) |
| 9 | [LDA for 3+ Categories](#9-lda-for-3-categories) |
| 10 | [LDA as a Classifier + Decision Rule](#10-lda-as-a-classifier--decision-rule) |
| 11 | [Connection to Other Classifiers](#11-connection-to-other-classifiers) |
| 12 | [ASCII Diagrams â€” Visual Reference](#12-ascii-diagrams--visual-reference) |
| 13 | [Cheatsheet](#13-cheatsheet) |
| 14 | [Mnemonics & Memory Tricks](#14-mnemonics--memory-tricks) |
| 15 | [Common Mistakes & Fixes](#15-common-mistakes--fixes) |
| 16 | [Quick Q&A for Exam](#16-quick-qa-for-exam) |

---

## 1. The Real-World Problem LDA Solves

### ğŸ¥ Story: Cancer Drug Prediction (Part 2 â€” Slides 1â€“24)

A cancer drug:
- âœ… **Works** for some patients
- âŒ **Makes worse** other patients

**Goal:** Can we predict *who* should take it using gene expression data?

```
STEP 1 â€” One Gene (1D number line):

Fewer Transcripts â†â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â†’ More Transcripts

ğŸŸ¢ğŸŸ¢ğŸŸ¢ğŸŸ¢  ğŸ”´  ğŸŸ¢  ğŸ”´ğŸ”´  ğŸ”´  ğŸ”´ğŸ”´

ğŸŸ¢ = Drug works  |  ğŸ”´ = Drug does NOT work

PROBLEM: There's an overlap zone â€” no clean cutoff!
```

```
STEP 2 â€” Two Genes (2D plot):

Gene Y â†‘
       |  ğŸŸ¢         ğŸ”´  ğŸ”´ğŸ”´
       |     ğŸŸ¢         ğŸ”´
       |  ğŸŸ¢ğŸŸ¢     ğŸ”´      ğŸ”´
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Gene X

With a DIAGONAL line we separate them much better! âœ…
```

```
STEP 3 â€” Three Genes (3D space):

Need a PLANE instead of a line to separate groups.
4+ genes? Can't draw it â€” need math!

â†’ This is exactly where LDA steps in.
```

---

## 2. What is LDA?

**Linear Discriminant Analysis (LDA)** is a **supervised** dimensionality reduction and classification technique that:

1. Takes high-dimensional data (e.g. 10,000 genes per patient)
2. Finds new **axes** that **maximize separation** between known categories
3. **Projects** data onto those axes (reduces dimensions)
4. Allows **classification** in lower-dimensional space

### In one sentence:
> **LDA finds the direction(s) in feature space where known classes are most separated.**

---

## 3. LDA vs PCA â€” The Core Difference

```
PCA:  "Which direction has the most SPREAD/VARIANCE?"
LDA:  "Which direction best SEPARATES the classes?"

                     Unsupervised â†â€”â€”â€”â†’ Supervised
                     (no labels)        (uses labels)
                         â†‘                   â†‘
                        PCA                 LDA
```

| Feature | PCA | LDA |
|---|---|---|
| Full Name | Principal Component Analysis | Linear Discriminant Analysis |
| Type | Unsupervised | Supervised |
| Needs class labels? | âŒ No | âœ… Yes |
| Goal | Maximize **variance** | Maximize **class separation** |
| Axes named | PC1, PC2, â€¦ (by variance) | LD1, LD2, â€¦ (by separation) |
| Max axes | up to p (# features) | up to Câˆ’1 (# classes âˆ’ 1) |

### Why PCA fails for classification:

```
PCA result on 3-class data:     LDA result on same data:
(PC1 vs PC2)                    (LD1 vs LD2)

ğŸ”µğŸ”´ğŸŸ¢ğŸ”µğŸ”´ğŸŸ¢ (mixed!)          ğŸ”µğŸ”µğŸ”µ   ğŸ”´ğŸ”´ğŸ”´   ğŸŸ¢ğŸŸ¢ğŸŸ¢
                                  â†‘ clean clusters! âœ…

PCA picked directions of most   LDA picked directions that
overall variance â€” these may    actually pull classes apart.
have NOTHING to do with class
boundaries.
```

---

## 4. Visual Intuition â€” Step by Step

### ğŸ¯ The Two Goals of LDA â€” "PULL APART, PACK TOGETHER"

```
GOAL 1 â€” PULL the class means APART (Big Gap)

     Î¼_green                       Î¼_red
        â†“                             â†“
   ğŸŸ¢ğŸŸ¢ğŸŸ¢ğŸŸ¢  â†â€”â€”â€” d (big!) â€”â€”â€”â†’  ğŸ”´ğŸ”´ğŸ”´ğŸ”´

   Maximize dÂ² = (Î¼â‚ âˆ’ Î¼â‚‚)Â²    â† WANT THIS LARGE â†‘


GOAL 2 â€” PACK each class TOGETHER (Small Mess)

   BAD (large scatter = overlap):
   ğŸŸ¢   ğŸŸ¢   ğŸŸ¢   ğŸ”´   ğŸŸ¢   ğŸ”´   ğŸ”´
           â†‘ spread out = they mix!

   GOOD (small scatter = clear gap):
   ğŸŸ¢ğŸŸ¢ğŸŸ¢ğŸŸ¢ğŸŸ¢           ğŸ”´ğŸ”´ğŸ”´ğŸ”´ğŸ”´
       â†‘ tight             â†‘ tight

   Minimize sÂ² (scatter) within each class   â† WANT THIS SMALL â†“
```

### ğŸ† The LDA Objective Formula (2 Classes)

```
         (Î¼â‚ âˆ’ Î¼â‚‚)Â²            Big Gap
ratio = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  =  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
         sâ‚Â² + sâ‚‚Â²           Small Mess

MAXIMIZE this ratio â†’ best separating axis found!
```

### "The Shadow Play" Analogy

```
Imagine shining a flashlight at green and red balls on a table.

LDA asks: "At what angle should I shine the flashlight so the
SHADOWS of the two groups are FAR APART and each group's
shadows are TIGHTLY CLUSTERED?"

        Flashlight (angle = w*)
             â†“
Table:     ğŸŸ¢ğŸŸ¢ğŸŸ¢   ğŸ”´ğŸ”´ğŸ”´

Wall:   ğŸŸ¢ğŸŸ¢ğŸŸ¢â”â”â”â”â”â”â”â”ğŸ”´ğŸ”´ğŸ”´
                 â†‘ Big gap!
```

---

## 5. All Notations & Symbols Decoded

| Symbol | Name | Meaning | Memory Trick |
|---|---|---|---|
| **Î¼â‚–** (mu) | Class mean vector | Average position of class k | "Mu = Middle of class" |
| **mÌƒâ‚–** | Projected class mean | Mean after projection onto w | Tilde = "transformed" |
| **sÂ²** | Scatter | Within-class spread/variance | "s = Spread Squared" |
| **Nâ‚, Nâ‚‚** | Class sizes | # samples in each class | "N = Number" |
| **Sáµ‚** | Within-class scatter matrix | Total spread inside all classes | **W** = **W**ithin |
| **SÊ™** | Between-class scatter matrix | Spread between class means | **B** = **B**etween |
| **w** | Direction vector | The LDA projection axis | "w = Where to project" |
| **w*** | Optimal w | The best LDA direction (solution) | "*" = optimal |
| **d** | Distance | Difference between class means | d = distance |
| **J(w)** | Fisher criterion | The ratio we maximize | "J = Judge how good" |
| **LD1, LD2** | Linear Discriminants | New axes created by LDA | LD = Linear Discriminant |
| **Î£** (capital) | Covariance matrix | Spread in all directions | "Sigma = Spread matrix" |
| **xâ‚™** | Data point n | One observation/sample | n = nth sample |
| **áµ€** | Transpose | Flip rows â†” columns | "T = Turn around" |
| **â»Â¹** | Matrix inverse | Undoes a matrix (like Ã·) | "-1 = reverse" |
| **C** | # Classes | Number of categories | C = Categories |
| **p** | # Features | Number of dimensions/genes | p = properties |
| **Î»** (lambda) | Eigenvalue | How good an LDA axis is | "Î» = Level of goodness" |

---

## 6. Part 1 â€” Fisher's LDA: The Math

### 6.1 The Projection Concept

```
Each data point x gets PROJECTED onto direction w:

   Projected value y = wáµ€x  (w-transpose times x)

What this means:
   w = a direction arrow in p-dimensional space
   wáµ€x = dot product = how far x lies along w
   y = a single number (the 1D coordinate)

Example (2D data):
   w = [0.8, 0.6]    â† LDA direction
   x = [5.0, 3.0]    â† one data point

   y = wáµ€x = (0.8 Ã— 5.0) + (0.6 Ã— 3.0)
           = 4.0 + 1.8
           = 5.8       â† this point's projected value
```

### 6.2 Class Means

```
Compute the average of each class:

   Î¼â‚– = (1/Nâ‚–) Ã— Î£ xâ‚™   for all n in class k
                nâˆˆCâ‚–

Example:
   Class 1 data: xâ‚=[5.1, 3.5], xâ‚‚=[4.9, 3.0]
   Î¼â‚ = ([5.1,3.5] + [4.9,3.0]) / 2 = [5.0, 3.25]

After projection onto w:
   mÌƒâ‚ = wáµ€Î¼â‚    (projected mean of class 1)
   mÌƒâ‚‚ = wáµ€Î¼â‚‚    (projected mean of class 2)

   We want |mÌƒâ‚‚ âˆ’ mÌƒâ‚| to be LARGE
```

### 6.3 Within-Class Scatter Matrices

```
For class k, the scatter matrix captures internal spread:

   Sâ‚– = Î£ (xâ‚™ âˆ’ Î¼â‚–)(xâ‚™ âˆ’ Î¼â‚–)áµ€
       nâˆˆCâ‚–

Reading step by step:
   For every point xâ‚™ in class k:
     1. Subtract class mean:  diff = xâ‚™ âˆ’ Î¼â‚–
     2. Outer product:        diff Ã— diffáµ€  â†’  gives a pÃ—p matrix
     3. Add all these matrices together

Example (1 point contribution):
   xâ‚™ = [5.1, 3.5],  Î¼â‚– = [5.0, 3.25]
   diff = [0.1, 0.25]

   diff Ã— diffáµ€ = [0.1 ] Ã— [0.1, 0.25] = [0.01   0.025 ]
                  [0.25]                  [0.025  0.0625]

Total within-class scatter (both classes combined):
   Sáµ‚ = Sâ‚ + Sâ‚‚
```

### 6.4 Between-Class Scatter Matrix

```
Captures how far APART the two class means are:

   SÊ™ = (Î¼â‚‚ âˆ’ Î¼â‚)(Î¼â‚‚ âˆ’ Î¼â‚)áµ€

Example:
   Î¼â‚ = [5.0, 3.25],  Î¼â‚‚ = [6.7, 3.2]
   diff = [1.7, âˆ’0.05]

   SÊ™ = [1.7  ] Ã— [1.7, âˆ’0.05] = [2.89    âˆ’0.085]
        [âˆ’0.05]                   [âˆ’0.085  0.0025]
```

### 6.5 Fisher's Criterion â€” The Formula

```
         wáµ€SÊ™w
J(w) = â”€â”€â”€â”€â”€â”€â”€â”€â”€
         wáµ€Sáµ‚w

Numerator   wáµ€SÊ™w = projected between-class spread  â†’ WANT BIG  â†‘
Denominator wáµ€Sáµ‚w = projected within-class spread   â†’ WANT SMALL â†“

Maximize J(w) to find the best w!
```

### 6.6 The Closed-Form Solution

```
LDA has a beautiful closed-form answer â€” no training loop needed!

   w* = Sáµ‚â»Â¹ (Î¼â‚‚ âˆ’ Î¼â‚)

Reading it:
   Sáµ‚â»Â¹          = inverse of within-class scatter matrix
   (Î¼â‚‚ âˆ’ Î¼â‚)     = vector pointing from class 1 to class 2
   Multiply them  = optimal projection direction

Memory: "Inverse Within, times Mean Difference"
         Sáµ‚â»Â¹   Ã—   (Î¼â‚‚ âˆ’ Î¼â‚)
```

---

## 7. Part 2 â€” The Full Algorithm (2 Classes)

```
INPUT:   Training data with class labels
OUTPUT:  LDA direction w*, projected data

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 1 â”€â”€ Compute class means                          â”‚
â”‚                                                          â”‚
â”‚  Î¼â‚ = (1/Nâ‚) Î£ xâ‚™   for all n in class 1               â”‚
â”‚  Î¼â‚‚ = (1/Nâ‚‚) Î£ xâ‚™   for all n in class 2               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 2 â”€â”€ Compute within-class scatter matrices        â”‚
â”‚                                                          â”‚
â”‚  Sâ‚ = Î£ (xâ‚™âˆ’Î¼â‚)(xâ‚™âˆ’Î¼â‚)áµ€    for n in class 1           â”‚
â”‚  Sâ‚‚ = Î£ (xâ‚™âˆ’Î¼â‚‚)(xâ‚™âˆ’Î¼â‚‚)áµ€    for n in class 2           â”‚
â”‚  Sáµ‚ = Sâ‚ + Sâ‚‚                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 3 â”€â”€ Compute between-class scatter matrix         â”‚
â”‚                                                          â”‚
â”‚  SÊ™ = (Î¼â‚‚ âˆ’ Î¼â‚)(Î¼â‚‚ âˆ’ Î¼â‚)áµ€                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 4 â”€â”€ Find optimal direction w*                    â”‚
â”‚                                                          â”‚
â”‚  w* = Sáµ‚â»Â¹ (Î¼â‚‚ âˆ’ Î¼â‚)                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 5 â”€â”€ Project all data                             â”‚
â”‚                                                          â”‚
â”‚  yâ‚™ = (w*)áµ€ xâ‚™   for every training point xâ‚™           â”‚
â”‚                                                          â”‚
â”‚  Also project class means:                              â”‚
â”‚  mÌƒâ‚ = (w*)áµ€ Î¼â‚                                         â”‚
â”‚  mÌƒâ‚‚ = (w*)áµ€ Î¼â‚‚                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 6 â”€â”€ Classify new point x_new                     â”‚
â”‚                                                          â”‚
â”‚  y_new = (w*)áµ€ x_new                                    â”‚
â”‚  threshold t = (mÌƒâ‚ + mÌƒâ‚‚) / 2                           â”‚
â”‚                                                          â”‚
â”‚  If y_new â‰¥ t  â†’  class 2                               â”‚
â”‚  If y_new < t  â†’  class 1                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### The FISH Mnemonic â€” Algorithm Steps
```
F â”€â”€ Find class means       (Step 1: Î¼â‚, Î¼â‚‚)
I â”€â”€ Internal scatter       (Step 2: Sáµ‚ = Sâ‚ + Sâ‚‚)
S â”€â”€ Separation scatter     (Step 3: SÊ™ = (Î¼â‚‚âˆ’Î¼â‚)(Î¼â‚‚âˆ’Î¼â‚)áµ€)
H â”€â”€ Hit best direction     (Step 4: w* = Sáµ‚â»Â¹(Î¼â‚‚âˆ’Î¼â‚))
```

---

## 8. Full Worked Example (Flower Classification)

### Problem Setup

```
Task: Classify a flower as Setosa or Versicolor

Training Data:
Class         Sepal Length    Sepal Width
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Setosa         5.1             3.5
Setosa         4.9             3.0
Versicolor     7.0             3.2
Versicolor     6.4             3.2

Test point:  x_new = [5.9, 3.0]
```

---

### STEP 1 â€” Compute Class Means

```
Setosa (Class 1):
   Î¼â‚ = ([5.1, 3.5] + [4.9, 3.0]) / 2
      = [10.0, 6.5] / 2
      = [5.0, 3.25]   â† Setosa center âœ“

Versicolor (Class 2):
   Î¼â‚‚ = ([7.0, 3.2] + [6.4, 3.2]) / 2
      = [13.4, 6.4] / 2
      = [6.7, 3.2]    â† Versicolor center âœ“
```

---

### STEP 2 â€” Compute Within-Class Scatter Sáµ‚

```
For Setosa:
  xâ‚ = [5.1, 3.5],  diffâ‚ = xâ‚ âˆ’ Î¼â‚ = [0.1, 0.25]
  xâ‚‚ = [4.9, 3.0],  diffâ‚‚ = xâ‚‚ âˆ’ Î¼â‚ = [âˆ’0.1, âˆ’0.25]

  diffâ‚ Ã— diffâ‚áµ€ = [0.01   0.025 ]     diffâ‚‚ Ã— diffâ‚‚áµ€ = [0.01   0.025 ]
                   [0.025  0.0625]                        [0.025  0.0625]

  Sâ‚ = [0.02   0.05  ]
       [0.05   0.125 ]

For Versicolor:
  xâ‚ƒ = [7.0, 3.2],  diffâ‚ƒ = [0.3, 0]
  xâ‚„ = [6.4, 3.2],  diffâ‚„ = [âˆ’0.3, 0]

  Sâ‚‚ = [0.18  0]
       [0     0]

Total within-class scatter:
  Sáµ‚ = Sâ‚ + Sâ‚‚ = [0.02+0.18   0.05+0] = [0.20   0.05 ]
                  [0.05+0      0.125 ]   [0.05   0.125]
```

---

### STEP 3 â€” Compute Between-Class Scatter SÊ™

```
Mean difference:
  Î¼â‚‚ âˆ’ Î¼â‚ = [6.7âˆ’5.0, 3.2âˆ’3.25] = [1.7, âˆ’0.05]

  SÊ™ = [1.7  ] Ã— [1.7, âˆ’0.05] = [2.89    âˆ’0.085]
       [âˆ’0.05]                   [âˆ’0.085  0.0025]
```

---

### STEP 4 â€” Find w* = Sáµ‚â»Â¹ (Î¼â‚‚ âˆ’ Î¼â‚)

```
Sáµ‚ = [0.20   0.05 ]
     [0.05   0.125]

Formula for 2Ã—2 inverse:
  For matrix [a b], inverse = 1/(adâˆ’bc) Ã— [ d  âˆ’b]
             [c d]                         [âˆ’c   a]

  det(Sáµ‚) = (0.20 Ã— 0.125) âˆ’ (0.05 Ã— 0.05)
           = 0.025 âˆ’ 0.0025 = 0.0225

  Sáµ‚â»Â¹ = (1/0.0225) Ã— [ 0.125  âˆ’0.05]
                        [âˆ’0.05    0.20]

       = [ 5.556  âˆ’2.222]
         [âˆ’2.222   8.889]

Now compute w* = Sáµ‚â»Â¹ Ã— (Î¼â‚‚ âˆ’ Î¼â‚):
  
  w*[1] = (5.556 Ã— 1.7) + (âˆ’2.222 Ã— âˆ’0.05) = 9.445 + 0.111 = 9.556
  w*[2] = (âˆ’2.222 Ã— 1.7) + (8.889 Ã— âˆ’0.05) = âˆ’3.777 âˆ’ 0.444 = âˆ’4.221

  w* = [9.556, âˆ’4.221]   â† This is the best LDA direction âœ“
```

---

### STEP 5 â€” Project Means and Test Point

```
Project class means:
  mÌƒâ‚ = (w*)áµ€ Î¼â‚ = (9.556 Ã— 5.0) + (âˆ’4.221 Ã— 3.25) = 47.78 âˆ’ 13.72 = 34.06
  mÌƒâ‚‚ = (w*)áµ€ Î¼â‚‚ = (9.556 Ã— 6.7) + (âˆ’4.221 Ã— 3.2)  = 64.03 âˆ’ 13.51 = 50.52

Decision threshold:
  t = (mÌƒâ‚ + mÌƒâ‚‚) / 2 = (34.06 + 50.52) / 2 = 42.29

Project test point x_new = [5.9, 3.0]:
  y_new = (9.556 Ã— 5.9) + (âˆ’4.221 Ã— 3.0)
        = 56.38 âˆ’ 12.66
        = 43.72
```

---

### STEP 6 â€” Classify

```
Projected 1D number line:

  34.06        42.29        43.72   50.52
    |            |            |       |
    â†“            â†“            â†“       â†“
   mÌƒâ‚         threshold    y_new    mÌƒâ‚‚
 (Setosa)                (test pt) (Versicolor)

  y_new (43.72) > threshold (42.29)

  â†’ Classified as VERSICOLOR (Class 2) âœ…
```

### Memory Technique â€” "MSPC"
```
M â”€â”€ Means         (compute Î¼â‚, Î¼â‚‚)
S â”€â”€ Scatter       (compute Sáµ‚)
P â”€â”€ Project       (find w*, compute y)
C â”€â”€ Classify      (compare to threshold)
```

---

## 9. LDA for 3+ Categories

### How Many LDA Axes?

```
Rule:  # LDA axes = min(Câˆ’1,  p)
                        â†‘      â†‘
                     # classes  # features

Examples:
  2 classes,  1000 genes  â†’  1 LDA axis  (LD1 only)
  3 classes,  1000 genes  â†’  2 LDA axes  (LD1, LD2)
  5 classes,  1000 genes  â†’  4 LDA axes  (LD1 ... LD4)
 10 classes,  1000 genes  â†’  9 LDA axes

Memory: "One Less Than the Team Count"
```

### Why? â€” Geometric Reasoning

```
2 class means  â†’  1 line separates them  â†’  need 1 axis
3 class means  â†’  1 plane separates them â†’  need 2 axes
4 class means  â†’  3D space needed        â†’  need 3 axes

   2 points define a line      â†’  1 axis (LD1)
   3 points define a plane     â†’  2 axes (LD1, LD2)
   N points define (Nâˆ’1)D space â†’ Nâˆ’1 axes
```

### Modified Between-Class Scatter for 3+ Classes

```
Step 1: Find OVERALL mean Î¼Ì„ (mean of ALL data, all classes):
  Î¼Ì„ = (1/N) Î£â‚– Nâ‚– Î¼â‚–

Step 2: Between-class scatter becomes:
  SÊ™ = Î£â‚– Nâ‚– (Î¼â‚– âˆ’ Î¼Ì„)(Î¼â‚– âˆ’ Î¼Ì„)áµ€

  where: k  = class index
         Nâ‚– = # points in class k
         Î¼â‚– = mean of class k
         Î¼Ì„  = overall mean

Step 3: Within-class scatter stays the same:
  Sáµ‚ = Î£â‚– Î£(nâˆˆCâ‚–) (xâ‚™ âˆ’ Î¼â‚–)(xâ‚™ âˆ’ Î¼â‚–)áµ€

Step 4: Solve the GENERALIZED EIGENVALUE PROBLEM:
  SÊ™ w = Î» Sáµ‚ w

  â†’ Eigenvectors w  = LDA axes (LD1, LD2, ...)
  â†’ Eigenvalues  Î»  = how good each axis is at separating
  â†’ Sort by Î»: largest Î» â†’ LD1 (best axis)
```

### The Extended Formula (3 Classes)

```
            dâ‚Â² + dâ‚‚Â² + dâ‚ƒÂ²
  J    =  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            sâ‚Â² + sâ‚‚Â² + sâ‚ƒÂ²

  where dâ‚– = distance of class k mean from OVERALL mean Î¼Ì„
        sâ‚–Â² = within-class scatter of class k

  Same idea: Big numerator (far means), small denominator (tight clusters)
```

### Visual for 3 Classes

```
Gene Y â†‘
       â”‚  ğŸ”µğŸ”µğŸ”µ
       â”‚   ğŸ”µğŸ”µ      ğŸ”´ğŸ”´ğŸ”´
       â”‚                ğŸ”´ğŸ”´
       â”‚  ğŸŸ¢ğŸŸ¢
       â”‚   ğŸŸ¢ğŸŸ¢
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Gene X

                 â†“  LDA projects to 2D (LD1 vs LD2)

LD2 â†‘
    â”‚      ğŸ”µğŸ”µğŸ”µ
    â”‚       ğŸ”µğŸ”µ
    â”‚
    â”‚  ğŸŸ¢ğŸŸ¢         ğŸ”´ğŸ”´ğŸ”´
    â”‚   ğŸŸ¢ğŸŸ¢          ğŸ”´ğŸ”´
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ LD1

Three clean clusters! 10,000 genes â†’ 2 axes âœ…
```

---

## 10. LDA as a Classifier + Decision Rule

### Decision Rule (2 Classes)

```
Given new point x:

   1. Compute:    y = wáµ€x               (project onto LDA axis)
   2. Compute:    t = (mÌƒâ‚ + mÌƒâ‚‚) / 2    (midpoint threshold)
   3. Decide:
       y â‰¥ t  â†’  class 2
       y < t  â†’  class 1
```

### LDA Assumptions

```
For LDA to work best:
  1. Classes follow Normal (Gaussian) distributions
  2. All classes share the SAME covariance matrix (Î£â‚ = Î£â‚‚ = ... = Î£)

If assumption 2 is violated:
  â†’ Use Quadratic Discriminant Analysis (QDA)
  â†’ Each class gets its OWN covariance matrix
  â†’ Decision boundary becomes CURVED (quadratic) instead of a straight line
```

---

## 11. Connection to Other Classifiers

```
Distance-from-Means Classifier
â”‚
â”‚  When Î£ = I (identity matrix, spherical data):
â”‚  LDA reduces to â†’ classify to nearest class centroid
â”‚
â”‚  f(x) = â€–Î¼â‚‹ âˆ’ xâ€–Â² âˆ’ â€–Î¼â‚Š âˆ’ xâ€–Â²
â”‚  If f(x) > 0  â†’ class +1
â”‚  If f(x) < 0  â†’ class âˆ’1
â”‚
â”‚  All produce same form:  f(x) = wáµ€x + b
â”‚                                  â†‘
â”‚                      Linear boundary (hyperplane)
â”‚
â”œâ”€ Distance from Means:  w = Î¼â‚Š âˆ’ Î¼â‚‹         (closed-form)
â”œâ”€ Perceptron:           w updated iteratively (any separator)
â”œâ”€ SVM:                  w = maximum margin   (best separator)
â””â”€ LDA:                  w = Sáµ‚â»Â¹(Î¼â‚‚âˆ’Î¼â‚)    (best separation ratio)
```

---

## 12. ASCII Diagrams â€” Visual Reference

### Diagram A â€” BAD vs GOOD Projection

```
ORIGINAL 2D DATA:
Gene Y â†‘
     5 â”‚  ğŸŸ¢              ğŸ”´  ğŸ”´
     4 â”‚     ğŸŸ¢         ğŸ”´
     3 â”‚  ğŸŸ¢  ğŸŸ¢     ğŸ”´      ğŸ”´
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Gene X
            1   2   3   4   5   6

BAD: Drop Gene Y, project onto Gene X only:
  ğŸŸ¢ğŸŸ¢ğŸŸ¢ ğŸ”´ ğŸŸ¢ ğŸ”´ğŸ”´ ğŸ”´  â† MIXED, useless!

LDA: Project onto diagonal w* axis:
  ğŸŸ¢ğŸŸ¢ğŸŸ¢ğŸŸ¢ğŸŸ¢   ğŸ”´ğŸ”´ğŸ”´ğŸ”´ğŸ”´
  â†class 1â†’   â†class 2â†’   â† PERFECT! âœ…
```

---

### Diagram B â€” Scatter Matrices Visualized

```
WITHIN-CLASS SCATTER Sáµ‚ (want SMALL):

  Class 1 (tight = good âœ“)      Class 2 (tight = good âœ“)
       ğŸŸ¢ğŸŸ¢                            ğŸ”´ğŸ”´
        ğŸŸ¢ğŸŸ¢                             ğŸ”´ğŸ”´
        â†sÂ²â†’                            â†sÂ²â†’
       small!                           small!


BETWEEN-CLASS SCATTER SÊ™ (want LARGE):

  Î¼â‚ â—                                    â— Î¼â‚‚
       â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ d â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’
                      LARGE gap! âœ“

FISHER'S RATIO = SÊ™ / Sáµ‚ = BIG / small = MAXIMIZE âœ…
```

---

### Diagram C â€” LDA Full Pipeline

```
10,000-GENE DATA (patients)
         â”‚
         â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚  Compute Î¼â‚, Î¼â‚‚  â”‚  â† Step 1: Class means
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚  Compute Sáµ‚, SÊ™  â”‚  â† Step 2 & 3: Scatter matrices
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚  w* = Sáµ‚â»Â¹(Î”Î¼)   â”‚  â† Step 4: Best direction
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚  y = wáµ€x         â”‚  â† Step 5: Project all data
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
 1D (or 2D) PLOT â€” Easy to classify! âœ…
```

---

### Diagram D â€” Why BOTH Criteria Matter

```
Case 1: Big d, but BIG scatter â†’ FAIL âŒ

  ğŸŸ¢   ğŸŸ¢  ğŸŸ¢ ğŸ”´ğŸ”´ ğŸŸ¢ ğŸ”´  ğŸ”´   ğŸ”´
       â†‘ high scatter = overlap despite large mean distance


Case 2: Small scatter, but SMALL d â†’ FAIL âŒ

  ğŸŸ¢ğŸŸ¢ğŸŸ¢ğŸŸ¢ğŸ”´ğŸ”´ğŸ”´ğŸ”´
          â†‘ too close together


Case 3: Large d AND small scatter â†’ PERFECT âœ…

  ğŸŸ¢ğŸŸ¢ğŸŸ¢ğŸŸ¢ğŸŸ¢     â†gapâ†’     ğŸ”´ğŸ”´ğŸ”´ğŸ”´ğŸ”´
  Small scatter âœ“         Large gap âœ“
```

---

## 13. Cheatsheet

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                     LDA COMPLETE CHEATSHEET                     â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                  â•‘
â•‘  WHAT:  Supervised dimensionality reduction + classification     â•‘
â•‘  WHY:   Maximize class separation (not just variance like PCA)   â•‘
â•‘                                                                  â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  CORE FORMULA â€” Fisher's Criterion:                             â•‘
â•‘                                                                  â•‘
â•‘          wáµ€ SÊ™ w         Between-class scatter                  â•‘
â•‘  J(w) = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  =  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                   â•‘
â•‘          wáµ€ Sáµ‚ w         Within-class scatter                   â•‘
â•‘                                                                  â•‘
â•‘  MAXIMIZE J(w) â†’ find the best projection direction w*          â•‘
â•‘                                                                  â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  CLOSED-FORM SOLUTION:                                          â•‘
â•‘                                                                  â•‘
â•‘  w* = Sáµ‚â»Â¹ (Î¼â‚‚ âˆ’ Î¼â‚)          [2-class case]                   â•‘
â•‘                                                                  â•‘
â•‘  General: solve  SÊ™ w = Î» Sáµ‚ w  [eigenvalue problem]            â•‘
â•‘                                                                  â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  SCATTER MATRICES:                                              â•‘
â•‘                                                                  â•‘
â•‘  Within:   Sáµ‚ = Sâ‚ + Sâ‚‚                                        â•‘
â•‘            Sâ‚– = Î£(xâ‚™âˆ’Î¼â‚–)(xâ‚™âˆ’Î¼â‚–)áµ€                              â•‘
â•‘                                                                  â•‘
â•‘  Between:  SÊ™ = (Î¼â‚‚âˆ’Î¼â‚)(Î¼â‚‚âˆ’Î¼â‚)áµ€              [2-class]        â•‘
â•‘            SÊ™ = Î£â‚– Nâ‚–(Î¼â‚–âˆ’Î¼Ì„)(Î¼â‚–âˆ’Î¼Ì„)áµ€           [multi-class]  â•‘
â•‘                                                                  â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  NUMBER OF LDA AXES:                                            â•‘
â•‘                                                                  â•‘
â•‘  # axes = min(Câˆ’1,  p)                                          â•‘
â•‘  C = # classes       p = # features                             â•‘
â•‘                                                                  â•‘
â•‘  2 classes â†’ 1 axis   â”‚  3 classes â†’ 2 axes                     â•‘
â•‘  5 classes â†’ 4 axes   â”‚  N classes â†’ Nâˆ’1 axes                   â•‘
â•‘                                                                  â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  CLASSIFICATION RULE:                                           â•‘
â•‘                                                                  â•‘
â•‘  y = wáµ€ x_new                                                   â•‘
â•‘  t = (mÌƒâ‚ + mÌƒâ‚‚) / 2      â† midpoint threshold                  â•‘
â•‘  y â‰¥ t  â†’  class 2                                              â•‘
â•‘  y < t  â†’  class 1                                              â•‘
â•‘                                                                  â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  2Ã—2 MATRIX INVERSE (exam helper):                              â•‘
â•‘                                                                  â•‘
â•‘  A = [a b]     Aâ»Â¹ = â”€â”€1â”€â”€  Ã— [ d  âˆ’b]                         â•‘
â•‘      [c d]           adâˆ’bc     [âˆ’c   a]                         â•‘
â•‘                                                                  â•‘
â•‘  det(A) = ad âˆ’ bc                                               â•‘
â•‘                                                                  â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  LDA vs PCA:                                                    â•‘
â•‘                                                                  â•‘
â•‘  PCA:  Unsupervised â”‚ max variance  â”‚ no labels   â”‚ any # axes  â•‘
â•‘  LDA:  Supervised   â”‚ max separationâ”‚ needs labelsâ”‚ Câˆ’1 axes    â•‘
â•‘                                                                  â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  ASSUMPTIONS:                                                   â•‘
â•‘  1. Classes are Gaussian (normally distributed)                 â•‘
â•‘  2. Equal covariance across classes (Î£â‚ = Î£â‚‚)                  â•‘
â•‘     [If violated â†’ use QDA instead]                             â•‘
â•‘                                                                  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## 14. Mnemonics & Memory Tricks

### ğŸ§  The Big 7 Mnemonics

---

#### 1. "PULL APART, PACK TOGETHER" â€” LDA's Two Goals
```
PULL the class means APART  â†’  maximize SÊ™ (between-class scatter)
PACK each class TOGETHER    â†’  minimize Sáµ‚ (within-class scatter)

Ratio = PULL APART / PACK TOGETHER = SÊ™ / Sáµ‚
```

---

#### 2. "PCA is BLIND, LDA can SEE" â€” The Key Difference
```
PCA is BLIND â†’ doesn't see class labels  â†’ unsupervised
LDA can SEE  â†’ uses class labels          â†’ supervised

"Blind PCA wanders; Seeing LDA separates."
```

---

#### 3. "BIG GAP, SMALL MESS" â€” The Ideal LDA Outcome
```
BIG GAP  =  large distance between class means  â†’  numerator big
SMALL MESS = small scatter within each class     â†’  denominator small

BIG GAP / SMALL MESS  â†’  best J(w) value!
```

---

#### 4. "ONE LESS AXIS" â€” Number of LDA Axes
```
C classes â†’ Câˆ’1 LDA axes

2 classes  â†’  1 axis (LD1)
3 classes  â†’  2 axes (LD1, LD2)
10 classes â†’  9 axes

"Count the classes, subtract one â€” that's your axes done!"
```

---

#### 5. "FISH" â€” The LDA Algorithm
```
F â”€â”€ Find class means       Î¼â‚, Î¼â‚‚
I â”€â”€ Internal scatter       Sáµ‚ = Sâ‚ + Sâ‚‚
S â”€â”€ Separation scatter     SÊ™ = (Î¼â‚‚âˆ’Î¼â‚)(Î¼â‚‚âˆ’Î¼â‚)áµ€
H â”€â”€ Hit best direction     w* = Sáµ‚â»Â¹(Î¼â‚‚âˆ’Î¼â‚)
```

---

#### 6. "MSPC" â€” Classification Steps
```
M â”€â”€ Means          compute Î¼â‚, Î¼â‚‚
S â”€â”€ Scatter        compute Sáµ‚, w*
P â”€â”€ Project        y = wáµ€x
C â”€â”€ Classify       compare y to threshold t
```

---

#### 7. "Sáµ‚ for Within, SÊ™ for Between" â€” Scatter Matrix Names
```
Sáµ‚ â†’ W = Within  â†’ measures spread INSIDE each class
SÊ™ â†’ B = Between â†’ measures gap BETWEEN different classes
```

---

#### 8. Closet Analogy â€” The Big Picture
```
LDA is like organizing a closet:

BEFORE:                   AFTER (LDA):
T-shirts, dresses,       [TTT]  gap  [DDD]
dresses, T-shirts   â†’    All       All
mixed together!          T-shirts  Dresses

1. Maximize space BETWEEN shirt types  â†’ maximize SÊ™
2. Minimize space WITHIN each type     â†’ minimize Sáµ‚
3. Result: Easy to find what you need! â†’ easy to classify!
```

---

## 15. Common Mistakes & Fixes

| Mistake | What Goes Wrong | The Fix |
|---|---|---|
| Only maximizing d without controlling sÂ² | Means are far but groups overlap due to high scatter | Always optimize the RATIO dÂ²/(sâ‚Â²+sâ‚‚Â²) |
| Confusing Sáµ‚ and SÊ™ | Wrong matrices â†’ wrong direction | W = Within (same class); B = Between (different classes) |
| Forgetting to invert Sáµ‚ | Wrong formula | w* = **Sáµ‚â»Â¹** (Î¼â‚‚âˆ’Î¼â‚), NOT Sáµ‚ Ã— (Î¼â‚‚âˆ’Î¼â‚) |
| Expecting C axes for C classes | Over-counting output dimensions | LDA gives **Câˆ’1** axes, not C |
| Using LDA when class covariances differ a lot | Poor performance | Switch to **QDA** |
| Assuming LDA and PCA give same result | Using wrong tool | PCA = max variance; LDA = max separation; different axes! |
| Not normalizing w before comparing | Scale-dependent results | Normalize: Åµ = w/â€–wâ€– |

---

## 16. Quick Q&A for Exam

**Q: Is LDA supervised or unsupervised?**
> **Supervised** â€” it requires class labels during training.

**Q: Can LDA be used for visualization only (not classification)?**
> Yes! LDA can reduce 10,000 dimensions to 2D (LD1 vs LD2) just to visualize cluster structure.

**Q: What is the relationship between Distance-from-Means and LDA?**
> Distance-from-Means is a **special case** of LDA. When all classes have equal spherical covariance (Î£ = I), LDA reduces to classifying by the nearest class centroid.

**Q: What if Sáµ‚ is not invertible?**
> Happens when features > samples, or features are linearly dependent. Solutions: regularize by adding Î»I to Sáµ‚ (Ridge-style), use pseudo-inverse, or reduce dimensions with PCA first.

**Q: What does eigenvalue Î» tell us in SÊ™w = Î»Sáµ‚w?**
> Î» measures how good that axis is for separation. Higher Î» = better separation. Sort eigenvectors by their Î» to get LD1 (best), LD2 (second best), etc.

**Q: What is QDA?**
> Quadratic Discriminant Analysis. Like LDA but each class gets its own covariance matrix Î£â‚–. The decision boundary becomes a curved (quadratic) surface instead of a straight hyperplane.

**Q: How does LDA compare to Logistic Regression?**
> Both are linear classifiers. LDA assumes Gaussian data + equal covariance â†’ works better with small data if assumptions hold. Logistic Regression makes no distribution assumption â†’ works better with large data or non-Gaussian data.

**Q: Why does PCA sometimes fail where LDA succeeds?**
> PCA finds the directions of maximum **variance**. But the most variable direction may be completely uncorrelated with class labels. LDA directly optimizes for **class separability**, guaranteeing the found axes are useful for classification.

---

## ğŸ Summary in 5 Lines

```
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
1. PROBLEM:   High-D data, classify into known groups
2. IDEA:      Find axis that SEPARATES classes best
3. MATH:      Maximize J(w) = SÊ™/Sáµ‚  â†’ w* = Sáµ‚â»Â¹(Î¼â‚‚âˆ’Î¼â‚)
4. OUTPUT:    Câˆ’1 new axes (LD1, LD2â€¦) for C classes
5. CLASSIFY:  Project new point, compare to midpoint threshold
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Key difference from PCA:
  PCA = BLIND (no labels) â†’ max variance
  LDA = HAS EYES (uses labels) â†’ max separation
```

---

*Covers: LDA Part-1 (Fisher's LDA + Math) & Part-2 (Gene Expression + 3-Class LDA)*
